{
  "hash": "d1f1b74967860b8a7ae890ac97bee7e9",
  "result": {
    "engine": "knitr",
    "markdown": "---\n  title: \"More hypothesis testing\"\n---\n\n\nAn hypothesis test is a way of determining which of two competing claims a dataset is most consistent with. If, for example, we are investigating whether two experimental groups (treatment and control) have the same probability of exhibiting some characteristic, we state the claims as null and alternative hypotheses:\n\n$$\n\\begin{align*}\n\\text{H}_0&:p_{\\text{trt}} = p_{\\text{ctr}}\\\\\n\\text{H}_A&:p_{\\text{trt}} \\neq p_{\\text{ctr}}.\n\\end{align*}\n$$\n\nSo the question becomes \"do these groups have the same probability, or not?\" An alternative way of writing these hypotheses is\n\n$$\n\\begin{align*}\n\\text{H}_0&:p_{\\text{trt}} - p_{\\text{ctr}} =0\\\\\n\\text{H}_A&:p_{\\text{trt}} - p_{\\text{ctr}}\\neq 0.\n\\end{align*}\n$$\n\nTo answer this question, we must judge how compatible our point estimates $\\hat{p}_{\\text{trt}}$ and $\\hat{p}_{\\text{ctr}}$ are with the null hypothesis, and we do this by calculating a $p$-value and deciding if it is small enough.\n\nThis primer leads you down the path of least resistance to calculating the $p$-value.\n\n## Load packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse) \nlibrary(tidymodels) \nlibrary(openintro)\n```\n:::\n\n\n## Dataset: is yawning contagious?\n\nAn experiment conducted by the MythBusters tested if a person can be subconsciously influenced into yawning if another person near them yawns. In this study 50 people were randomly assigned to two groups: 34 to a group where a person near them yawned (treatment) and 16 to a control group where they didn’t see someone yawn (control). This is the `yawn` dataset, pre-loaded in the `openintro` package:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nyawn\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 50 × 2\n   result group\n   <fct>  <fct>\n 1 yawn   trmt \n 2 yawn   trmt \n 3 yawn   trmt \n 4 yawn   trmt \n 5 yawn   trmt \n 6 yawn   trmt \n 7 yawn   trmt \n 8 yawn   trmt \n 9 yawn   trmt \n10 yawn   trmt \n# ℹ 40 more rows\n```\n\n\n:::\n:::\n\n\nIn this case, we are interested in a *one-sided* alternative. We want to know if there is evidence to suggest that proximity to a yawner specifically *increases* your probability of yawning. So we are testing:\n\n$$\n\\begin{align*}\n\\text{H}_0&:p_{\\text{trt}} - p_{\\text{ctr}} =0\\\\\n\\text{H}_A&:p_{\\text{trt}} - p_{\\text{ctr}}> 0.\n\\end{align*}\n$$\n\nThe sample proportions in each experimental group are as follows\n\n\n::: {.cell}\n\n```{.r .cell-code}\nyawn |>\n  count(group, result) |>\n  group_by(group) |>\n  mutate(p_hat = n / sum(n))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 4\n# Groups:   group [2]\n  group result       n p_hat\n  <fct> <fct>    <int> <dbl>\n1 ctrl  not yawn    12 0.75 \n2 ctrl  yawn         4 0.25 \n3 trmt  not yawn    24 0.706\n4 trmt  yawn        10 0.294\n```\n\n\n:::\n:::\n\n\nThis code computes the differences in the yawning proportions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nobs_stat_yawner <- yawn |>\n  specify(response = result, explanatory = group, success = \"yawn\") |>\n  calculate(stat = \"diff in props\", order = c(\"trmt\", \"ctrl\"))\nobs_stat_yawner\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nResponse: result (factor)\nExplanatory: group (factor)\n# A tibble: 1 × 1\n    stat\n   <dbl>\n1 0.0441\n```\n\n\n:::\n:::\n\n\nWe see that there is *some* positive difference in the proportion of yawners in each group, but how can we tell if it is merely an artifact of random sampling, or if we are actually picking up on a meaningful difference? That is what hypothesis testing is all about.\n\n## Simulating the null distribution\n\nIn order to assess whether or not the estimated difference of 29.4% - 25% = 4.4% is compatible with the hypothesized difference of 0%, we look at the *null distribution* of the estimator. This is the *hypothetical* sampling distribution that the sample difference would possess if the null hypothesis happened to be true. When we visualize this distribution, it displays the range of values that our estimate would be likely to have if the null were true. We then compare this to the actual estimate that our actual data actually gave us, and if the actual estimate and the null distribution appear to be incompatible with one another, then we reject the null hypothesis and accept the alternative.\n\nThe following code simulates the null distribution of the sample difference in proportions:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(25)\n\nnull_dist_yawner <- yawn |>\n  specify(response = result, explanatory = group, success = \"yawn\") |>\n  hypothesize(null = \"independence\") |>\n  generate(reps = 1000, type = \"permute\") |>\n  calculate(stat = \"diff in props\", order = c(\"trmt\", \"ctrl\"))\nnull_dist_yawner\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nResponse: result (factor)\nExplanatory: group (factor)\nNull Hypothesis: independence\n# A tibble: 1,000 × 2\n   replicate    stat\n       <int>   <dbl>\n 1         1  0.0441\n 2         2 -0.140 \n 3         3 -0.0478\n 4         4 -0.232 \n 5         5  0.0441\n 6         6 -0.232 \n 7         7 -0.0478\n 8         8 -0.0478\n 9         9  0.136 \n10        10 -0.0478\n# ℹ 990 more rows\n```\n\n\n:::\n:::\n\n\nSo this code generates 1,000 fake data sets, and calculates the difference in sample proportions for each one. Here is what each line is doing:\n\n-   `set.seed(25)`: we are setting the random number seed to ensure that our results are perfectly replicable;\n-   `specify(response = result, explanatory = group, success = \"yawn\")`: first we specify which columns in our data frame we are using. These columns are categorical variables, each with two levels, so next we have to tell the computer which level of the response (\"yawn\" versus \"not yawn\") we are calculating the proportions of;\n-   `hypothesize(null = \"independence\")`: since our null hypothesis states that there is no difference between these groups. In other words, the groups are \"independent;\"\n-   `generate(reps = 1000, type = \"permute\")`: the null distribution is a sampling distribution, the same as when we studied confidence intervals and bootstrapping. So here we tell it how many alternative, hypothetical datasets we wish to simulate in order to get a pretty-looking histogram. The more the merrier, I say;\n-   `calculate(stat = \"diff in props\", order = c(\"trmt\", \"ctrl\"))`: the statistic we are estimating in this case is the difference in group proportions. On Lab 5 we considered other statistics of interest: sample means, medians, standard deviations, and so forth.\n\n## Visualizing the null distribution, observed statistic, and $p$-value\n\nHere is the code:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnull_dist_yawner |>\n  visualize() +\n  shade_p_value(obs_stat = obs_stat_yawner, direction = \"greater\")\n```\n\n::: {.cell-output-display}\n![](computing-hypotheses-2_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nThere are three aspects to it:\n\n-   When we simulate the null distribution of the sample proportion as above, we get a list of 1,000 numbers. To visualize the null distribution, we can simply plot a histogram of these;\n-   to visualize where the actual estimated difference of 4.4% falls in that distribution, we can add a vertical line;\n-   The $p$-value is the probability, assuming the null is true, of observing a sample statistic as or more extreme then the one we actually got. So it's the probability under the null of seeing something *even farther* to the right of 4.4%. The fraction of the area shaded red is the $p$-value. We say `direction = \"greater\"` because our alternative hypothesis is one-sided.\n\nInspecting the picture, we see that our estimated difference is pretty much smack in the middle of the null distribution. So the null hypothesis of no difference is very much in play. Evidence is insufficient to reject it.\n\n## Computing the p-value\n\nTo quantify that intuition, this code calculates the $p$-value:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnull_dist_yawner |>\n  get_p_value(obs_stat = obs_stat_yawner, \"greater\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 1\n  p_value\n    <dbl>\n1   0.522\n```\n\n\n:::\n:::\n\n\nThe interpretation of this number is \"if there were in fact no difference between treatment and control, there is a 52.2% chance that we would observe a difference even greater than the one we actually observed when we ran the experiment.\" This p-value is above every standard discernibility level, and so we fail to reject the null. If the null hypothesis were indeed true, the results we got would not be super surprising, so we cannot rule the null out.\n",
    "supporting": [
      "computing-hypotheses-2_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}