{
  "hash": "f9f6b04081a66bf8ef7d396ebd7a7ea4",
  "result": {
    "engine": "knitr",
    "markdown": "---\n  title: \"Multiple linear regression (with two predictors)\"\n---\n\n\nThe multiple linear regression model relates several predictors to a single response via a linear function with error. Here is what it looks like with two predictors:\n\n$$\ny=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\varepsilon.\n$$\n\nThis primer leads you down the path of least resistance to fitting this model when one of the predictors ($x_1$) is a numerical variable and the other ($x_2$) is a categorical variable with two levels. You will learn how to...\n\n-   create a scatter plot of $x_1$ versus $y$ that differentiates between the levels of $x_2$;\n-   add several best fitting lines to this plot;\n-   produce a table with the coefficient estimates ($\\hat{\\beta}_0,\\,\\hat{\\beta}_1,\\,\\hat{\\beta}_2$);\n-   calculate $R^2$ and adjusted $R^2$ for assessing model fit.\n\n## Setup\n\nThe commands for plotting are in the `tidyverse` package, and the commands for working with linear regressions are in the `tidymodels` package, so we crank that up:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse) \nlibrary(tidymodels) \n```\n:::\n\n\nNext consider the dataset `allbacks` from the `DAAG` package (you may need to install this):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(DAAG) \n\nallbacks\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   volume area weight cover\n1     885  382    800    hb\n2    1016  468    950    hb\n3    1125  387   1050    hb\n4     239  371    350    hb\n5     701  371    750    hb\n6     641  367    600    hb\n7    1228  396   1075    hb\n8     412    0    250    pb\n9     953    0    700    pb\n10    929    0    650    pb\n11   1492    0    975    pb\n12    419    0    350    pb\n13   1010    0    950    pb\n14    595    0    425    pb\n15   1034    0    725    pb\n```\n\n\n:::\n:::\n\n\nThe dataset has 15 observations and 4 columns. Each observation represents a book. Note that `volume` is measured in cubic centimeters and `weight` is measured in grams. More information on the dataset can be found in the documentation for `allbacks`, with `?allbacks`.\n\nWe are interested in the relationship between book volume ($x_1$) and book weight ($y$), but as you can see if you view the spreadsheet, some of the books are hardcover and some are paperback. We might expect a hardcover book to weigh more than a paperback book with the same volume because of the different materials used, and so we should account for that.\n\n## Creating a grouped scatterplot\n\nThis code creates a scatterplot of `volume` vs. `weight` where the points have different color and shape depending on the value of `cover`:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(allbacks, aes(x = volume, y = weight)) +\n  geom_point(aes(color = cover, shape = cover)) +\n  labs(\n    x = \"Volume (cubic centimeters)\",\n    y = \"Weight (grams)\"\n  )\n```\n\n::: {.cell-output-display}\n![](computing-mlr_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nThe only difference between this code and a simple scatter plot is that we included an aesthetic mapping `aes(color = cover, shape = cover)` inside the call to `geom_point`. You're asking it to alter the color and shape of the points depending on the value of the variable `cover`. The computer then made some default choices for you about what the different colors and shapes will be.\n\n## Adding lines of best fit\n\nIf we want to add lines of best fit *for each group*, we just have to add one new layer to our code above:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(allbacks, aes(x = volume, y = weight)) +\n  geom_point(aes(color = cover, shape = cover)) + \n  geom_smooth(aes(color = cover), method = \"lm\", se = F) +\n  labs(\n    x = \"Volume (cubic centimeters)\",\n    y = \"Weight (grams)\"\n  )\n```\n\n::: {.cell-output-display}\n![](computing-mlr_files/figure-html/plot-weight-volume-1.png){width=672}\n:::\n:::\n\n\nAgain, the only difference between this code and the code that would generate one line of best fit is that we added an aesthetic mapping `aes(color = cover)` to the arguments of `geom_smooth`, and the computer knows to plot several lines for each level of the variable `cover`, which only has two levels in this case.\n\nAs we expected, the line of best fit for the paperback books is below the line for the hardcover books, capturing our intuition that, for the same volume, a hardcover book will weigh a bit more.\n\n## Getting the actual coefficient estimates\n\nThis code will give you a table with the estimates:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nweight_cover_fit <- linear_reg() |>\n  fit(weight ~ volume + cover, data = allbacks)\n\ntidy(weight_cover_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term        estimate std.error statistic      p.value\n  <chr>          <dbl>     <dbl>     <dbl>        <dbl>\n1 (Intercept)  198.      59.2         3.34 0.00584     \n2 volume         0.718    0.0615     11.7  0.0000000660\n3 coverpb     -184.      40.5        -4.55 0.000672    \n```\n\n\n:::\n:::\n\n\nSo the fitted model is\n\n$$\n\\begin{align*}\n\\widehat{\\text{weight}}&=\\hat{\\beta}_0+\\hat{\\beta}_1{\\text{volume}}+\\hat{\\beta}_2{\\text{cover}}\\\\\n&\\approx197.96+0.71\\cdot{\\text{volume}}-184.05\\cdot{\\text{cover}}.\n\\end{align*}\n$$ The interpretation of these estimates is:\n\n-   197.96 is the weight you would predict for a hardback book with zero volume (it's a little silly that this is not zero, which highlights one limitation of a linear model here);\n-   0.71 is the amount (in grams) you would predict the weight to increase by if the volume of the book increased by 1 cubic centimeter (remember, slope = rise/run, so $\\Delta\\text{weight}/\\Delta\\text{volume}$ in this case);\n-   -184.05 is the amount by which the line shifts *downward* (because it's negative) to account for the fact that paperback books will typically be lighter than hardcover, even keeping the volume constant.\n\nThe only difference from the code for simple linear regression is that we added variables to the formula `weight ~ volume + cover` in the `fit` command. So in general, if you have a data frame `df` with columns `y`, `x1`, `x2`, and so on, you can fit a multiple linear regression with code that will look something like this:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmy_regression_fit <- linear_reg() |>\n  fit(y ~ x1 + x2 + x3, data = df)\n```\n:::\n\n\nTo add more predictors, you just add them to the formula `y ~ x1 + x2 + x3 + ...`.\n\n## Compute the $R^2$ values\n\nOnce you have created a regression object with the code like `linear_reg() |> fit()`, you can view the $R^2$ values with the `glance` function to assess the goodness-of-fit of the model. Here they are for the simple linear regression that just has `volume` as a predictor:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nweight_fit <- linear_reg() |>\n  fit(weight ~ volume, data = allbacks)\n\nglance(weight_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic    p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>      <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.803         0.787  124.      52.9 0.00000626     1  -92.5  191.  193.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n\n\n:::\n:::\n\n\nHere they are for the multiple linear regression that has `volume` and `cover` as predictors:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nweight_fit <- linear_reg() |>\n  fit(weight ~ volume + cover, data = allbacks)\n\nglance(weight_cover_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic     p.value    df logLik   AIC   BIC\n      <dbl>         <dbl> <dbl>     <dbl>       <dbl> <dbl>  <dbl> <dbl> <dbl>\n1     0.927         0.915  78.2      76.7 0.000000145     2  -85.0  178.  181.\n# ℹ 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\n```\n\n\n:::\n:::\n\n\nWe see that adjusted $R^2$ increased substantially when we added `cover` as a predictor, supporting our intuition that including this covariate to the model ought to improve fit.\n",
    "supporting": [
      "computing-mlr_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}