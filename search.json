[
  {
    "objectID": "course-grading.html",
    "href": "course-grading.html",
    "title": "Assignments and grading",
    "section": "",
    "text": "The final course grade will be calculated as follows:\nThe final letter grade will be determined based on the following thresholds, which will not change:",
    "crumbs": [
      "Syllabus",
      "Assignments and grading"
    ]
  },
  {
    "objectID": "course-grading.html#labs",
    "href": "course-grading.html#labs",
    "title": "Assignments and grading",
    "section": "Labs",
    "text": "Labs\nIn labs, you will apply the concepts discussed in lecture to various data analysis scenarios. Labs will focus on both computation and conceptualization. Lab assignments will be completed using Quarto and submitted as as a PDF for grading in Gradescope.\n\n\n\n\n\n\nNote\n\n\n\nYour lowest lab score will be dropped.",
    "crumbs": [
      "Syllabus",
      "Assignments and grading"
    ]
  },
  {
    "objectID": "course-grading.html#exams",
    "href": "course-grading.html#exams",
    "title": "Assignments and grading",
    "section": "Exams",
    "text": "Exams\nThere will be two exams. Each exam will be comprised of two components:\n\nIn class: 75 minute in-class exam. This exam is closed book, however you are allowed to use one sheet of notes (“cheat sheet”) to the midterm and the final. This sheet must be no larger than 8.5 inches x 11 inches, and must be prepared by you. You may use both sides of the sheet. (70% of the grade)\nTake home: Following the in class exam, you’ll have 48 hours to complete the take home portion of your exam. The take home portion will follow from the in class exam and focus on the analysis of a dataset introduced in the take home exam. (30% of the grade)\n\nThrough these exams you have the opportunity to demonstrate what you’ve learned in the course thus far. Each exam will include small analyses and computational tasks related to the content in application exercises and labs. More details about the content and structure of the exams will be discussed during the semester.\nSee the course schedule for dates and times of the exams. Exam dates cannot be changed and no make-up exams will be given. If you can’t take the exams on these dates, you should drop this class.",
    "crumbs": [
      "Syllabus",
      "Assignments and grading"
    ]
  },
  {
    "objectID": "course-grading.html#final-project",
    "href": "course-grading.html#final-project",
    "title": "Assignments and grading",
    "section": "Final project",
    "text": "Final project\nThe course ends with a final project where you will explore a question and data set of your own. More details about the projects will be provided during the semester. The project will be completed in teams, and your final submission will consist of a written report and a five minute video presentation of your work. All team members are expected to contribute equally to the completion of each project and you will be asked to evaluate your team members. Failure to adequately contribute to an assignment will result in a penalty to your mark relative to the team’s overall mark.\nSee the course schedule for dates and times of project deadlines. Project deadlines cannot be changed.",
    "crumbs": [
      "Syllabus",
      "Assignments and grading"
    ]
  },
  {
    "objectID": "computing/computing-normal.html",
    "href": "computing/computing-normal.html",
    "title": "The normal distribution",
    "section": "",
    "text": "The normal distribution is the familiar bell curve. The central limit theorem says that the sampling distribution of a statistic (sample mean, proportion, difference, etc) looks more and more like a normal distribution the more data we have. This gives us permission to use a normal distribution to approximate the sampling distribution, and use the convenient properties of the bell curve to compute confidence intervals and hypothesis tests.\nThis primer introduces you to the basic commands for working with the normal distribution.",
    "crumbs": [
      "Computing primers",
      "Normal distribution"
    ]
  },
  {
    "objectID": "computing/computing-normal.html#load-packages",
    "href": "computing/computing-normal.html#load-packages",
    "title": "The normal distribution",
    "section": "Load packages",
    "text": "Load packages\nThe openintro package has tools for working with the normal distribution:\n\nlibrary(tidyverse) \nlibrary(openintro)",
    "crumbs": [
      "Computing primers",
      "Normal distribution"
    ]
  },
  {
    "objectID": "computing/computing-normal.html#example-bone-density",
    "href": "computing/computing-normal.html#example-bone-density",
    "title": "The normal distribution",
    "section": "Example: bone density",
    "text": "Example: bone density\nSuppose the bone density of 65-year-old women is normally distributed with mean 809 \\(mg/cm^3\\) and standard deviation of 140 \\(mg/cm^3\\). For reference, here are the densities for three types of wood:\n\nPlywood: 540 \\(mg/cm^3\\)\nPine: 600 \\(mg/cm^3\\)\nMahogany: 710 \\(mg/cm^3\\)\n\nIf \\(X\\) represents the bone density of 65-year-old women, then we can summarize the situation mathematically by writing:\n\\[\nX \\sim N(\\mu = 809, \\sigma = 140).\n\\]\nLet’s give all of the numbers above names and store them in the computer for later:\n\nbone_density_mean &lt;- 809\nbone_density_sd &lt;- 140\n\nplywood &lt;- 540\npine &lt;- 600\nmahogany &lt;- 710",
    "crumbs": [
      "Computing primers",
      "Normal distribution"
    ]
  },
  {
    "objectID": "computing/computing-normal.html#plot-the-bell-curve",
    "href": "computing/computing-normal.html#plot-the-bell-curve",
    "title": "The normal distribution",
    "section": "Plot the bell curve",
    "text": "Plot the bell curve\nThis handy-dandy function plots a bell curve:\n\nnormTail(m = 0, s = 1)\n\n\n\n\n\n\n\n\nYou can add several arguments to customize:\n\nnormTail(m = bone_density_mean, \n         s = bone_density_sd, \n         main = \"Distribution of bone density of 65-year-old women\",\n         xlab = \"mg/cm^3\",\n         xlim = c(200, 1400))",
    "crumbs": [
      "Computing primers",
      "Normal distribution"
    ]
  },
  {
    "objectID": "computing/computing-normal.html#compute-quantiles-under-the-normal-distribution",
    "href": "computing/computing-normal.html#compute-quantiles-under-the-normal-distribution",
    "title": "The normal distribution",
    "section": "Compute quantiles under the normal distribution",
    "text": "Compute quantiles under the normal distribution\nThe qnorm function computes the quantiles of a normal distribution. If we wanted the 50% quantile (AKA the median), we would say\n\nqnorm(0.5, mean = 0, sd = 1)\n\n[1] 0\n\n\nIn this case we just get back the mean, because the normal distribution is perfectly symmetric, and mean and median are equal for symmetric distributions.\nIf we wanted the inner quartile range (IQR) for a normal distribution, here it goes:\n\nq25 &lt;- qnorm(0.25, mean = 0, sd = 1)\nq75 &lt;- qnorm(0.75, mean = 0, sd = 1)\n\nIQR &lt;- q75 - q25\nIQR\n\n[1] 1.34898",
    "crumbs": [
      "Computing primers",
      "Normal distribution"
    ]
  },
  {
    "objectID": "computing/computing-normal.html#compute-and-visualize-probabilities-under-the-normal-distribution",
    "href": "computing/computing-normal.html#compute-and-visualize-probabilities-under-the-normal-distribution",
    "title": "The normal distribution",
    "section": "Compute and visualize probabilities under the normal distribution",
    "text": "Compute and visualize probabilities under the normal distribution\nWhat is the probability that a randomly selected 65-year-old woman has bones that are as or less dense than pine? If we add an L argument (for “lower”) to normTail, we can visualize the answer:\n\nnormTail(m = bone_density_mean, s = bone_density_sd, L = pine,\n         main = \"Distribution of bone density of 65-year-old women\",\n         xlab = \"mg/cm^3\",)\n\n\n\n\n\n\n\n\nNotice how this may come in handy when we have to compute something like a \\(p\\)-value. In order to compute the probability (area) under the bell curve of being less than 600 \\(mg/cm^3\\), we can use the pnorm function:\n\npnorm(pine, mean = bone_density_mean, sd = bone_density_sd, lower.tail = TRUE)\n\n[1] 0.06773729\n\n\nAbove, we asked about being under 600 in the distribution. But we could just have easily looked at being over 600 by adding a U argument (for “upper”):\n\nnormTail(m = bone_density_mean, s = bone_density_sd, U = pine,\n         main = \"Distribution of bone density of 65-year-old women\",\n         xlab = \"mg/cm^3\",)\n\n\n\n\n\n\n\n\nTo get the number, we change the lower.tail argument:\n\npnorm(pine, mean = bone_density_mean, sd = bone_density_sd, lower.tail = FALSE)\n\n[1] 0.9322627\n\n\nNotice that the probability of being at or below 600 is 6.77%. The probability of being above 600 is 93.23% That covers everything, so taken together you get 100%, which is the total area under the bell curve.",
    "crumbs": [
      "Computing primers",
      "Normal distribution"
    ]
  },
  {
    "objectID": "computing/computing-logistic.html",
    "href": "computing/computing-logistic.html",
    "title": "Logistic regression",
    "section": "",
    "text": "The logistic regression model uses a set of predictors (\\(x_1\\), \\(x_2\\), …, \\(x_p\\)) to model the probability that a binary response \\(y\\) is equal to one:\n\\[\n\\text{Prob}(y=1)=\\frac{1}{1+e^{-(\\beta_0+\\beta_1x_1+...+\\beta_px_p)}}.\n\\]\nThis primer leads you down the path of least resistance to fitting this model, plotting the best-fitting S-curve in the single-predictor case, plotting the decision boundary in the bivariate case, and generating predictions.",
    "crumbs": [
      "Computing primers",
      "Logistic regression"
    ]
  },
  {
    "objectID": "computing/computing-logistic.html#setup",
    "href": "computing/computing-logistic.html#setup",
    "title": "Logistic regression",
    "section": "Setup",
    "text": "Setup\nThe commands for working with logistic regressions are in the tidymodels package, so load that:\n\nlibrary(tidyverse) \nlibrary(tidymodels) \n\nNext, we need something to model, so let us load in a data set. We will consider this data set from Hewlett-Packard on spam emails:\n\nhp_spam &lt;- read_csv(\"hp-spam.csv\")\n\nhp_spam &lt;- hp_spam |&gt;\n  mutate(type = as.factor(type))\n\n\n\n\n\n\n\nNote\n\n\n\nBe prepared to adjust the file path to match how you have organized your files and folders.\n\n\n\nhp_spam |&gt;\n  select(type, george, capitalTotal, you, charDollar)\n\n# A tibble: 4,601 × 5\n   type  george capitalTotal   you charDollar\n   &lt;fct&gt;  &lt;dbl&gt;        &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;\n 1 1          0          278  1.93      0    \n 2 1          0         1028  3.47      0.18 \n 3 1          0         2259  1.36      0.184\n 4 1          0          191  3.18      0    \n 5 1          0          191  3.18      0    \n 6 1          0           54  0         0    \n 7 1          0          112  3.85      0.054\n 8 1          0           49  0         0    \n 9 1          0         1257  1.23      0.203\n10 1          0          749  1.67      0.081\n# ℹ 4,591 more rows\n\n\nEach observation (row) represents an email that was sent to a person named George, and the variables (columns) include:\n\ntype: is the email spam or not?\ngeorge: percentage of words in email that are “George”;\nyou: percentage of words in email that are “you”;\ncapitalTotal: number of capitalized letters in email;\ncharDollar: percentage of characters that are a dollar sign.\n\nIntuition suggests that a less personalized email (does not mention George by name) with lots of capital letters (I’M NOT SCREAMING) and dollar signs is more likely to be spam. Can we capture this in a model?",
    "crumbs": [
      "Computing primers",
      "Logistic regression"
    ]
  },
  {
    "objectID": "computing/computing-logistic.html#run-a-simple-logistic-regression-and-plot-the-best-fitting-s-curve",
    "href": "computing/computing-logistic.html#run-a-simple-logistic-regression-and-plot-the-best-fitting-s-curve",
    "title": "Logistic regression",
    "section": "Run a “simple” logistic regression and plot the best-fitting S-curve",
    "text": "Run a “simple” logistic regression and plot the best-fitting S-curve\nTake the same code we’ve seen before, change linear_reg to logistic_reg, and bada bing:\n\ntype_you_fit &lt;- logistic_reg() |&gt;\n  fit(type ~ you, data = hp_spam)\n  \ntidy(type_you_fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic   p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)   -1.01     0.0456     -22.2 4.72e-109\n2 you            0.341    0.0193      17.6 1.68e- 69\n\n\nThese estimates determine the shape of the best fitting S-curve for these data. Chapter Nine in our textbook provides further discussion of how we might interpret the estimates probabilistically, but we will not concern ourselves with this at the present. For us, estimating these will simply be a means to an end of determining a model that we can use as a black box for prediction.\nThis code plots the S-curve. Apart from the weird and ugly y = as.numeric(type) - 1) part at the beginning, the only difference with plotting a straight best fit line is we changed the method (and some other things) inside geom_smooth:\n\nggplot(hp_spam, aes(x = you, y = as.numeric(type) - 1)) + \n  geom_point() + \n  geom_smooth(\n    method = \"glm\", \n    se = FALSE, \n    method.args = list(family = binomial)\n  ) + \n  labs(\n    x = \"Percent of words equal to `you`\",\n    y = \"Probability the email is spam\"\n  )\n\n\n\n\n\n\n\n\nThe logistic regression model is a special case of a generalized linear model (glm), so method = \"glm\" is us telling R that we want to do…that. But there are many generalized linear models out there, and so method.args = list(family = binomial) is us telling it we want the logistic one.",
    "crumbs": [
      "Computing primers",
      "Logistic regression"
    ]
  },
  {
    "objectID": "computing/computing-logistic.html#run-a-multiple-logistic-regression-and-plot-the-decision-boundary",
    "href": "computing/computing-logistic.html#run-a-multiple-logistic-regression-and-plot-the-decision-boundary",
    "title": "Logistic regression",
    "section": "Run a “multiple” logistic regression and plot the decision boundary",
    "text": "Run a “multiple” logistic regression and plot the decision boundary\nJust like there is nothing special about a single predictor in linear regression, there is nothing special about a single predictor in logistic regression. To add more predictors to the model, we just add them to the formula (y ~ x) inside fit:\n\ntype_you_capital_fit &lt;- logistic_reg() |&gt;\n  fit(type ~ you + capitalTotal, data = hp_spam)\n  \ntidy(type_you_capital_fit)\n\n# A tibble: 3 × 5\n  term         estimate std.error statistic   p.value\n  &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)  -1.50     0.0554       -27.1 2.97e-162\n2 you           0.361    0.0198        18.3 1.84e- 74\n3 capitalTotal  0.00173  0.000104      16.6 5.66e- 62\n\n\nAfter doing some itchy algebra that you don’t necessarily have to concern yourself with1, we plot a scatter plot of our two predictors, color the points according to the type of the email, and then add a straight line (geom_abline) to visualize the decision boundary when we threshold \\(\\text{Prob}(y=1)\\) at 1/2:\n\n# extract the estimates from their tidy clutches\nb0 &lt;- tidy(type_you_capital_fit)$estimate[1]\nb1 &lt;- tidy(type_you_capital_fit)$estimate[2]\nb2 &lt;- tidy(type_you_capital_fit)$estimate[3]\n\n# set the threshold for classification\np_thresh &lt;- 0.5\n\n# compute intercept and slope of decision boundary\nbd_incpt &lt;- (log(p_thresh / (1 - p_thresh)) - b0) / b2\nbd_slp &lt;- -b1 / b2\n\n# plot that thing\nhp_spam |&gt;\n  mutate(type = if_else(type == 1, \"Spam\", \"Not Spam\")) |&gt;\n  ggplot(aes(x = you, y = capitalTotal, color = type)) +\n  geom_point(alpha = 0.1) +\n  coord_cartesian(xlim = c(0, 6), ylim = c(0, 6000)) + \n  geom_abline(slope = bd_slp, intercept = bd_incpt) + \n  labs(\n    x = \"Percent of words equal to `you`\",\n    y = \"Number of capitalized letters\",\n    title = \"Is this email spam?\"\n  )\n\n\n\n\n\n\n\n\nIf an email has features that place it above the line, we predict that it is a spam email. If an email has features that place it below the line, we predict that it is not. As you can see, this boundary is not perfect – there are red points above and blue points below. But nevertheless, it captures our intuitions pretty well; an email with a high fraction of “you” and many capital letters is more likely spam. If you keep studying statistics and machine learning, you will learn plenty of fancy methods for drawing funky, nonlinear decision boundaries that account for all sorts of contingencies.\nSomething to play around with: adjust the threshold p_thresh and see how it changes the decision boundary.",
    "crumbs": [
      "Computing primers",
      "Logistic regression"
    ]
  },
  {
    "objectID": "computing/computing-logistic.html#make-a-prediction-when-a-new-email-arrives",
    "href": "computing/computing-logistic.html#make-a-prediction-when-a-new-email-arrives",
    "title": "Logistic regression",
    "section": "Make a prediction when a new email arrives",
    "text": "Make a prediction when a new email arrives\nThe scatterplot with the colored points and the linear decision boundary is cute, but if we have more than two predictors, this stuff becomes difficult or impossible to visualize. Fortunately, we do not have to eyeball a picture in order to use this machinery to make decisions.\nIf we get a new email where the frequency of you is 5% and there are 2,500 capital letters, what is the probability that this email is spam?\n\nnew_email &lt;- tibble(\n  you = 5,\n  capitalTotal = 2500\n)\n\npredict(type_you_capital_fit, new_data = new_email, type = \"prob\")\n\n# A tibble: 1 × 2\n  .pred_0 .pred_1\n    &lt;dbl&gt;   &lt;dbl&gt;\n1 0.00963   0.990\n\n\nAccording to our model, the probability of this email being spam is 99%, which is above any normal threshold we might choose. Send it to the trash!",
    "crumbs": [
      "Computing primers",
      "Logistic regression"
    ]
  },
  {
    "objectID": "computing/computing-logistic.html#footnotes",
    "href": "computing/computing-logistic.html#footnotes",
    "title": "Logistic regression",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThis is not hard to derive. Set \\((1+e^{-(\\beta_0+\\beta_1x_1+\\beta_2x_2)})^{-1}\\) equal to your chosen threshold \\(p^\\star\\), and then solve for \\(x_2\\) as a function of \\(x_1\\). You’ll get a line.↩︎",
    "crumbs": [
      "Computing primers",
      "Logistic regression"
    ]
  },
  {
    "objectID": "computing/computing-cheatsheets.html",
    "href": "computing/computing-cheatsheets.html",
    "title": "R cheatsheets",
    "section": "",
    "text": "The following cheatsheets come from https://posit.co/resources/cheatsheets. We haven’t covered every function and functionality listed on them, but you might still find them useful as references.",
    "crumbs": [
      "Computing primers",
      "Cheatsheets"
    ]
  },
  {
    "objectID": "computing/computing-hypotheses-2.html",
    "href": "computing/computing-hypotheses-2.html",
    "title": "More hypothesis testing",
    "section": "",
    "text": "An hypothesis test is a way of determining which of two competing claims a dataset is most consistent with. If, for example, we are investigating whether two experimental groups (treatment and control) have the same probability of exhibiting some characteristic, we state the claims as null and alternative hypotheses:\n\\[\n\\begin{align*}\n\\text{H}_0&:p_{\\text{trt}} = p_{\\text{ctr}}\\\\\n\\text{H}_A&:p_{\\text{trt}} \\neq p_{\\text{ctr}}.\n\\end{align*}\n\\]\nSo the question becomes “do these groups have the same probability, or not?” An alternative way of writing these hypotheses is\n\\[\n\\begin{align*}\n\\text{H}_0&:p_{\\text{trt}} - p_{\\text{ctr}} =0\\\\\n\\text{H}_A&:p_{\\text{trt}} - p_{\\text{ctr}}\\neq 0.\n\\end{align*}\n\\]\nTo answer this question, we must judge how compatible our point estimates \\(\\hat{p}_{\\text{trt}}\\) and \\(\\hat{p}_{\\text{ctr}}\\) are with the null hypothesis, and we do this by calculating a \\(p\\)-value and deciding if it is small enough.\nThis primer leads you down the path of least resistance to calculating the \\(p\\)-value.",
    "crumbs": [
      "Computing primers",
      "More hypothesis testing"
    ]
  },
  {
    "objectID": "computing/computing-hypotheses-2.html#load-packages",
    "href": "computing/computing-hypotheses-2.html#load-packages",
    "title": "More hypothesis testing",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse) \nlibrary(tidymodels) \nlibrary(openintro)",
    "crumbs": [
      "Computing primers",
      "More hypothesis testing"
    ]
  },
  {
    "objectID": "computing/computing-hypotheses-2.html#dataset-is-yawning-contagious",
    "href": "computing/computing-hypotheses-2.html#dataset-is-yawning-contagious",
    "title": "More hypothesis testing",
    "section": "Dataset: is yawning contagious?",
    "text": "Dataset: is yawning contagious?\nAn experiment conducted by the MythBusters tested if a person can be subconsciously influenced into yawning if another person near them yawns. In this study 50 people were randomly assigned to two groups: 34 to a group where a person near them yawned (treatment) and 16 to a control group where they didn’t see someone yawn (control). This is the yawn dataset, pre-loaded in the openintro package:\n\nyawn\n\n# A tibble: 50 × 2\n   result group\n   &lt;fct&gt;  &lt;fct&gt;\n 1 yawn   trmt \n 2 yawn   trmt \n 3 yawn   trmt \n 4 yawn   trmt \n 5 yawn   trmt \n 6 yawn   trmt \n 7 yawn   trmt \n 8 yawn   trmt \n 9 yawn   trmt \n10 yawn   trmt \n# ℹ 40 more rows\n\n\nIn this case, we are interested in a one-sided alternative. We want to know if there is evidence to suggest that proximity to a yawner specifically increases your probability of yawning. So we are testing:\n\\[\n\\begin{align*}\n\\text{H}_0&:p_{\\text{trt}} - p_{\\text{ctr}} =0\\\\\n\\text{H}_A&:p_{\\text{trt}} - p_{\\text{ctr}}&gt; 0.\n\\end{align*}\n\\]\nThe sample proportions in each experimental group are as follows\n\nyawn |&gt;\n  count(group, result) |&gt;\n  group_by(group) |&gt;\n  mutate(p_hat = n / sum(n))\n\n# A tibble: 4 × 4\n# Groups:   group [2]\n  group result       n p_hat\n  &lt;fct&gt; &lt;fct&gt;    &lt;int&gt; &lt;dbl&gt;\n1 ctrl  not yawn    12 0.75 \n2 ctrl  yawn         4 0.25 \n3 trmt  not yawn    24 0.706\n4 trmt  yawn        10 0.294\n\n\nThis code computes the differences in the yawning proportions:\n\nobs_stat_yawner &lt;- yawn |&gt;\n  specify(response = result, explanatory = group, success = \"yawn\") |&gt;\n  calculate(stat = \"diff in props\", order = c(\"trmt\", \"ctrl\"))\nobs_stat_yawner\n\nResponse: result (factor)\nExplanatory: group (factor)\n# A tibble: 1 × 1\n    stat\n   &lt;dbl&gt;\n1 0.0441\n\n\nWe see that there is some positive difference in the proportion of yawners in each group, but how can we tell if it is merely an artifact of random sampling, or if we are actually picking up on a meaningful difference? That is what hypothesis testing is all about.",
    "crumbs": [
      "Computing primers",
      "More hypothesis testing"
    ]
  },
  {
    "objectID": "computing/computing-hypotheses-2.html#simulating-the-null-distribution",
    "href": "computing/computing-hypotheses-2.html#simulating-the-null-distribution",
    "title": "More hypothesis testing",
    "section": "Simulating the null distribution",
    "text": "Simulating the null distribution\nIn order to assess whether or not the estimated difference of 29.4% - 25% = 4.4% is compatible with the hypothesized difference of 0%, we look at the null distribution of the estimator. This is the hypothetical sampling distribution that the sample difference would possess if the null hypothesis happened to be true. When we visualize this distribution, it displays the range of values that our estimate would be likely to have if the null were true. We then compare this to the actual estimate that our actual data actually gave us, and if the actual estimate and the null distribution appear to be incompatible with one another, then we reject the null hypothesis and accept the alternative.\nThe following code simulates the null distribution of the sample difference in proportions:\n\nset.seed(25)\n\nnull_dist_yawner &lt;- yawn |&gt;\n  specify(response = result, explanatory = group, success = \"yawn\") |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 1000, type = \"permute\") |&gt;\n  calculate(stat = \"diff in props\", order = c(\"trmt\", \"ctrl\"))\nnull_dist_yawner\n\nResponse: result (factor)\nExplanatory: group (factor)\nNull Hypothesis: independence\n# A tibble: 1,000 × 2\n   replicate    stat\n       &lt;int&gt;   &lt;dbl&gt;\n 1         1  0.0441\n 2         2 -0.140 \n 3         3 -0.0478\n 4         4 -0.232 \n 5         5  0.0441\n 6         6 -0.232 \n 7         7 -0.0478\n 8         8 -0.0478\n 9         9  0.136 \n10        10 -0.0478\n# ℹ 990 more rows\n\n\nSo this code generates 1,000 fake data sets, and calculates the difference in sample proportions for each one. Here is what each line is doing:\n\nset.seed(25): we are setting the random number seed to ensure that our results are perfectly replicable;\nspecify(response = result, explanatory = group, success = \"yawn\"): first we specify which columns in our data frame we are using. These columns are categorical variables, each with two levels, so next we have to tell the computer which level of the response (“yawn” versus “not yawn”) we are calculating the proportions of;\nhypothesize(null = \"independence\"): since our null hypothesis states that there is no difference between these groups. In other words, the groups are “independent;”\ngenerate(reps = 1000, type = \"permute\"): the null distribution is a sampling distribution, the same as when we studied confidence intervals and bootstrapping. So here we tell it how many alternative, hypothetical datasets we wish to simulate in order to get a pretty-looking histogram. The more the merrier, I say;\ncalculate(stat = \"diff in props\", order = c(\"trmt\", \"ctrl\")): the statistic we are estimating in this case is the difference in group proportions. On Lab 5 we considered other statistics of interest: sample means, medians, standard deviations, and so forth.",
    "crumbs": [
      "Computing primers",
      "More hypothesis testing"
    ]
  },
  {
    "objectID": "computing/computing-hypotheses-2.html#visualizing-the-null-distribution-observed-statistic-and-p-value",
    "href": "computing/computing-hypotheses-2.html#visualizing-the-null-distribution-observed-statistic-and-p-value",
    "title": "More hypothesis testing",
    "section": "Visualizing the null distribution, observed statistic, and \\(p\\)-value",
    "text": "Visualizing the null distribution, observed statistic, and \\(p\\)-value\nHere is the code:\n\nnull_dist_yawner |&gt;\n  visualize() +\n  shade_p_value(obs_stat = obs_stat_yawner, direction = \"greater\")\n\n\n\n\n\n\n\n\nThere are three aspects to it:\n\nWhen we simulate the null distribution of the sample proportion as above, we get a list of 1,000 numbers. To visualize the null distribution, we can simply plot a histogram of these;\nto visualize where the actual estimated difference of 4.4% falls in that distribution, we can add a vertical line;\nThe \\(p\\)-value is the probability, assuming the null is true, of observing a sample statistic as or more extreme then the one we actually got. So it’s the probability under the null of seeing something even farther to the right of 4.4%. The fraction of the area shaded red is the \\(p\\)-value. We say direction = \"greater\" because our alternative hypothesis is one-sided.\n\nInspecting the picture, we see that our estimated difference is pretty much smack in the middle of the null distribution. So the null hypothesis of no difference is very much in play. Evidence is insufficient to reject it.",
    "crumbs": [
      "Computing primers",
      "More hypothesis testing"
    ]
  },
  {
    "objectID": "computing/computing-hypotheses-2.html#computing-the-p-value",
    "href": "computing/computing-hypotheses-2.html#computing-the-p-value",
    "title": "More hypothesis testing",
    "section": "Computing the p-value",
    "text": "Computing the p-value\nTo quantify that intuition, this code calculates the \\(p\\)-value:\n\nnull_dist_yawner |&gt;\n  get_p_value(obs_stat = obs_stat_yawner, \"greater\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.522\n\n\nThe interpretation of this number is “if there were in fact no difference between treatment and control, there is a 52.2% chance that we would observe a difference even greater than the one we actually observed when we ran the experiment.” This p-value is above every standard discernibility level, and so we fail to reject the null. If the null hypothesis were indeed true, the results we got would not be super surprising, so we cannot rule the null out.",
    "crumbs": [
      "Computing primers",
      "More hypothesis testing"
    ]
  },
  {
    "objectID": "computing/computing-hypotheses.html",
    "href": "computing/computing-hypotheses.html",
    "title": "Hypothesis testing with randomization",
    "section": "",
    "text": "An hypothesis test is a way of determining which of two competing claims a dataset is most consistent with. If, for example, we are investigating the value of a true but unknown probability \\(p\\), we state the claims as null and alternative hypotheses:\n\\[\n\\begin{align*}\n\\text{H}_0&:p=p_0\\\\\n\\text{H}_A&:p\\neq p_0\n\\end{align*}\n\\] So the question becomes “is the true value equal to the hypothesized value, or is it not?” To answer this, we must judge how compatible our point estimate \\(\\hat{p}\\) is with \\(p_0\\), and we do this by calculating a \\(p\\)-value and deciding if it is small enough.\nThis primer leads you down the path of least resistance to calculating the \\(p\\)-value.",
    "crumbs": [
      "Computing primers",
      "Hypothesis testing"
    ]
  },
  {
    "objectID": "computing/computing-hypotheses.html#load-packages",
    "href": "computing/computing-hypotheses.html#load-packages",
    "title": "Hypothesis testing with randomization",
    "section": "Load packages",
    "text": "Load packages\n\nlibrary(tidyverse) \nlibrary(tidymodels) \nlibrary(openintro)",
    "crumbs": [
      "Computing primers",
      "Hypothesis testing"
    ]
  },
  {
    "objectID": "computing/computing-hypotheses.html#dataset-medical-consultants-for-organ-donors",
    "href": "computing/computing-hypotheses.html#dataset-medical-consultants-for-organ-donors",
    "title": "Hypothesis testing with randomization",
    "section": "Dataset: medical consultants for organ donors",
    "text": "Dataset: medical consultants for organ donors\nSome prospective organ donors hire a medical consultant to advise and assist in aspects of the surgery. We would hope that, by hiring this person, we decrease the risk of complications. If we did not hire anybody and just went in blind, the average complication rate for liver donor surgeries in the US is known to be about 10%.\nSo say we are interviewing a medical consultant, and we must decide whether or not to hire her. Some of her clients will experience complications, and some will not. Intrinsic to her is her personal “complication rate” \\(p\\) – the probability that a given client of hers experiences complications. If her complication rate is 10%, we are wasting your money hiring her because she is no better than if we went in blind. If her complication rate is greater than 10%, that’s even worse. If her complication rate is less than 10%, then great! Hire her.\nSo we are interested in these hypotheses:\n\\[\n\\begin{align*}\n\\text{H}_0&:p=0.10\\\\\n\\text{H}_A&:p\\neq 0.10.\n\\end{align*}\n\\] In order to evaluate these competing claims about the effectiveness of the consultant, we have historical data on her track record:\n\norgan_donor &lt;- read_csv(\"organ-donor.csv\")\norgan_donor\n\n# A tibble: 62 × 1\n   outcome        \n   &lt;chr&gt;          \n 1 complication   \n 2 complication   \n 3 complication   \n 4 no complication\n 5 no complication\n 6 no complication\n 7 no complication\n 8 no complication\n 9 no complication\n10 no complication\n# ℹ 52 more rows\n\n\nShe has advised 62 liver donors in her career, and this code calculates the proportion of those donors that experienced complications:\n\nobs_stat_donor &lt;- organ_donor |&gt;\n  specify(response = outcome, success = \"complication\") |&gt;\n  calculate(stat = \"prop\")\n\nobs_stat_donor\n\nResponse: outcome (factor)\n# A tibble: 1 × 1\n    stat\n   &lt;dbl&gt;\n1 0.0484\n\n\nSo 3 out her 62 clients, or about 4.8%, have experienced complications. 4.8% is smaller than 10%, so she would appear to be better than average and worth hiring. But 4.8% is a noisy estimate based on imperfect data, and we might wonder if we have enough information to confidently conclude that she is better than average. Maybe these results are simply due to chance, and if instead she had worked with 200 clients, it would become clear that her skills actually correspond to 9% complications, or 11% complications. How can we tell with the information we have? That’s what hypothesis testing is all about.",
    "crumbs": [
      "Computing primers",
      "Hypothesis testing"
    ]
  },
  {
    "objectID": "computing/computing-hypotheses.html#simulating-the-null-distribution",
    "href": "computing/computing-hypotheses.html#simulating-the-null-distribution",
    "title": "Hypothesis testing with randomization",
    "section": "Simulating the null distribution",
    "text": "Simulating the null distribution\nIn order to assess whether or not the point estimate of 4.8% is compatible with the hypothesized value of 10%, we look at the null distribution of the estimator. This is the hypothetical sampling distribution that the sample proportion would possess if the null hypothesis happened to be true. When we visualize this distribution, it displays the range of values that our estimate would be likely to have if the null were true. We then compare this to the actual estimate that our actual data actually gave us, and if the actual estimate and the null distribution appear to be incompatible with one another, then we reject the null hypothesis and accept the alternative.\nThe following code simulates the null distribution of the sample proportion:\n\nset.seed(25)\n\nnull_dist_donor &lt;- organ_donor |&gt;\n  specify(response = outcome, success = \"complication\") |&gt;\n  hypothesize(null = \"point\", p = 0.10) |&gt; \n  generate(reps = 15000, type = \"draw\") |&gt; \n  calculate(stat = \"prop\")\nnull_dist_donor\n\nResponse: outcome (factor)\nNull Hypothesis: point\n# A tibble: 15,000 × 2\n   replicate   stat\n       &lt;int&gt;  &lt;dbl&gt;\n 1         1 0.0645\n 2         2 0.0484\n 3         3 0.113 \n 4         4 0.0806\n 5         5 0.145 \n 6         6 0.145 \n 7         7 0.113 \n 8         8 0.0806\n 9         9 0.113 \n10        10 0.145 \n# ℹ 14,990 more rows\n\n\nSo this code generates 15,000 fake data sets, and calculates the sample proportion of “complications” for each one. Here is what each line is doing:\n\nset.seed(25): we are setting the random number seed to ensure that our results are perfectly replicable;\nspecify(response = outcome, success = \"complication\"): first we specify which column in our data frame we are using, which is easy in this case because there is only one: outcome. That column is a categorical variable with two levels, so next we have to tell the computer which level (“complication” versus “no complication”) we are calculating the proportions of;\nhypothesize(null = \"point\", p = 0.10): since our null hypothesis is checking a single hypothesized value (as opposed to many), we have a point null hypothesis, and then we tell it the hypothesized value – 10% in this case;\ngenerate(reps = 15000, type = \"draw\"): the null distribution is a sampling distribution, the same as when we studied confidence intervals and bootstrapping. So here we tell it how many alternative, hypothetical datasets we wish to simulate in order to get a pretty-looking histogram. The more the merrier, I say;\ncalculate(stat = \"prop\"): the statistic we are estimate is the sample proportion in this case. On Lab 5 we considered other statistics of interest: sample means, medians, standard deviations, and so forth.",
    "crumbs": [
      "Computing primers",
      "Hypothesis testing"
    ]
  },
  {
    "objectID": "computing/computing-hypotheses.html#visualizing-the-null-distribution-and-observed-statistic",
    "href": "computing/computing-hypotheses.html#visualizing-the-null-distribution-and-observed-statistic",
    "title": "Hypothesis testing with randomization",
    "section": "Visualizing the null distribution and observed statistic",
    "text": "Visualizing the null distribution and observed statistic\nWhen we simulate the null distribution of the sample proportion as above, we get a list of 15,000 numbers. To visualize the null distribution, we can simply plot a histogram of these, and to visualize where the actual estimate of 4.8% falls in that distribution, we can add a vertical line:\n\nggplot(null_dist_donor, aes(x = stat)) +\n  geom_histogram(bins = 15, color = \"white\") +\n  geom_vline(xintercept = obs_stat_donor$stat, color = \"red\", linewidth = 2)\n\n\n\n\n\n\n\n\nInspecting the picture, we see that our estimate is not smack in the middle of the null distribution, nor is it far out in the tails away from any of the values we simulated. So eyeballing the picture will not lead us to a firm conclusion. We need something quantitatively more precise.",
    "crumbs": [
      "Computing primers",
      "Hypothesis testing"
    ]
  },
  {
    "objectID": "computing/computing-hypotheses.html#visualizing-the-p-value",
    "href": "computing/computing-hypotheses.html#visualizing-the-p-value",
    "title": "Hypothesis testing with randomization",
    "section": "Visualizing the p-value",
    "text": "Visualizing the p-value\nThe \\(p\\)-value is the probability, assuming the null is true, of observing a sample statistic as or more extreme then the one we actually got. So it’s the probability under the null of seeing something even farther to the left of 4.8%, or something correspondingly far to the right. Here’s what that looks like:\n\nnull_dist_donor |&gt;\n  visualize() +\n  shade_p_value(obs_stat = obs_stat_donor, direction = \"two-sided\")\n\n\n\n\n\n\n\n\nThe fraction of the area shaded red is the \\(p\\)-value.",
    "crumbs": [
      "Computing primers",
      "Hypothesis testing"
    ]
  },
  {
    "objectID": "computing/computing-hypotheses.html#computing-the-p-value",
    "href": "computing/computing-hypotheses.html#computing-the-p-value",
    "title": "Hypothesis testing with randomization",
    "section": "Computing the p-value",
    "text": "Computing the p-value\nThis code calculates the \\(p\\)-value:\n\nnull_dist_donor |&gt;\n  get_p_value(obs_stat = obs_stat_donor, direction = \"two-sided\")\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1   0.241\n\n\nThe \\(p\\)-value is quantifying how far out our estimate is in the tails of the null distribution. If the estimate is far out in the tails, the \\(p\\)-value will be small, and this provides evidence that the estimate and the null are incompatible, and therefore the null should be rejected in favor of the alternative. If the estimate is closer to the middle of the null distribution, the \\(p\\)-value will be large, and this means that we cannot rule out the null hypothesis.\nIn this case, we calculated a \\(p\\)-value of 24.1%. Is that small enough to jump to a conclusion? It ain’t 80%. It’s less than 50%. But it ain’t 1% either. How small is small? We will pick this question up in lecture.",
    "crumbs": [
      "Computing primers",
      "Hypothesis testing"
    ]
  },
  {
    "objectID": "computing/computing-colors.html",
    "href": "computing/computing-colors.html",
    "title": "Colors",
    "section": "",
    "text": "We’ve seen many examples of plots in which we make use of the “color” and “fill” arguments to map unique values of some categorical variable to different colors in a resultant plot. Below are two examples, for reference, using Fisher’s Iris data (iris).\n\nlibrary(ggplot2)\n\niris |&gt;\n  ggplot(aes(x = Sepal.Length, y = Petal.Length, color = Species)) +\n  geom_point() +\n  labs(x = \"Sepal Length\",\n       y = \"Petal Length\",\n       color = \"Species\",\n       title = \"Scatterplot of Sepal Length vs. Petal Length by Species\")\n\n\n\n\n\n\n\nmean_value &lt;- mean(iris$Petal.Length)\n\niris |&gt;\n  ggplot(aes(x = Petal.Length, fill = Species)) +\n  geom_histogram(bins = 25, alpha = 0.7, position = \"identity\") +\n  geom_vline(xintercept = mean_value, color = \"red\") + \n  labs(x = \"Petal Length\",\n       y = \"Count\",\n       fill = \"Species\",\n       title = \"Histogram of Petal Length by Species\")",
    "crumbs": [
      "Computing primers",
      "Colors"
    ]
  },
  {
    "objectID": "computing/computing-colors.html#coloring-in-r",
    "href": "computing/computing-colors.html#coloring-in-r",
    "title": "Colors",
    "section": "",
    "text": "We’ve seen many examples of plots in which we make use of the “color” and “fill” arguments to map unique values of some categorical variable to different colors in a resultant plot. Below are two examples, for reference, using Fisher’s Iris data (iris).\n\nlibrary(ggplot2)\n\niris |&gt;\n  ggplot(aes(x = Sepal.Length, y = Petal.Length, color = Species)) +\n  geom_point() +\n  labs(x = \"Sepal Length\",\n       y = \"Petal Length\",\n       color = \"Species\",\n       title = \"Scatterplot of Sepal Length vs. Petal Length by Species\")\n\n\n\n\n\n\n\nmean_value &lt;- mean(iris$Petal.Length)\n\niris |&gt;\n  ggplot(aes(x = Petal.Length, fill = Species)) +\n  geom_histogram(bins = 25, alpha = 0.7, position = \"identity\") +\n  geom_vline(xintercept = mean_value, color = \"red\") + \n  labs(x = \"Petal Length\",\n       y = \"Count\",\n       fill = \"Species\",\n       title = \"Histogram of Petal Length by Species\")",
    "crumbs": [
      "Computing primers",
      "Colors"
    ]
  },
  {
    "objectID": "computing/computing-colors.html#the-wizarding-world-of-r",
    "href": "computing/computing-colors.html#the-wizarding-world-of-r",
    "title": "Colors",
    "section": "The Wizarding World of R",
    "text": "The Wizarding World of R\nAh, the wonders of the R community and open source software. If you have some extra time on your hands and want to get extra creative, I encourage you explore the wide world of R and search around for off-the-shelf themed color palettes created by other R fanatics for public consumption. With a bit of Googling, you can find some pretty cool themed palettes (F1, Barbie, Oppenheimer, GOT, to name a few). Visit this site and check out the code below for an example of Harry Potter themed plots!\n\nlibrary(harrypotter)\n\niris |&gt;\n  ggplot(aes(x = Sepal.Length, y = Petal.Length, color = Species)) +\n  geom_point() +\n  scale_color_hp_d(option = \"HarryPotter\", name = \"Species\") +\n  labs(x = \"Sepal Length\",\n       y = \"Petal Length\",\n       color = \"Species\",\n       title = \"Scatterplot of Sepal Length vs. Petal Length by Species\")\n\n\n\n\n\n\n\niris |&gt;\n  ggplot(aes(x = Petal.Length, fill = Species)) +\n  geom_histogram(bins = 25, alpha = 0.7, position = \"identity\") +\n  geom_vline(xintercept = mean_value, color = \"black\") + \n  scale_fill_hp_d(option = \"HermioneGranger\", name = \"Species\") +\n  labs(x = \"Petal Length\",\n       y = \"Count\",\n       fill = \"Species\",\n       title = \"Histogram of Petal Length by Species\")",
    "crumbs": [
      "Computing primers",
      "Colors"
    ]
  },
  {
    "objectID": "computing/computing-moving.html",
    "href": "computing/computing-moving.html",
    "title": "Moving files around",
    "section": "",
    "text": "We will distribute files to you (datasets, Quarto documents, etc) by placing them in the Files section of the course Canvas page:\n\nFrom there, you will download them to your computer. The best way to do this is by following the “three dots” on the right hand side of the file name:\n\nOnce you have downloaded a file to your computer, you can then upload it to your RStudio instance using the yellow button in the Files tab in the lower right:\n\nIt may be the case that you do not see your uploaded file right away. Try clicking the Home button to “refresh” the file list:\n\nThe Files section in your RStudio instance is like a lil’ Dropbox or Google Drive account that you can use to store your STA101-related files during the semester. I recommend keeping it organized with folders, like this:\n\nThis button creates new folders:\n\nTo move files between the folders, check the box to the left of the file you want to move, and go here:",
    "crumbs": [
      "Computing primers",
      "Moving files around"
    ]
  },
  {
    "objectID": "computing/computing-pipe.html",
    "href": "computing/computing-pipe.html",
    "title": "Piping",
    "section": "",
    "text": "A pipe in R is a way of stitching together many commands to make your code easier for a human to read, and to keep it from running off the pages of the PDF documents you will submit to us.",
    "crumbs": [
      "Computing primers",
      "Piping"
    ]
  },
  {
    "objectID": "computing/computing-pipe.html#a-very-silly-example",
    "href": "computing/computing-pipe.html#a-very-silly-example",
    "title": "Piping",
    "section": "A very silly example",
    "text": "A very silly example\nThese two code chunks do exactly the same thing:\n\nsum(1, 2)\n\n[1] 3\n\n\n\n1 |&gt;\n  sum(2)\n\n[1] 3\n\n\nSo the pipe operator |&gt; passes (or pipes), the number 1 into the sum function as the first input. In a simple example like this, sum(1, 2) is definitely the way I would write the code, but as things get more elaborate, you will want to pipe.",
    "crumbs": [
      "Computing primers",
      "Piping"
    ]
  },
  {
    "objectID": "computing/computing-pipe.html#tallying-stuff-up-in-a-spreadsheet",
    "href": "computing/computing-pipe.html#tallying-stuff-up-in-a-spreadsheet",
    "title": "Piping",
    "section": "Tallying stuff up in a spreadsheet",
    "text": "Tallying stuff up in a spreadsheet\nLet’s consider the COVID data set from class on 9/3/2024:\n\nlibrary(tidyverse)\n\ndelta &lt;- read_csv(\"delta.csv\")\n\n\n\n\n\n\n\nNote\n\n\n\nRecalling the information here, be prepared to adjust the file path to match how you have organized your files and folders.\n\n\nA row in this data set is a person, and we record whether or not that person died from/with COVID, and whether or not they were vaccinated. So there are four categories in all:\n\nunvaccinated and died\nunvaccinated and survived\nvaccinated and died\nvaccinated and survived\n\nThe following code creates a nifty lil’ table that tallies up the number of people in each group and calculates the proportion of people that died versus survived within each vaccination group:\n\ndelta |&gt;\n  count(vaccine, outcome) |&gt;\n  group_by(vaccine) |&gt;\n  mutate(prop = n / sum(n))\n\n# A tibble: 4 × 4\n# Groups:   vaccine [2]\n  vaccine      outcome       n    prop\n  &lt;chr&gt;        &lt;chr&gt;     &lt;int&gt;   &lt;dbl&gt;\n1 Unvaccinated died        250 0.00166\n2 Unvaccinated survived 150802 0.998  \n3 Vaccinated   died        477 0.00407\n4 Vaccinated   survived 116637 0.996  \n\n\nSo within each vaccination group, the numbers sum to one. This code is equivalent, but it provides a truly horrific reading experience:\n\n  mutate(group_by(count(delta, vaccine, outcome), vaccine), prop = n / sum(n))\n\n# A tibble: 4 × 4\n# Groups:   vaccine [2]\n  vaccine      outcome       n    prop\n  &lt;chr&gt;        &lt;chr&gt;     &lt;int&gt;   &lt;dbl&gt;\n1 Unvaccinated died        250 0.00166\n2 Unvaccinated survived 150802 0.998  \n3 Vaccinated   died        477 0.00407\n4 Vaccinated   survived 116637 0.996",
    "crumbs": [
      "Computing primers",
      "Piping"
    ]
  },
  {
    "objectID": "computing/computing-pipe.html#now-you-try",
    "href": "computing/computing-pipe.html#now-you-try",
    "title": "Piping",
    "section": "Now you try",
    "text": "Now you try\nThe COVID data contains another variable indicating whether or not the person was older or younger than fifty. Write some code (it will be very similar to the code above) that produces a table that breaks things down by vaccination status, outcome, and age. How many rows should this table contain?",
    "crumbs": [
      "Computing primers",
      "Piping"
    ]
  },
  {
    "objectID": "computing/computing-proportion.html",
    "href": "computing/computing-proportion.html",
    "title": "Worked example: estimating a single proportion",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)",
    "crumbs": [
      "Computing primers",
      "Inference for one proportion"
    ]
  },
  {
    "objectID": "computing/computing-proportion.html#packages",
    "href": "computing/computing-proportion.html#packages",
    "title": "Worked example: estimating a single proportion",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)",
    "crumbs": [
      "Computing primers",
      "Inference for one proportion"
    ]
  },
  {
    "objectID": "computing/computing-proportion.html#data",
    "href": "computing/computing-proportion.html#data",
    "title": "Worked example: estimating a single proportion",
    "section": "Data",
    "text": "Data\nIn a survey conducted by Survey USA between September 30, 2023 and October 3, 2023, 2759 registered voters from all 50 US states were asked\n\nAmerica will hold an election for President of the United States next November. Not everyone makes the time to vote in every election. Which best describes you? Are you certain to vote? Will you probably vote? Are the chances you will vote about 50/50? Or will you probably not vote?\n\nThe data from this survey can be found in voting-survey.csv:\n\nvoting_survey &lt;- read_csv(\"voting-survey.csv\")\n\nRows: 2759 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): vote\ndbl (1): id\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nglimpse(voting_survey)\n\nRows: 2,759\nColumns: 2\n$ id   &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19…\n$ vote &lt;chr&gt; \"Certain to vote\", \"Certain to vote\", \"Certain to vote\", \"Certain…",
    "crumbs": [
      "Computing primers",
      "Inference for one proportion"
    ]
  },
  {
    "objectID": "computing/computing-proportion.html#visualize-and-summarize-the-data",
    "href": "computing/computing-proportion.html#visualize-and-summarize-the-data",
    "title": "Worked example: estimating a single proportion",
    "section": "Visualize and summarize the data",
    "text": "Visualize and summarize the data\nThis simple bar plot displays the number of respondents who gave each of the five possible answers:\n\nggplot(voting_survey, aes(y = vote)) + \n  geom_bar()\n\n\n\n\n\n\n\n\nThis code calculates the proportion of respondents that gave each answer:\n\nvoting_survey |&gt;\n  count(vote) |&gt;\n  mutate(props = n / sum(n))\n\n# A tibble: 5 × 3\n  vote                       n  props\n  &lt;chr&gt;                  &lt;int&gt;  &lt;dbl&gt;\n1 About 50/50 chance       293 0.106 \n2 Certain to vote         1921 0.696 \n3 Not sure                  44 0.0159\n4 Probably will not vote    92 0.0333\n5 Will probably vote       409 0.148",
    "crumbs": [
      "Computing primers",
      "Inference for one proportion"
    ]
  },
  {
    "objectID": "computing/computing-proportion.html#interval-estimation",
    "href": "computing/computing-proportion.html#interval-estimation",
    "title": "Worked example: estimating a single proportion",
    "section": "Interval estimation",
    "text": "Interval estimation\nBased on these data, we want to estimate the true proportion of registered US voters who are certain to vote in the next presidential election. Our point estimate based on the survey responses is 69.6%, but how reliably estimated is that? To answer this, we need to construct a confidence interval.\n\n# Prepare the data for analysis (pre-processing)\n\nvoting_survey &lt;- voting_survey |&gt;\n  mutate(vote = if_else(vote == \"Certain to vote\", \"Certain to vote\", \"Not certain to vote\"))\n\nset.seed(20)\n\nboot_dist &lt;- voting_survey |&gt;\n  specify(response = vote, success = \"Certain to vote\") |&gt;\n  generate(reps = 1000, type = \"bootstrap\") |&gt;\n  calculate(stat = \"prop\")\n\nci &lt;- boot_dist |&gt;\n  get_ci()\nci\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1    0.679    0.714\n\nvisualize(boot_dist) + \n  shade_ci(ci)",
    "crumbs": [
      "Computing primers",
      "Inference for one proportion"
    ]
  },
  {
    "objectID": "computing/computing-proportion.html#hypothesis-testing",
    "href": "computing/computing-proportion.html#hypothesis-testing",
    "title": "Worked example: estimating a single proportion",
    "section": "Hypothesis testing",
    "text": "Hypothesis testing\nPew says that 66% of eligible US voters turned out for the 2020 presidential election. A newspaper claims that even more people will turnout in 2024, and cites this survey as evidence. Do these data provide convincing evidence for this claim?\nThe formal hypotheses are\n\\[\n\\begin{aligned}\n\\text{H}_0: p&=0.66\\\\\n\\text{H}_A: p&&gt;0.66,\n\\end{aligned}\n\\] where \\(p\\) is the true proportion that say they are guaranteed to turn out in 2024.\n\n# compute the observed proportion\n\nobs_stat &lt;- voting_survey |&gt;\n    specify(response = vote, success = \"Certain to vote\") |&gt;\n    calculate(stat = \"prop\")\n\n# simulate the null distribution\n\nset.seed(525600)\n\nnull_dist &lt;- voting_survey |&gt;\n  specify(response = vote, success = \"Certain to vote\") |&gt;\n  hypothesize(null = \"point\", p = 0.66) |&gt;\n  generate(reps = 1000, type = \"draw\") |&gt;\n  calculate(stat = \"prop\")\n\n# visualize and compute the p-value\n\nnull_dist |&gt;\n  visualize() +\n  shade_p_value(obs_stat = obs_stat, direction = \"greater\")\n\n\n\n\n\n\n\nnull_dist |&gt;\n  get_p_value(obs_stat, direction = \"greater\")\n\nWarning: Please be cautious in reporting a p-value of 0. This result is an approximation\nbased on the number of `reps` chosen in the `generate()` step.\nℹ See `get_p_value()` (`?infer::get_p_value()`) for more information.\n\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1       0\n\n\nOur p-value is approximately zero, which is below every standard discernibility level, and therefore we reject the null. The interpretation is:\n\nIf the null were true and there were just as many certain voters in 2023 as in 2020, then there is a 0% chance we would get an estimate as or more extreme (greater) than the one we actually got. As such, the null is probably a bunch of balderdash.",
    "crumbs": [
      "Computing primers",
      "Inference for one proportion"
    ]
  },
  {
    "objectID": "project/project.html",
    "href": "project/project.html",
    "title": "Final project",
    "section": "",
    "text": "In this project, you and your teammates will select a data set that you are interested in studying and compose an original data analysis. This will culminate in the submission of two things:",
    "crumbs": [
      "Project"
    ]
  },
  {
    "objectID": "project/project.html#proposal-feedback",
    "href": "project/project.html#proposal-feedback",
    "title": "Final project",
    "section": "Proposal feedback",
    "text": "Proposal feedback\nWe will review your project proposals and give you feedback before Thanksgiving recess. We will comment about feasibility of each proposal, and gently nudge you in a particular direction. We will also advise you about technical hurdles we anticipate that you will encounter, and give you tools for tackling them. This may involve coaching you on how to implement computational or statistical methods that go beyond the things we teach in this course, cleaning your data, loading an excel sheet into R, etc.\nBased on this feedback, you will make the final decision about which data set to use, and you will proceed to complete the remainder of your project.",
    "crumbs": [
      "Project"
    ]
  },
  {
    "objectID": "project/project.html#criteria-for-datasets",
    "href": "project/project.html#criteria-for-datasets",
    "title": "Final project",
    "section": "Criteria for datasets",
    "text": "Criteria for datasets\nThe data sets should meet the following criteria:\n\nAt least 100 observations\nAt least 5 columns\nAt least 4 of the columns must be useful and unique explanatory variables.\n\nIdentifier variables such as “name”, “social security number”, etc. are not useful explanatory variables.\nIf you have multiple columns with the same information (e.g. “state abbreviation” and “state name”), then they are not unique explanatory variables.\n\nYou may not use data that has previously been used in any course materials, or any derivation of data that has been used in course materials.\n\n\n\n\n\n\n\nTip\n\n\n\nPlease ask a member of the teaching team if you’re unsure whether your data set meets the criteria.\n\n\nIf you set your hearts on a dataset that has fewer observations or variables than what’s suggested here, that might still be ok; use these numbers as guidance for a successful proposal, not as minimum requirements.",
    "crumbs": [
      "Project"
    ]
  },
  {
    "objectID": "project/project.html#resources",
    "href": "project/project.html#resources",
    "title": "Final project",
    "section": "Resources for datasets",
    "text": "Resources for datasets\nYou can find data wherever you like, but here are some recommendations to get you started. You shouldn’t feel constrained to datasets that are already in a tidy format, you can start with data that needs cleaning and tidying, scrape data off the web, or collect your own data.\n\nUNICEF Data\nYouth Risk Behavior Surveillance System (YRBSS)\nGoogle Dataset Search\nData is Plural\nElection Studies\nUS Census Data\nWorld Bank Data\nCDC\nEuropean Statistics\nCORGIS: The Collection of ReallyGreat, Interesting, Situated Datasets\nGeneral Social Survey\nHarvard Dataverse\nInternational Monetary Fund\nIPUMS survey data from around the world\nLos Angeles Open Data\nNHS Scotland Open Data\nNYC OpenData\nOpen access to Scotland’s official statistics\nPew Research\nPRISM Data Archive Project\nResponsible Datasets in Context\nStatistics Canada\nTidyTuesday\nThe National Bureau of Economic Research\nUCI Machine Learning Repository\nUK Government Data\nUnited Nations Data\nUnited Nations Statistics Division\nUS Government Data\nFRED Economic Data\nData.gov\nAwesome public datasets\nDurham Open Data Portal\nEdinburgh Open Data\nFiveThirtyEight",
    "crumbs": [
      "Project"
    ]
  },
  {
    "objectID": "project/project.html#components",
    "href": "project/project.html#components",
    "title": "Final project",
    "section": "Components",
    "text": "Components\nYou should include, at a minimum, the following sections in your report.\n\nIntroduction (7 pts)\nThe introduction provides motivation and context for your research.\nTo begin, introduce the data set in a few short sentences. Next, create a code book (aka a “data dictionary”) of the variables in the data set. Although a code book is provided above, you should include one in your report as well so that your report is self-contained. Specifically, only include in your report a code book of the variables that you use.\nComplete the introduction by providing a concise, clear statement of your research question and hypotheses. Be sure to motivate why the research question is interesting/useful.\nExample research question and hypotheses (if we were predicting penguin weights instead of baby weights):\nCan we predict body mass with bill depth? We hypothesize that penguins with deeper bills will also have more mass.\n\n\nMethodology (15 pts)\nHere you should introduce any statistical methods you use and describe why you choose the methods you do to answer your question. You might also include any preliminary summary statistics or figures you use to explore the data.\n\n\nResults (15 pts)\nPlace figure(s) here to illustrate the main results from your analysis. 1 beautiful figure is worth more than several poorly formatted figures. You must have at least 1 figure.\nProvide only the main results from your analysis. The goal is not to do an exhaustive data analysis (calculate every possible statistic and create every possible model for all variables). Rather, you should demonstrate that you are proficient at asking meaningful questions and answering them using data, that you are skilled in writing about and interpreting results, and that you can accomplish these tasks using R. More is not better.\n\n\nDiscussion (6 pts)\nThis section is a conclusion and discussion. You should\n\nSummarize your main finding in a sentence or two.\nDiscuss your finding and why it is useful (put in the context of your motivation from the introduction).\nCritique your own analyses and include a brief paragraph on what you would do differently if you were able to start the project over.\n\n\n\nAppendix (2 pts)\nList a brief (1 or 2 sentence) summary of the relative contributions of each team member, e.g., “Aang built the models, Katara implemented them in R, and Sokka wrote the introduction and discussion.”\n\n\n\n\n\n\nImportant\n\n\n\nAll team members should be comfortable describing all aspects of the project and understanding all code.\n\n\n\n\nFormatting (5 pts)\nYour project should be professionally formatted. For example, this means labeling graphs and figures, turning off code chunks, using proper citations and cross-references, and following typical style guidelines.",
    "crumbs": [
      "Project"
    ]
  },
  {
    "objectID": "project/project.html#submission",
    "href": "project/project.html#submission",
    "title": "Final project",
    "section": "Submission",
    "text": "Submission\n\nSelect one team member to upload the team’s PDF submission to Gradescope.\nBe sure to select every team member’s name in Gradescope.\nAssociate all pages with “Full report”.",
    "crumbs": [
      "Project"
    ]
  },
  {
    "objectID": "project/project.html#slides",
    "href": "project/project.html#slides",
    "title": "Final project",
    "section": "Slides",
    "text": "Slides\nFor your presentation, you must create presentation slides that summarize and showcase your project. Introduce your research question and data set, showcase visualizations, and provide some conclusions. These slides should serve as a brief visual accompaniment to your write-up and will be graded for content and quality.\nHere is a suggested outline as you think through the slides; you do not have to use this exact format for the slide deck.\n\nTitle Slide\nSlide 1: Introduce the topic and motivation\nSlide 2: Introduce the data\nSlide 3 - 4: Highlights from exploratory data analysis\nSlide 4 - 5: Highlights from inference and/or modeling\nSlide 6: Conclusions + critique/shortcomings\n\nYou can create your slides with any software you like (Keynote, PowerPoint, Google Slides, etc.). We recommend choosing an option that’s easy to collaborate with, e.g., Google Slides.\n\n\n\n\n\n\nNote\n\n\n\nYou can also use Quarto to make your slides! While we won’t be covering making slides with Quarto in the class, we would be happy to help you with it in office hours. It’s no different than writing other documents with Quarto, so the learning curve will not be steep!",
    "crumbs": [
      "Project"
    ]
  },
  {
    "objectID": "project/project.html#recording",
    "href": "project/project.html#recording",
    "title": "Final project",
    "section": "Recording",
    "text": "Recording\nPresentations will be submitted as a pre-recorded video by the due date.\nFor recording, you may use any platform that works best for your group to record your presentation. Below are a few resources on recording videos:\n\nRecording presentations in Zoom\nApple Quicktime for screen recording\nWindows 10 built-in screen recording functionality\nKap for screen recording\n\nOnce your video is ready, upload the video to Warpwire in Canvas or another video platform (e.g., YouTube). To upload your video to Warpwire:\n\nClick the Warpwire tab in the course Canvas site.\nClick the “+” and select “Upload files”.\nLocate the video on your computer and click to upload.\nOnce you’ve uploaded the video to Warpwire, click to share the video and copy the video’s URL.\n\nOnce you have URL for the video (from YouTube, Warpwire, whatever), you will send it to us in a format TBD.",
    "crumbs": [
      "Project"
    ]
  },
  {
    "objectID": "project/project.html#grading-summary",
    "href": "project/project.html#grading-summary",
    "title": "Final project",
    "section": "Grading summary",
    "text": "Grading summary\nGrading of the project will take into account the following:\n\nContent - What is the quality of research and/or policy question and relevancy of data to those questions?\nCorrectness - Are statistical procedures carried out and explained correctly?\nWriting and Presentation - What is the quality of the statistical presentation, writing, and explanations?\nCreativity and Critical Thought - Is the project carefully thought out? Are the limitations carefully considered? Does it appear that time and effort went into the planning and implementation of the project?\n\nA general breakdown of scoring is as follows:\n\n90%-100%: Outstanding effort. Student understands how to apply all statistical concepts, can put the results into a cogent argument, can identify weaknesses in the argument, and can clearly communicate the results to others.\n80%-89%: Good effort. Student understands most of the concepts, puts together an adequate argument, identifies some weaknesses of their argument, and communicates most results clearly to others.\n70%-79%: Passing effort. Student has misunderstanding of concepts in several areas, has some trouble putting results together in a cogent argument, and communication of results is sometimes unclear.\n60%-69%: Struggling effort. Student is making some effort, but has misunderstanding of many concepts and is unable to put together a cogent argument. Communication of results is unclear.\nBelow 60%: Student is not making a sufficient effort.",
    "crumbs": [
      "Project"
    ]
  },
  {
    "objectID": "project/project.html#late-work-policy",
    "href": "project/project.html#late-work-policy",
    "title": "Final project",
    "section": "Late work policy",
    "text": "Late work policy\nBe sure to turn in your work early to avoid any technological mishaps.\n\n\n\n\n\n\nWarning\n\n\n\nThere is no late work accepted on this project.",
    "crumbs": [
      "Project"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA 101 Data Analysis and Statistical Inference",
    "section": "",
    "text": "Below is a prospective outline for the course, but things may change with advanced notice:\n\n\n\n\n\n\n\n\nWEEK\nDATE\nTOPIC\nPREPARE\nMATERIALS\nDUE\n\n\n\n\n1\nTue, Aug 27\nWelcome!\n\n\n\n\n\n\n\n\n\n\nThu, Aug 29\nGetting started\n📖 ims: Ch 1  💻 tutorial: 01-data-01\nlecture-2  (filled-in)\nGetting to know you @ 3PM\n\n\n\n\nFri, Aug 30\nLab 1: Hello R!\n\n\nlab-1\n\n\n\n\n2\nTue, Sep 3\nNumerical data and study design\n📖 ims: Ch 2  💻 tutorial: 01-data-02  💻 tutorial: 01-data-03\nslides lecture-3  (filled-in)\n\n\n\n\n\n\nThu, Sep 5\nCategorical data\n📖 ims: Ch 4  💻 tutorial: 02-explore-01\nlecture-4  (filled-in)\n\n\n\n\n\n\nFri, Sep 6\nLab 2: data\n📖 ims: Ch 3  💻 tutorial: 01-data-04\nlab-2\nLab 1 @ 8AM\n\n\n3\nTue, Sep 10\nCheckpoint: exploratory data analysis\n📖 ims: Ch 5  💻 tutorial: 02-explore-02\nslides\n\n\n\n\n\n\nThu, Sep 12\nSimple linear regression\n📖 ims: Ch 7  💻 tutorial: 03-model-01  💻 tutorial: 03-model-02\nslides  Play around!  Play around some more!  Primer\n\n\n\n\n\n\nFri, Sep 13\nLab 3: exploratory data analysis\n📖 ims: Ch 6  💻 tutorial: 02-explore-03\nlab-3\nLab 2 @ 8AM\n\n\n4\nTue, Sep 17\nMultiple linear regression\n📖 ims: Ch 8.1 - 8.2\nslides  Primer\n\n\n\n\n\n\nThu, Sep 19\n\\(R^2\\)\n📖 ims: Ch 8.3 - 8.4\nslides  Primer\n\n\n\n\n\n\nFri, Sep 20\nLab 4: regression\n\n\nlab-4\nLab 3 @ 8AM\n\n\n5\nTue, Sep 24\nModel selection\n📖 ims: Ch 8.4 - 8.5\n\n\n\n\n\n\n\n\nThu, Sep 26\nModel selection\n📖 ims: Ch 7  📖 ims: Ch 8\nslides\n\n\n\n\n\n\nFri, Sep 27\nLab: exam review\n\n\n\n\nLab 4 @ 8AM\n\n\n6\nTue, Oct 1\nIn-class Exam 1\nPractice exams (Canvas)  Make your cheet sheet!\n\n\n\n\n\n\n\n\nThu, Oct 3\nNo lecture\n\n\n\n\nTake-home Exam 1 @ 5PM\n\n\n\n\nFri, Oct 4\nNo lab\n\n\n\n\n\n\n\n\n7\nTue, Oct 8\nLogistic Regression 1\n📖 ims: Ch 9\nslides\n\n\n\n\n\n\nThu, Oct 10\nLogistic Regression 2  Ethics\n📖 ims: Ch 9\nslides  Primer\n\n\n\n\n\n\nFri, Oct 11\nNo lab\n\n\n\n\n\n\n\n\n8\nTue, Oct 15\nNo lecture\n\n\n\n\n\n\n\n\n\n\nThu, Oct 17\nIntroduction to statistics  Interval estimation 1\n📖 ims: Ch 12  💻 tutorial: 04-foundations-04\nslides  Primer  Play around!\n\n\n\n\n\n\nFri, Oct 18\nLab 5: confidence intervals\n\n\nlab-5\n\n\n\n\n9\nTue, Oct 22\nInterval estimation 2\n📖 ims: Ch 12  💻 tutorial: 04-foundations-04\nslides\n\n\n\n\n\n\nThu, Oct 24\nHypothesis testing 1\n📖 ims: Ch 11  📖 ims: Ch 14\nslides\n\n\n\n\n\n\nFri, Oct 25\nLab 6: hypothesis testing\n\n\nlab-6  Primer\nLab 5 @ 8AM\n\n\n10\nTue, Oct 29\nHypothesis testing 2\n📖 ims: Ch 11  📖 ims: Ch 14  💻 tutorial: 04-foundations-01  💻 tutorial: 04-foundations-02  💻 tutorial: 04-foundations-03\nslides  Primer\n\n\n\n\n\n\nThu, Oct 31\nHypothesis testing recap  Intro to central limit theorem\n📖 ims: Ch 13\nslides  Primer\n\n\n\n\n\n\nFri, Nov 1\nLab 7: statistical inference overview\n\n\nlab-7\nLab 6 @ 8AM\n\n\n11\nTue, Nov 5\nInference for one proportion\n📖 ims: Ch 16\nPrimer\n\n\n\n\n\n\nThu, Nov 7\nInference for mathematical models\n📖 ims: Ch 13  📖 ims: Ch 16\nnotes\n\n\n\n\n\n\nFri, Nov 8\nLab: exam review\n\n\n\n\nLab 7 @ 8AM\n\n\n12\nTue, Nov 12\nIn-class Exam 2\nPractice exams (Canvas)  Make your cheet sheet!\n\n\n\n\n\n\n\n\nThu, Nov 14\nNo lecture\n\n\n\n\nTake-home Exam 2 @ 5PM\n\n\n\n\nFri, Nov 15\nNo lab\n\n\n\n\n\n\n\n\n13\nTue, Nov 19\nInference for one mean\n📖 ims: Chp 19  💻 tutorial: 05-infer-05  💻 tutorial: 05-infer-06\nTo be posted\n\n\n\n\n\n\nThu, Nov 21\nInference for two means\n📖 ims: Chp 20  💻 tutorial: 05-infer-07\nTo be posted\n\n\n\n\n\n\nFri, Nov 22\n(project work period)\n\n\n\n\nProject proposal @ 5PM\n\n\n14\nTue, Nov 26\nProbability potpourri\n\n\nThe birthday problem  The Galton board  Prediction markets  The Monty Hall problem\n\n\n\n\n\n\nThu, Nov 28\nNo lecture\n\n\n\n\n\n\n\n\n\n\nFri, Nov 29\nNo lab\n\n\n\n\n\n\n\n\n15\nTue, Dec 3\n\n\nto be posted\nto be posted\n\n\n\n\n\n\nThu, Dec 5\n\n\nto be posted\nto be posted\n\n\n\n\n\n\nFri, Dec 6\n(project work period)\n\n\n\n\nPeer review @ 5PM\n\n\n16\nSun, Dec 15\nNo meeting\n\n\n\n\nFinal project @ 5PM",
    "crumbs": [
      "Syllabus",
      "Schedule"
    ]
  },
  {
    "objectID": "course-resources.html",
    "href": "course-resources.html",
    "title": "University resources",
    "section": "",
    "text": "If you are having difficulty with the costs associated with this course (obtaining a laptop, mostly), here are some resources:\n\nKarsh Office of Undergraduate Support: Regardless of your aid package, Karsh offers loans and resources for connecting students with campus programs that might help alleviate course costs.\nDukeLIFE: The Course Material Assistance program offers assistance for eligible students, including through the LIFE Loaner Laptop Program. Students who are eligible for DukeLIFE benefits are notified before the start of the semester; program resources are limited.\nDuke Link: They have a small supply of laptops that can be rented out for five days at a time.",
    "crumbs": [
      "Syllabus",
      "University resources"
    ]
  },
  {
    "objectID": "course-resources.html#course-costs",
    "href": "course-resources.html#course-costs",
    "title": "University resources",
    "section": "",
    "text": "If you are having difficulty with the costs associated with this course (obtaining a laptop, mostly), here are some resources:\n\nKarsh Office of Undergraduate Support: Regardless of your aid package, Karsh offers loans and resources for connecting students with campus programs that might help alleviate course costs.\nDukeLIFE: The Course Material Assistance program offers assistance for eligible students, including through the LIFE Loaner Laptop Program. Students who are eligible for DukeLIFE benefits are notified before the start of the semester; program resources are limited.\nDuke Link: They have a small supply of laptops that can be rented out for five days at a time.",
    "crumbs": [
      "Syllabus",
      "University resources"
    ]
  },
  {
    "objectID": "course-resources.html#tech-support",
    "href": "course-resources.html#tech-support",
    "title": "University resources",
    "section": "Tech support",
    "text": "Tech support\nContact the Duke OIT Service Desk at oit.duke.edu/help.",
    "crumbs": [
      "Syllabus",
      "University resources"
    ]
  },
  {
    "objectID": "course-resources.html#academic-support",
    "href": "course-resources.html#academic-support",
    "title": "University resources",
    "section": "Academic support",
    "text": "Academic support\nThere are times you may need help with the class that is beyond what can be provided by the teaching team. In those instances, I encourage you to visit the Academic Resource Center. The Academic Resource Center (ARC) offers free services to all students during their undergraduate careers at Duke. Services include Learning Consultations, Peer Tutoring and Study Groups, ADHD/LD Coaching, Outreach Workshops, and more. Because learning is a process unique to every individual, they work with each student to discover and develop their own academic strategy for success at Duke. Contact the ARC to schedule an appointment. Undergraduates in any year, studying any discipline can benefit! Contact ARC@duke.edu, 919-684-5917.",
    "crumbs": [
      "Syllabus",
      "University resources"
    ]
  },
  {
    "objectID": "course-resources.html#accessibility",
    "href": "course-resources.html#accessibility",
    "title": "University resources",
    "section": "Accessibility",
    "text": "Accessibility\nIf any portion of the course is not accessible to you due to challenges with technology or the course format, please let me know so we can make appropriate accommodations.\nThe Student Disability Access Office (SDAO) is available to ensure that students can engage with their courses and related assignments. Students should contact the SDAO to request or update accommodations under these circumstances.",
    "crumbs": [
      "Syllabus",
      "University resources"
    ]
  },
  {
    "objectID": "course-resources.html#mental-health-and-well-being",
    "href": "course-resources.html#mental-health-and-well-being",
    "title": "University resources",
    "section": "Mental health and well-being",
    "text": "Mental health and well-being\nDuke is committed to holistic student well-being, including mental, emotional, and physical health. The university offers resources to help students manage daily stress, encourage intentional self-care, and access just-in-time support. If you find you need support, your mental and/or emotional health concerns are impacting your day-to-day activities and your academic performance, or you need someone to talk to, the resources below are available to you:\n\nDukeReach: DukeReach provides comprehensive outreach services to support students in managing all aspects of well-being, including referrals and follow-up services for students who are experiencing significant challenges related to mental health, physical health, social adjustment, and/or a variety of other stressors. You can reach the DukeReach team at dukereach@duke.edu.\nCounseling and Psychological Services (CAPS): CAPS services include individual and group counseling services, psychiatric services, and workshops. CAPS also provides referrals to off-campus resources for specialized care. You can reach CAPS at (919) 660-1000.\nTimelyCare1: TimelyCare is an online platform that is a convenient, confidential, and free way for Duke students to receive 24/7 mental health support through TalkNow and scheduled counseling.\nBC Fellows for Healthy Relationship: The BC Fellows meet with students individually and in groups, supporting the development of healthy relationships and building meaningful community in all areas of a student’s life.\nDukeLine: Students who want to connect anonymously with a Peer Coach can text 984-230-4888 from 5 to 11 p.m. daily. DukeLine offers in-the-moment anonymous, non-emergency text support from a peer.\nDuWell: DuWell provides Moments of Mindfulness (stress management and resilience building) and meditation programming (Koru workshop) to assist students in developing a daily emotional well-being practice. All are welcome, and no experience is necessary. You can reach DuWell at (919) 681-8421.",
    "crumbs": [
      "Syllabus",
      "University resources"
    ]
  },
  {
    "objectID": "course-resources.html#footnotes",
    "href": "course-resources.html#footnotes",
    "title": "University resources",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nFormerly known as Blue Devils Care.↩︎",
    "crumbs": [
      "Syllabus",
      "University resources"
    ]
  },
  {
    "objectID": "ae/ae-07-books.html",
    "href": "ae/ae-07-books.html",
    "title": "Weights of books",
    "section": "",
    "text": "Today we’ll explore the question “How do volume and weights books relate?” and “How, if at all, does that change when we take whether the book is hardback or paperback into consideration?”"
  },
  {
    "objectID": "ae/ae-07-books.html#plot-the-best-fit-line",
    "href": "ae/ae-07-books.html#plot-the-best-fit-line",
    "title": "Weights of books",
    "section": "Plot the best fit line",
    "text": "Plot the best fit line\nVisualize the relationship between volume (on the x-axis) and weight (on the y-axis). Overlay the line of best fit. Describe the relationship between these variables.\n\nggplot(allbacks, aes(x = volume, y = weight)) +\n  geom_point() +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  labs(\n    x = \"Volume (cubic centimeters)\",\n    y = \"Weight (grams)\"\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\nAdd response here."
  },
  {
    "objectID": "ae/ae-07-books.html#get-the-estimated-regression-coefficients",
    "href": "ae/ae-07-books.html#get-the-estimated-regression-coefficients",
    "title": "Weights of books",
    "section": "Get the estimated regression coefficients",
    "text": "Get the estimated regression coefficients\nFit a model predicting weight from volume for these books and save it as weight_fit. Display a tidy output of the model.\n\nweight_fit &lt;- linear_reg() |&gt;\n  fit(weight ~ volume, data = allbacks)\n\ntidy(weight_fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic    p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)  108.      88.4         1.22 0.245     \n2 volume         0.709    0.0975      7.27 0.00000626"
  },
  {
    "objectID": "ae/ae-07-books.html#plot-the-best-fit-lines",
    "href": "ae/ae-07-books.html#plot-the-best-fit-lines",
    "title": "Weights of books",
    "section": "Plot the best fit line(s)",
    "text": "Plot the best fit line(s)\nVisualize the relationship between volume (on the x-axis) and weight (on the y-axis), taking into consideration the cover type of the book. Use different colors and shapes for hardback and paperback books. Also use different colors for lines of best fit for the two types of books. In addition, add the overall line of best fit (from Exercise 1) as a gray dashed line so that you can see the difference between the lines when considering and not considering cover type.\n\nggplot(allbacks, aes(x = volume, y = weight)) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"gray\", linetype = \"dashed\") +\n  geom_point() +\n  labs(\n    x = \"Volume (cubic centimeters)\",\n    y = \"Weight (grams)\",\n    shape = \"Cover\", color = \"Cover\"\n  )\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "ae/ae-07-books.html#get-the-estimated-regression-coefficients-1",
    "href": "ae/ae-07-books.html#get-the-estimated-regression-coefficients-1",
    "title": "Weights of books",
    "section": "Get the estimated regression coefficients",
    "text": "Get the estimated regression coefficients\nFit a model predicting weight from volume for these books and save it as weight_cover_fit. Display a tidy output of the model.\n\nweight_cover_fit &lt;- linear_reg() |&gt;\n  fit(weight ~ volume, data = allbacks)\n\ntidy(weight_cover_fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic    p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)  108.      88.4         1.22 0.245     \n2 volume         0.709    0.0975      7.27 0.00000626"
  },
  {
    "objectID": "ae/ae-07-books.html#prediction",
    "href": "ae/ae-07-books.html#prediction",
    "title": "Weights of books",
    "section": "Prediction",
    "text": "Prediction\nUsing the model you chose, predict the weight of a hardcover book that is 1000 cubic centimeters (that is, roughly 25 centimeters in length, 20 centimeters in width, and 2 centimeters in height/thickness).\n\nnew_book &lt;- tibble(\n  cover = \"hb\",\n  volume = 1000\n)\n\npredict(weight_cover_fit, new_data = new_book)\n\n# A tibble: 1 × 1\n  .pred\n  &lt;dbl&gt;\n1  816."
  },
  {
    "objectID": "ae/ae-02-flint.html",
    "href": "ae/ae-02-flint.html",
    "title": "Exploring Flint Michigan’s water data",
    "section": "",
    "text": "By the end of this application exercise you will\n\nmeet the computational toolkit for the course\ndefine and compute various statistics\nbegin to gain familiarity with making data visualizations with ggplot()\n\nWe will do this using water lead content data from Flint, MI. The following paragraph will be useful in evaluating the lead amount values we’ll see in the dataset.\n\nWhile there is no completely safe amount of lead consumption, the limit allowed by the Lead and Copper Rule (LCR) of 1991 is 15 parts per billion (ppb). If this is exceeded in more than 10% of homes tested (or if the 90th percentile value of the total sample is above 15 ppb), action is required. And to make sure problems are caught, sampling for lead in water is supposed to target the “worst-case” homes – those in areas served by lead pipes.\n\nIf you’re interested in this sort of thing, Cullud Wattah by Erika Dickerson-Despenza is a very good play about the human toll of this fiasco."
  },
  {
    "objectID": "ae/ae-02-flint.html#rstudio",
    "href": "ae/ae-02-flint.html#rstudio",
    "title": "Exploring Flint Michigan’s water data",
    "section": "RStudio",
    "text": "RStudio\n\nFiles, plots, viewer, environment, etc. panes\nConsole\nEditor"
  },
  {
    "objectID": "ae/ae-02-flint.html#r",
    "href": "ae/ae-02-flint.html#r",
    "title": "Exploring Flint Michigan’s water data",
    "section": "R",
    "text": "R\n\nWriting code in the console\nBasic math with R\nCreating variables in R, the assignment operator (&lt;-), and the Environment pane\nR functions and packages and the Packages pane\nGetting help with R and the Help pane"
  },
  {
    "objectID": "ae/ae-02-flint.html#quarto",
    "href": "ae/ae-02-flint.html#quarto",
    "title": "Exploring Flint Michigan’s water data",
    "section": "Quarto",
    "text": "Quarto\n\nYAML: Metadata\nNarrative: Edited with the visual editor (or the source editor)\nCode: In code chunks\n\nChunk options (following #|)\nComments (following #)\nCode\n\nRunning individual code chunks vs. rendering a document"
  },
  {
    "objectID": "ae/ae-02-flint.html#load-packages",
    "href": "ae/ae-02-flint.html#load-packages",
    "title": "Exploring Flint Michigan’s water data",
    "section": "Load packages",
    "text": "Load packages\nWe’ll use the tidyverse package for analysis, which offers functionality for data import, wrangling, visualization, and more.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nLoading this package prints out a message. What does this message mean? How can we suppress the message from the output?"
  },
  {
    "objectID": "ae/ae-02-flint.html#load-data",
    "href": "ae/ae-02-flint.html#load-data",
    "title": "Exploring Flint Michigan’s water data",
    "section": "Load data",
    "text": "Load data\nThe read_csv() function can be used for reading CSV (comma separated values) files. The file we’re reading is called flint with the suffix (.csv) which indicates its file type. The file is in the data folder.\nBefore reading in the file, go to the data folder in the Files pane to confirm that it is, indeed, there. Then, read the file by running the code chunk below by clicking on the green triangle icon on the code chunk.\n\nflint &lt;- read_csv(\"data/flint.csv\")\n\nRows: 813 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): draw\ndbl (4): id, zip, ward, lead\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nOne of two things may have happened:\n\nThe file was read successfully and you now see a dataset called flint in your Environment pane.\nThe file was not read successfully and you see an error Error in read_csv(\"data/flint.csv\") : could not find function \"read_csv\".\n\nIf (1) happened, great!\nIf (2) happened, let’s troubleshoot first before continuing."
  },
  {
    "objectID": "ae/ae-02-flint.html#data-dictionary",
    "href": "ae/ae-02-flint.html#data-dictionary",
    "title": "Exploring Flint Michigan’s water data",
    "section": "Data dictionary",
    "text": "Data dictionary\nThe following variables are in the flint data frame:\n\nid: sample ID number (identifies the home)\nzip: ZIP code in Flint of the sample’s location\nward: ward in Flint of the sample’s location\ndraw: which time point the water was sampled from\nlead: lead content in parts per billion (ppb)"
  },
  {
    "objectID": "ae/ae-02-flint.html#populations-and-samples",
    "href": "ae/ae-02-flint.html#populations-and-samples",
    "title": "Exploring Flint Michigan’s water data",
    "section": "Populations and samples",
    "text": "Populations and samples\nWe want to learn about the population using a sample.\nIn the case we want to learn about the lead content in all of Flint, MI homes but only have available water readings from a sample of homes (our data set).\nExercise 1: Look at the data, how many observations are there? How many variables?\n\n# add code here"
  },
  {
    "objectID": "ae/ae-02-flint.html#frequencies",
    "href": "ae/ae-02-flint.html#frequencies",
    "title": "Exploring Flint Michigan’s water data",
    "section": "Frequencies",
    "text": "Frequencies\nLet’s count() to find the number of different time points water was sampled with the count() function.\n\nThe first argument is flint: the data frame\nThe second argument is draw: the variable\n\n\ncount(flint, draw)\n\n# A tibble: 3 × 2\n  draw       n\n  &lt;chr&gt;  &lt;int&gt;\n1 first    271\n2 second   271\n3 third    271\n\n\nWe can achieve the same result with the following “piped” operation as well.\n\nThe first line is flint: the data frame\nThen the pipe operator, read as “and then”, which places what comes before it as the first argument of what comes after it\nThe second line is count(draw)\n\n\nflint |&gt;\n  count(draw)\n\n# A tibble: 3 × 2\n  draw       n\n  &lt;chr&gt;  &lt;int&gt;\n1 first    271\n2 second   271\n3 third    271\n\n\nWe can use a similar approach to fund out how many unique homes are in the data set:\n\nflint |&gt;\n  count(id)\n\n# A tibble: 269 × 2\n      id     n\n   &lt;dbl&gt; &lt;int&gt;\n 1     1     3\n 2     2     3\n 3     4     3\n 4     5     3\n 5     6     3\n 6     7     3\n 7     8     3\n 8     9     3\n 9    12     3\n10    13     3\n# ℹ 259 more rows\n\n\nExercise 2: How many samples were taken from each zip code?\n\n# add code here\n\nExercise 3: Which ZIP code had the most samples drawn? Hint: See the help for count.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-02-flint.html#measures-of-central-tendency",
    "href": "ae/ae-02-flint.html#measures-of-central-tendency",
    "title": "Exploring Flint Michigan’s water data",
    "section": "Measures of central tendency",
    "text": "Measures of central tendency\n\nmean\nmedian\nmode"
  },
  {
    "objectID": "ae/ae-02-flint.html#measures-of-spread",
    "href": "ae/ae-02-flint.html#measures-of-spread",
    "title": "Exploring Flint Michigan’s water data",
    "section": "Measures of spread",
    "text": "Measures of spread\n\nvariance\nstandard deviation\nrange\nquartiles\ninter-quartile range (IQR)"
  },
  {
    "objectID": "ae/ae-02-flint.html#order-statistics",
    "href": "ae/ae-02-flint.html#order-statistics",
    "title": "Exploring Flint Michigan’s water data",
    "section": "Order statistics",
    "text": "Order statistics\n\nquantiles\nminimum (0 percentile)\nmedian (50th percentile)\nmaximum (100 percentile)\n\n… and any other arbitrary function of the data you can come up with!\nExercise 4: Compute each of these statistics for lead ppb.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-02-flint.html#histograms",
    "href": "ae/ae-02-flint.html#histograms",
    "title": "Exploring Flint Michigan’s water data",
    "section": "Histograms",
    "text": "Histograms\nLet’s take a look at the distribution of lead content in homes in Flint, MI.\n\nggplot(flint, aes(x = lead)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nWe can make this plot look nicer/more useful by adjusting the number of bins and zooming into the x-axis.\n\nggplot(flint, aes(x = lead)) +\n  geom_histogram(bins = 50) +\n  coord_cartesian(xlim = c(0, 100))\n\n\n\n\n\n\n\n\nLet’s visualize some of our summary statistics on the plot.\nExercise 5: Add a new layer, geom_vline(xintercept = __, color = \"red\"), to the histogram below, filling in the blank with the mean.\n\nggplot(flint, aes(x = lead)) + \n  geom_histogram(bins = 50) + \n  coord_cartesian(xlim = c(0, 100))\n\n\n\n\n\n\n\n\nExercise 6: Add one more layer which overlays the median, in a different color.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-02-flint.html#box-plots",
    "href": "ae/ae-02-flint.html#box-plots",
    "title": "Exploring Flint Michigan’s water data",
    "section": "Box plots",
    "text": "Box plots\nNext, let’s narrow our focus to the zip codes 48503, 48504, 48505, 48506, and 48507 and observations with lead values less than 1,000 ppb.\n\nflint_focus &lt;- flint |&gt;\n  filter(zip %in% 48503:48507 & lead &lt; 1000)\n\nExercise 7: Below are side-by-side box plots for the three flushing times in each of the five zip codes we considered. Add x and y labels; add a title by inserting title = \"title_name\" inside the labs() function.\n\nggplot(data = flint_focus, aes(y = factor(zip), x = lead)) +\n  geom_boxplot(aes(fill = factor(draw))) +\n  labs(x = \"___\", y = \"___\", fill = \"Flushing time\") +\n  scale_fill_discrete(\n    breaks = c(\"first\", \"second\", \"third\"),\n    labels = c(\"0 (sec)\", \"45 (sec)\", \"120 (sec)\")\n  )\n\n\n\n\n\n\n\n\nExercise 8: Add labels for x, y, a title, and subtitle to the code below to update the corresponding plot.\n\nggplot(data = flint_focus, aes(y = factor(zip), x = lead)) +\n  geom_boxplot(aes(fill = factor(draw))) + \n  labs(\n    x = \"___\", y = \"___\", fill = \"Flushing time\",\n    title = \"___\",\n    subtitle = \"___\"\n    ) +\n  scale_fill_discrete(\n    breaks = c(\"first\", \"second\", \"third\"),\n    labels = c(\"0 (sec)\", \"45 (sec)\", \"120 (sec)\")\n  ) +\n  coord_cartesian(xlim = c(0, 50)) +\n  theme_bw()\n\n\n\n\n\n\n\n\nExercise 9: What is the difference between the two plots? What are the advantages and disadvantages to each plot?\n[Add your answer here]"
  },
  {
    "objectID": "ae/ae-03-delta-sa.html",
    "href": "ae/ae-03-delta-sa.html",
    "title": "COVID vaccine and deaths from Delta variant",
    "section": "",
    "text": "Important\n\n\n\nThese are suggested answers for the application exercise. They’re not necessarily complete or 100% accurate, they’re roughly what we develop in class while going through the exercises.\nThe main question we’ll explore today is “How do deaths from COVID cases compare between vaccinated and unvaccinated?”\nWhat do you think?"
  },
  {
    "objectID": "ae/ae-03-delta-sa.html#exercise-1",
    "href": "ae/ae-03-delta-sa.html#exercise-1",
    "title": "COVID vaccine and deaths from Delta variant",
    "section": "Exercise 1",
    "text": "Exercise 1\nHow many rows and columns are in this dataset? Answer in a full sentence using inline code. What does each row represent and what does each column represent? For each variable, identify its type.\n\nThere are 268166 rows and 3 columns in the dataset. Each row represents a person with COVID, and the columns represent whether the person was vaccinated or not, their age, and whether they died or survived."
  },
  {
    "objectID": "ae/ae-03-delta-sa.html#exercise-2",
    "href": "ae/ae-03-delta-sa.html#exercise-2",
    "title": "COVID vaccine and deaths from Delta variant",
    "section": "Exercise 2",
    "text": "Exercise 2\nDo these data come from an observational study or experiment? Why?\n\nObservational study, people in the study chose to get vaccinated or not, they weren’t randomized into groups."
  },
  {
    "objectID": "ae/ae-03-delta-sa.html#exercise-3",
    "href": "ae/ae-03-delta-sa.html#exercise-3",
    "title": "COVID vaccine and deaths from Delta variant",
    "section": "Exercise 3",
    "text": "Exercise 3\nCreate a visualization of health outcome by vaccine status that allows you to compare the proportion of deaths across those who are and are not vaccinated. What can you say about death rates in these two groups based on this visualization?\n\nWhile this is very difficult to see, the proportion of patients who died is slightly higher for the vaccinated group compared to the unvaccinated group.\n\n\nggplot(delta, aes(x = vaccine, fill = outcome)) +\n  geom_bar(position = \"fill\")"
  },
  {
    "objectID": "ae/ae-03-delta-sa.html#exercise-4",
    "href": "ae/ae-03-delta-sa.html#exercise-4",
    "title": "COVID vaccine and deaths from Delta variant",
    "section": "Exercise 4",
    "text": "Exercise 4\nCalculate the proportion of deaths in among those who are vaccinated. Then, calculate the proportion among those who are not vaccinated.\n\nProportion of deaths among the vaccinated is 0.00407 and the proportion of deaths among the unvaccinated is 0.00166.\n\n\ndelta |&gt;\n  count(vaccine, outcome) |&gt;\n  group_by(vaccine) |&gt;\n  mutate(prop = n / sum(n))\n\n# A tibble: 4 × 4\n# Groups:   vaccine [2]\n  vaccine      outcome       n    prop\n  &lt;chr&gt;        &lt;chr&gt;     &lt;int&gt;   &lt;dbl&gt;\n1 Unvaccinated died        250 0.00166\n2 Unvaccinated survived 150802 0.998  \n3 Vaccinated   died        477 0.00407\n4 Vaccinated   survived 116637 0.996"
  },
  {
    "objectID": "ae/ae-03-delta-sa.html#exercise-5",
    "href": "ae/ae-03-delta-sa.html#exercise-5",
    "title": "COVID vaccine and deaths from Delta variant",
    "section": "Exercise 5",
    "text": "Exercise 5\nCreate the visualization and calculate proportions from the two previous exercises, this time controlling for age. How do the proportions compare?\n\nAmong both the younger patients (&lt;50) and the older patients (50+), proportions of deaths is smaller for the vaccinated.\n\n\nggplot(delta, aes(x = vaccine, fill = outcome)) +\n  geom_bar(position = \"fill\") +\n  facet_wrap(~age)\n\n\n\n\n\n\n\ndelta |&gt;\n  count(age, vaccine, outcome) |&gt;\n  group_by(age, vaccine) |&gt;\n  mutate(prop = n / sum(n))\n\n# A tibble: 8 × 5\n# Groups:   age, vaccine [4]\n  age   vaccine      outcome       n     prop\n  &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;     &lt;int&gt;    &lt;dbl&gt;\n1 50+   Unvaccinated died        205 0.0596  \n2 50+   Unvaccinated survived   3235 0.940   \n3 50+   Vaccinated   died        459 0.0168  \n4 50+   Vaccinated   survived  26848 0.983   \n5 &lt;50   Unvaccinated died         45 0.000305\n6 &lt;50   Unvaccinated survived 147567 1.00    \n7 &lt;50   Vaccinated   died         18 0.000200\n8 &lt;50   Vaccinated   survived  89789 1.00"
  },
  {
    "objectID": "ae/ae-03-delta-sa.html#exercise-6",
    "href": "ae/ae-03-delta-sa.html#exercise-6",
    "title": "COVID vaccine and deaths from Delta variant",
    "section": "Exercise 6",
    "text": "Exercise 6\nBased on your findings so far, fill in the blanks with more, less, or equally: Is there anything surprising about these statements? Speculate on what, if anything, the discrepancy might be due to.\n\nIn 2021, among those in the UK who were COVID Delta cases, the vaccinated were more likely to die than the unvaccinated.\nFor those under 50, those who were unvaccinated were more likely to die than those who were vaccinated.\nFor those 50 and up, those who were unvaccinated were more likely to die than those who were vaccinated.\n\n\nThe relationshio between outcome and vaccine status changes depending on the age of the person."
  },
  {
    "objectID": "ae/ae-03-delta-sa.html#exercise-7",
    "href": "ae/ae-03-delta-sa.html#exercise-7",
    "title": "COVID vaccine and deaths from Delta variant",
    "section": "Exercise 7",
    "text": "Exercise 7\nLet’s rephrase the previous question which asked you to speculate on why deaths among vaccinated cases overall is higher while deaths among unvaccinated cases are higher when we split the data into two groups (below 50 and 50 and up). What might be the confounding variable in the relationship between vaccination and deaths?\n\nAge."
  },
  {
    "objectID": "ae/ae-03-delta-sa.html#exercise-8",
    "href": "ae/ae-03-delta-sa.html#exercise-8",
    "title": "COVID vaccine and deaths from Delta variant",
    "section": "Exercise 8",
    "text": "Exercise 8\nVisualize and describe the distribution of seniors (50 and up) based on (a.k.a. conditional on) vaccination status. Hint: Your description will benefit from calculating proportions of seniors in each of the vaccination groups and working those values into your narrative.\n\nThe proportion of seniors (50+) is higher for the vaccinated group (0.233) compared to the unvaccinated group (0.0228).\n\n\nggplot(delta, aes(x = vaccine, fill = age)) +\n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\ndelta |&gt;\n  count(vaccine, age) |&gt;\n  group_by(vaccine) |&gt;\n  mutate(prop = n / sum(n))\n\n# A tibble: 4 × 4\n# Groups:   vaccine [2]\n  vaccine      age        n   prop\n  &lt;chr&gt;        &lt;chr&gt;  &lt;int&gt;  &lt;dbl&gt;\n1 Unvaccinated 50+     3440 0.0228\n2 Unvaccinated &lt;50   147612 0.977 \n3 Vaccinated   50+    27307 0.233 \n4 Vaccinated   &lt;50    89807 0.767"
  },
  {
    "objectID": "ae/ae-02-flint-sa.html",
    "href": "ae/ae-02-flint-sa.html",
    "title": "Exploring Flint’s water data",
    "section": "",
    "text": "Important\n\n\n\nThese are suggested answers for the application exercise. They’re not necessarily complete or 100% accurate, they’re roughly what we develop in class while going through the exercises."
  },
  {
    "objectID": "ae/ae-02-flint-sa.html#rstudio",
    "href": "ae/ae-02-flint-sa.html#rstudio",
    "title": "Exploring Flint’s water data",
    "section": "RStudio",
    "text": "RStudio\n\nFiles, plots, viewer, environment, etc. panes\nConsole\nEditor"
  },
  {
    "objectID": "ae/ae-02-flint-sa.html#r",
    "href": "ae/ae-02-flint-sa.html#r",
    "title": "Exploring Flint’s water data",
    "section": "R",
    "text": "R\n\nWriting code in the console\nBasic math with R\nCreating variables in R, the assignment operator (&lt;-), and the Environment pane\nR functions and packages and the Packages pane\nGetting help with R and the Help pane"
  },
  {
    "objectID": "ae/ae-02-flint-sa.html#quarto",
    "href": "ae/ae-02-flint-sa.html#quarto",
    "title": "Exploring Flint’s water data",
    "section": "Quarto",
    "text": "Quarto\n\nYAML: Metadata\nNarrative: Edited with the visual editor (or the source editor)\nCode: In code chunks\n\nChunk options (following #|)\nComments (following #)\nCode\n\nRunning individual code chunks vs. rendering a document"
  },
  {
    "objectID": "ae/ae-02-flint-sa.html#load-packages",
    "href": "ae/ae-02-flint-sa.html#load-packages",
    "title": "Exploring Flint’s water data",
    "section": "Load packages",
    "text": "Load packages\nWe’ll use the tidyverse package for analysis, which offers functionality for data import, wrangling, visualization, and more.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.3     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\nLoading this package prints out a message. What does this message mean? How can we suppress the message from the output?"
  },
  {
    "objectID": "ae/ae-02-flint-sa.html#load-data",
    "href": "ae/ae-02-flint-sa.html#load-data",
    "title": "Exploring Flint’s water data",
    "section": "Load data",
    "text": "Load data\nThe read_csv() function can be used for reading CSV (comma separated values) files. The file we’re reading is called flint with the suffix (.csv) which indicates its file type. The file is in the data folder.\nBefore reading in the file, go to the data folder in the Files pane to confirm that it is, indeed, there. Then, read the file by running the code chunk below by clicking on the green triangle icon on the code chunk.\n\nflint &lt;- read_csv(\"data/flint.csv\")\n\nRows: 813 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): draw\ndbl (4): id, zip, ward, lead\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nOne of two things may have happened:\n\nThe file was read successfully and you now see a dataset called flint in your Environment pane.\nThe file was not read successfully and you see an error Error in read_csv(\"data/flint.csv\") : could not find function \"read_csv\".\n\nIf (1) happened, great!\nIf (2) happened, let’s troubleshoot first before continuing."
  },
  {
    "objectID": "ae/ae-02-flint-sa.html#data-dictionary",
    "href": "ae/ae-02-flint-sa.html#data-dictionary",
    "title": "Exploring Flint’s water data",
    "section": "Data dictionary",
    "text": "Data dictionary\nThe following variables are in the flint data frame:\n\nid: sample ID number (identifies the home)\nzip: ZIP code in Flint of the sample’s location\nward: ward in Flint of the sample’s location\ndraw: which time point the water was sampled from\nlead: lead content in parts per billion (ppb)"
  },
  {
    "objectID": "ae/ae-02-flint-sa.html#populations-and-samples",
    "href": "ae/ae-02-flint-sa.html#populations-and-samples",
    "title": "Exploring Flint’s water data",
    "section": "Populations and samples",
    "text": "Populations and samples\nWe want to learn about the population using a sample.\nIn the case we want to learn about the lead content in all of Flint, MI homes but only have available water readings from a sample of homes (our data set).\nExercise 1: Look at the data, how many observations are there? How many variables?\n\nThere are 813 observations and 5 variables.\n\n\nnrow(flint)\n\n[1] 813\n\nncol(flint)\n\n[1] 5"
  },
  {
    "objectID": "ae/ae-02-flint-sa.html#frequencies",
    "href": "ae/ae-02-flint-sa.html#frequencies",
    "title": "Exploring Flint’s water data",
    "section": "Frequencies",
    "text": "Frequencies\nLet’s count() to find the number of different time points water was sampled with the count() function.\n\nThe first argument is flint: the data frame\nThe second argument is draw: the variable\n\n\ncount(flint, draw)\n\n# A tibble: 3 × 2\n  draw       n\n  &lt;chr&gt;  &lt;int&gt;\n1 first    271\n2 second   271\n3 third    271\n\n\nWe can achieve the same result with the following “piped” operation as well.\n\nThe first line is flint: the data frame\nThen the pipe operator, read as “and then”, which places what comes before it as the first argument of what comes after it\nThe second line is count(draw)\n\n\nflint |&gt;\n  count(draw)\n\n# A tibble: 3 × 2\n  draw       n\n  &lt;chr&gt;  &lt;int&gt;\n1 first    271\n2 second   271\n3 third    271\n\n\nWe can use a similar approach to fund out how many unique homes are in the data set:\n\nflint |&gt;\n  count(id)\n\n# A tibble: 269 × 2\n      id     n\n   &lt;dbl&gt; &lt;int&gt;\n 1     1     3\n 2     2     3\n 3     4     3\n 4     5     3\n 5     6     3\n 6     7     3\n 7     8     3\n 8     9     3\n 9    12     3\n10    13     3\n# ℹ 259 more rows\n\n\nExercise 2: How many samples were taken from each zip code?\n\nflint |&gt;\n  count(zip)\n\n# A tibble: 8 × 2\n    zip     n\n  &lt;dbl&gt; &lt;int&gt;\n1 48502     3\n2 48503   207\n3 48504   165\n4 48505   144\n5 48506   132\n6 48507   153\n7 48529     3\n8 48532     6\n\n\nExercise 3: Which ZIP code had the most samples drawn? Hint: See the help for count.\n\nThe zip code 48503 had the most samples drawn (207 samples).\n\n\nflint |&gt;\n  count(zip, sort = TRUE)\n\n# A tibble: 8 × 2\n    zip     n\n  &lt;dbl&gt; &lt;int&gt;\n1 48503   207\n2 48504   165\n3 48507   153\n4 48505   144\n5 48506   132\n6 48532     6\n7 48502     3\n8 48529     3"
  },
  {
    "objectID": "ae/ae-02-flint-sa.html#measures-of-central-tendency",
    "href": "ae/ae-02-flint-sa.html#measures-of-central-tendency",
    "title": "Exploring Flint’s water data",
    "section": "Measures of central tendency",
    "text": "Measures of central tendency\n\nmean\nmedian\nmode"
  },
  {
    "objectID": "ae/ae-02-flint-sa.html#measures-of-spread",
    "href": "ae/ae-02-flint-sa.html#measures-of-spread",
    "title": "Exploring Flint’s water data",
    "section": "Measures of spread",
    "text": "Measures of spread\n\nvariance\nstandard deviation\nrange\nquartiles\ninter-quartile range (IQR)"
  },
  {
    "objectID": "ae/ae-02-flint-sa.html#order-statistics",
    "href": "ae/ae-02-flint-sa.html#order-statistics",
    "title": "Exploring Flint’s water data",
    "section": "Order statistics",
    "text": "Order statistics\n\nquantiles\nminimum (0 percentile)\nmedian (50th percentile)\nmaximum (100 percentile)\n\n… and any other arbitrary function of the data you can come up with!\nExercise 4: Compute each of these statistics for lead ppb.\n\nflint |&gt;\n  summarize(\n    mean_lead = mean(lead),\n    median_lead = median(lead),\n    var_lead = var(lead),\n    sd_lead = sd(lead),\n    # etc.\n  )\n\n# A tibble: 1 × 4\n  mean_lead median_lead var_lead sd_lead\n      &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1      8.20        1.85    1718.    41.5"
  },
  {
    "objectID": "ae/ae-02-flint-sa.html#histograms",
    "href": "ae/ae-02-flint-sa.html#histograms",
    "title": "Exploring Flint’s water data",
    "section": "Histograms",
    "text": "Histograms\nLet’s take a look at the distribution of lead content in homes in Flint, MI.\n\nggplot(flint, aes(x = lead)) +\n  geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nWe can make this plot look nicer/more useful by adjusting the number of bins and zooming into the x-axis.\n\nggplot(flint, aes(x = lead)) +\n  geom_histogram(bins = 50) +\n  coord_cartesian(xlim = c(0, 100))\n\n\n\n\n\n\n\n\nLet’s visualize some of our summary statistics on the plot.\nExercise 5: Add a new layer, geom_vline(xintercept = __, color = \"red\"), to the histogram below, filling in the blank with the mean.\n\nggplot(flint, aes(x = lead)) + \n  geom_histogram(bins = 50) + \n  coord_cartesian(xlim = c(0, 100)) +\n  geom_vline(xintercept = 8.202614, color = \"red\")\n\n\n\n\n\n\n\n\nExercise 6: Add one more layer which overlays the median, in a different color.\n\nggplot(flint, aes(x = lead)) + \n  geom_histogram(bins = 50) + \n  coord_cartesian(xlim = c(0, 100)) +\n  geom_vline(xintercept = 8.202614, color = \"red\") +\n  geom_vline(xintercept = 1.852, color = \"blue\")"
  },
  {
    "objectID": "ae/ae-02-flint-sa.html#box-plots",
    "href": "ae/ae-02-flint-sa.html#box-plots",
    "title": "Exploring Flint’s water data",
    "section": "Box plots",
    "text": "Box plots\nNext, let’s narrow our focus to the zip codes 48503, 48504, 48505, 48506, and 48507 and observations with lead values less than 1,000 ppb.\n\nflint_focus &lt;- flint |&gt;\n  filter(zip %in% 48503:48507 & lead &lt; 1000)\n\nExercise 7: Below are side-by-side box plots for the three flushing times in each of the five zip codes we considered. Add x and y labels; add a title by inserting title = \"title_name\" inside the labs() function.\n\nggplot(data = flint_focus, aes(y = factor(zip), x = lead)) +\n  geom_boxplot(aes(fill = factor(draw))) +\n  labs(x = \"Lead (ppb)\", y = \"Zip code\", fill = \"Flushing time\") +\n  scale_fill_discrete(\n    breaks = c(\"first\", \"second\", \"third\"),\n    labels = c(\"0 (sec)\", \"45 (sec)\", \"120 (sec)\")\n  )\n\n\n\n\n\n\n\n\nExercise 8: Add labels for x, y, a title, and subtitle to the code below to update the corresponding plot.\n\nggplot(data = flint_focus, aes(y = factor(zip), x = lead)) +\n  geom_boxplot(aes(fill = factor(draw))) + \n  labs(\n    x = \"Lead (ppb)\", y = \"Zip code\", fill = \"Flushing time\",\n    title = \"Lead amount by flushing time\",\n    subtitle = \"In five zip codes\"\n    ) +\n  scale_fill_discrete(\n    breaks = c(\"first\", \"second\", \"third\"),\n    labels = c(\"0 (sec)\", \"45 (sec)\", \"120 (sec)\")\n  ) +\n  coord_cartesian(xlim = c(0, 50)) +\n  theme_bw()\n\n\n\n\n\n\n\n\nExercise 9: What is the difference between the two plots? What are the advantages and disadvantages to each plot?\n\nThe first plot shows the extreme outliers, while the second plot makes it easier to see the bulk of the distribution."
  },
  {
    "objectID": "course-policies.html",
    "href": "course-policies.html",
    "title": "Policies",
    "section": "",
    "text": "As a student in this course, you have agreed to uphold the Duke Community Standard and the practices specific to this course.",
    "crumbs": [
      "Syllabus",
      "Course policies"
    ]
  },
  {
    "objectID": "course-policies.html#duke-community-standard",
    "href": "course-policies.html#duke-community-standard",
    "title": "Policies",
    "section": "",
    "text": "As a student in this course, you have agreed to uphold the Duke Community Standard and the practices specific to this course.",
    "crumbs": [
      "Syllabus",
      "Course policies"
    ]
  },
  {
    "objectID": "course-policies.html#collaboration",
    "href": "course-policies.html#collaboration",
    "title": "Policies",
    "section": "Collaboration",
    "text": "Collaboration\n\nYou are encouraged to discuss and help one another with the labs, but you should not share solutions (e.g. emailing a friend your code). All submitted work must be your own;\nYou are forbidden from collaborating in any way on the exams;\nYou are enthusiastically encouraged to collaborate with your teammates on all aspects of the final project.",
    "crumbs": [
      "Syllabus",
      "Course policies"
    ]
  },
  {
    "objectID": "course-policies.html#communication",
    "href": "course-policies.html#communication",
    "title": "Policies",
    "section": "Communication",
    "text": "Communication\nIf you wish to ask content-related questions in writing, please do not do so via e-mail. Instead, please use the course discussion forum Ed Discussion. That way all members of the teaching team can see your question, and all students can benefit from the ensuing discussion. You are also encouraged to answer one another’s questions.\nIf you have questions about personal matters that may not be appropriate for the public course forum (e.g. illness, accommodations, etc), then please e-mail the instructor directly (john.zito@duke.edu).",
    "crumbs": [
      "Syllabus",
      "Course policies"
    ]
  },
  {
    "objectID": "course-policies.html#use-of-online-resources-including-ai",
    "href": "course-policies.html#use-of-online-resources-including-ai",
    "title": "Policies",
    "section": "Use of online resources, including AI",
    "text": "Use of online resources, including AI\nYou may make use of any online resources (e.g. StackOverflow) but you must explicitly cite where you obtained any code you directly use (or use as inspiration). Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism.\nYou should treat generative AI, such as ChatGPT, like other online resources. Two guiding principles govern how to use AI in this course:\n\nCognitive dimension: Working with AI should not reduce your thinking ability. We will practice using AI to facilitate—rather than hinder—learning.\nEthical dimension: Students using AI should be transparent about their use and ensure it aligns with academic integrity.\n\n\n AI tools for code: You may use the technology for coding examples on assignments; if you do so, you must explicitly cite where you obtained the code. Any recycled code that is discovered and is not explicitly cited will be treated as plagiarism. You may use these guidelines to cite AI-generated content. The bare minimum citation must include the AI tool you’re using (e.g., ChatGPT) and your prompt. The prompt you use cannot be copied and pasted directly from the assignment; you must create a prompt yourself.\n AI tools for narrative: Unless instructed otherwise, you may not use generative AI to generate a narrative that you then copy-paste verbatim into an assignment or edit and then insert into your assignment.\n\nIn general, you may use generative AI as a resource as you complete assignments but not to answer the exercises for you. You are ultimately responsible for the work you turn in; it should reflect your understanding of the course content. Identifying AI-generated content is fairly straightforward. Any code identified as AI-generated but not cited as such and any narrative identified as AI-generated will be considered plagiarism and treated as such.",
    "crumbs": [
      "Syllabus",
      "Course policies"
    ]
  },
  {
    "objectID": "course-policies.html#late-work-and-extensions",
    "href": "course-policies.html#late-work-and-extensions",
    "title": "Policies",
    "section": "Late work and extensions",
    "text": "Late work and extensions\nNo late work will be accepted unless you request an extension in advance by e-mailing the instructor (john.zito@duke.edu). All reasonable requests will be entertained, but extensions will not be long.",
    "crumbs": [
      "Syllabus",
      "Course policies"
    ]
  },
  {
    "objectID": "course-policies.html#regrade-requests",
    "href": "course-policies.html#regrade-requests",
    "title": "Policies",
    "section": "Regrade requests",
    "text": "Regrade requests\nIf you receive a graded assignment back, and you believe that some part of it was graded incorrectly, you may dispute it. You have one week after you receive a grade to submit a regrade request in Gradescope. The instructor will do the regrading.\n\n\n\n\n\n\nWarning\n\n\n\nA regrade request can result in your grade going up, staying the same, or going down if the instructor determines that, in fact, the original grader was too lenient.",
    "crumbs": [
      "Syllabus",
      "Course policies"
    ]
  },
  {
    "objectID": "labs/lab-6.html",
    "href": "labs/lab-6.html",
    "title": "Lab 6",
    "section": "",
    "text": "By the end of this lab you will create formulate and conduct hypothesis tests using randomization.",
    "crumbs": [
      "Labs",
      "Lab 6"
    ]
  },
  {
    "objectID": "labs/lab-6.html#getting-started",
    "href": "labs/lab-6.html#getting-started",
    "title": "Lab 6",
    "section": "Getting started",
    "text": "Getting started\n\nUse these steps to navigate to the STA101 version of RStudio using the Duke Container Manager;\nUse these steps to download all of the lab 6 files from our Canvas page, upload them to your RStudio files, and move them into an appropriately named folder (lab-6, for instance);\nOnce lab-6.qmd is where it needs to be, open it, and verify that you can click the “Render” button in RStudio and get a PDF file. See this answer on Ed if you want more guidance here;\nNow proceed to complete the exercises in this lab.",
    "crumbs": [
      "Labs",
      "Lab 6"
    ]
  },
  {
    "objectID": "labs/lab-6.html#pointers",
    "href": "labs/lab-6.html#pointers",
    "title": "Lab 6",
    "section": "Pointers",
    "text": "Pointers\n\nFor all visualizations you create, be sure to include informative titles for the plot, axes, and legend;\nRespond in complete sentences as much as possible;\nBe sure to observe good code style:\n\nThere is a line break after each |&gt; in a pipeline or + in a ggplot;\nThere are spaces around = signs;\nThere is a space after each ,;\nCode is properly indented;\nCode doesn’t exceed 80 characters in each line, longer lines of code are spread across multiple lines with appropriately placed line breaks (so in the rendered PDF, your code shouldn’t run off the page);\nCode chunks are labeled, informatively and without spaces.",
    "crumbs": [
      "Labs",
      "Lab 6"
    ]
  },
  {
    "objectID": "labs/lab-6.html#packages",
    "href": "labs/lab-6.html#packages",
    "title": "Lab 6",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse) \nlibrary(tidymodels)\nlibrary(openintro)",
    "crumbs": [
      "Labs",
      "Lab 6"
    ]
  },
  {
    "objectID": "labs/lab-6.html#part-1-heart-transplants---outcome",
    "href": "labs/lab-6.html#part-1-heart-transplants---outcome",
    "title": "Lab 6",
    "section": "Part 1: Heart transplants - outcome\n",
    "text": "Part 1: Heart transplants - outcome\n\nToday, we will be working with data from The Stanford University Heart Transplant Study (Turnbull, Brown, and Hu 1974):\n\nheart_transplant &lt;- read_csv(\"data/heart-transplant.csv\")\nglimpse(heart_transplant)\n\nRows: 103\nColumns: 8\n$ id         &lt;dbl&gt; 15, 43, 61, 75, 6, 42, 54, 38, 85, 2, 103, 12, 48, 102, 35,…\n$ outcome    &lt;chr&gt; \"deceased\", \"deceased\", \"deceased\", \"deceased\", \"deceased\",…\n$ transplant &lt;chr&gt; \"control\", \"control\", \"control\", \"control\", \"control\", \"con…\n$ age        &lt;dbl&gt; 53, 43, 52, 52, 54, 36, 47, 41, 47, 51, 39, 53, 56, 40, 43,…\n$ survtime   &lt;dbl&gt; 1, 2, 2, 2, 3, 3, 3, 5, 5, 6, 6, 8, 9, 11, 12, 16, 16, 16, …\n$ acceptyear &lt;dbl&gt; 68, 70, 71, 72, 68, 70, 71, 70, 73, 68, 67, 68, 71, 74, 70,…\n$ prior      &lt;chr&gt; \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\",…\n$ wait       &lt;dbl&gt; NA, NA, NA, NA, NA, NA, NA, 5, NA, NA, NA, NA, NA, NA, NA, …\n\n\nThe data dictionary is as follows:\n\n\n\n\n\n\nVariable\nDescription\n\n\n\nid\nID number of the patient\n\n\noutcome\nSurvival status with levels alive and deceased\n\n\n\ntransplant\nTransplant group with levels control (did not receive a transplant) and treatment (received a transplant)\n\n\nage\nAge of the patient at the beginning of the study\n\n\nsurvtime\nNumber of days patients were alive after the date they were determined to be a candidate for a heart transplant until the termination date of the study\n\n\nacceptyear\nYear of acceptance as a heart transplant candidate\n\n\nprior\nWhether or not the patient had prior surgery with levels yes and no\n\n\n\nwait\nWaiting time for transplant\n\n\n\nExercise 1\n\nIMS - Chapter 11 exercises, #8: Heart transplants.\n\nExercise 2\n\nRecreate the stacked bar plot in Exercise 1.\n\n\n\n\n\n\n\nHint\n\n\n\nThe colors are #569BBD and #114B5F. Use scale_fill_manual() to indicate these are the colors you want to use.\n\n\n\n\n\n\n\n\nPause and render\n\n\n\nNow that you’ve completed a few exercises, pause and render your document. If it renders without any issues, great! Move on to the next exercise. If it does not, debug the issue before moving on. Ask for help from your TA if you need. Do not proceed without rendering your document.\n\n\nExercise 3\n\nCalculate the point estimate for the difference in proportions of patients who died between the treatment and control groups: \\(\\hat{p}_{treatment} - \\hat{p}_{control}\\), where \\(\\hat{p}\\) is the observed probability of dying in each group. Do this in two ways:\n\nConstruct a frequency table of outcome by transplant and calculate the difference in proportions “by hand”.\nUse specify() and calculate() to calculate the difference in proportions as shown below.\n\n\nobs_diff_outcome &lt;- heart_transplant |&gt;\n  specify(response = outcome, explanatory = transplant, success = \"deceased\") |&gt;\n  calculate(stat = \"diff in props\", order = c(\"treatment\", \"control\"))\n\nThen, confirm that you obtained the same results with the two methods.\n\nExercise 4\n\nUsing the code below, generate a null distribution for the hypothesis test for the hypothesis you formulated in Exercise 1. Use 100 resamples (reps). However, change the seed to use a different seed.\n\nset.seed(1234)\n\nnull_dist_outcome &lt;- heart_transplant |&gt;\n  specify(response = outcome, explanatory = transplant, success = \"deceased\") |&gt;\n  hypothesize(null = \"independence\") |&gt;\n  generate(reps = 100, type = \"permute\") |&gt;\n  calculate(stat = \"diff in props\", order = c(\"treatment\", \"control\"))\n\nThen, inspect the object null_dist_outcome using either glimpse() or just printing it to screen. How many rows and how many columns does it have? What does each row represent? What does each variable represent?\n\nExercise 5\n\nVisualize the distribution of simulated differences in proportions of deceased between treatment and control groups. What is the shape of this distribution? What is the center of this distribution? Are these results expected? Explain your reasoning.\n\n\n\n\n\n\n\nHint\n\n\n\nThese simulated values are in the stat column of null_dist_outcome.\n\n\nExercise 6\n\nCalculate the p-value using the get_p_value function. Interpret the p-value in context of the data and the research question and comment on the statistical discernibility of this result using a discernibility level of 5%.\n\n\n\n\n\n\n\nPause and render\n\n\n\nNow that you’ve completed a few exercises, pause and render your document. If it renders without any issues, great! Move on to the next exercise. If it does not, debug the issue before moving on. Ask for help from your TA if you need. Do not proceed without rendering your document.",
    "crumbs": [
      "Labs",
      "Lab 6"
    ]
  },
  {
    "objectID": "labs/lab-6.html#part-2-heart-transplants---survtime",
    "href": "labs/lab-6.html#part-2-heart-transplants---survtime",
    "title": "Lab 6",
    "section": "Part 2: Heart transplants - survtime\n",
    "text": "Part 2: Heart transplants - survtime\n\nExercise 7\n\nRecreate the side-by–side box plots in Exercise 1. The variable of interest is survtime, which is the number of days patients were alive after the date they were determined to be a candidate for a heart transplant until the termination date of the study.\n\nExercise 8\n\nDo these data provide convincing evidence of a difference between average survival times of patient who did and did not get a heart transplant?\nWrite the hypotheses for answering this question and conduct the hypothesis test using randomization. Use 1,000 resamples. In order to avoid name collision, use object names like obs_diff_survtime, null_dist_survtime, and p_value_survtime. Find and interpret the p-value in context of the data and the research question and comment on the statistical discernability of this result using a discernability level of 5%.",
    "crumbs": [
      "Labs",
      "Lab 6"
    ]
  },
  {
    "objectID": "labs/lab-6.html#part-3-more-ims-exercises",
    "href": "labs/lab-6.html#part-3-more-ims-exercises",
    "title": "Lab 6",
    "section": "Part 3: More IMS Exercises",
    "text": "Part 3: More IMS Exercises\nThe exercises in this section do not require code. Make sure to answer the questions in full sentences.\nExercise 9\n\nIMS - Chapter 11 exercises, #2: Identify the parameter, II.\n\nExercise 10\n\nIMS - Chapter 11 exercises, #6: Identify hypotheses, II.",
    "crumbs": [
      "Labs",
      "Lab 6"
    ]
  },
  {
    "objectID": "labs/lab-6.html#lastly",
    "href": "labs/lab-6.html#lastly",
    "title": "Lab 6",
    "section": "Lastly",
    "text": "Lastly\nRecommend some music for us to listen to while we grade this.",
    "crumbs": [
      "Labs",
      "Lab 6"
    ]
  },
  {
    "objectID": "labs/lab-6.html#wrap-up",
    "href": "labs/lab-6.html#wrap-up",
    "title": "Lab 6",
    "section": "Wrap up",
    "text": "Wrap up\nSubmitting\n\n\n\n\n\n\nImportant\n\n\n\nBefore you proceed, first, make sure that you have updated the document YAML with your name! Then, render your document one last time, for good measure.\n\n\nTo submit your assignment to Gradescope:\n\nGo to your Files pane and check the box next to the PDF output of your document (lab-6.pdf).\nThen, in the Files pane, go to More &gt; Export. This will download the PDF file to your computer. Save it somewhere you can easily locate, e.g., your Downloads folder or your Desktop.\nGo to the course Canvas page and click on Gradescope and then click on the assignment. You’ll be prompted to submit it.\nMark the pages associated with each exercise. All of the papers of your lab should be associated with at least one question (i.e., should be “checked”).\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you fail to mark the pages associated with an exercise, that exercise won’t be graded. This means, if you fail to mark the pages for all exercises, you will receive a 0 on the assignment. The TAs can’t mark your pages for you, and for them to be able to grade, you must mark them.",
    "crumbs": [
      "Labs",
      "Lab 6"
    ]
  },
  {
    "objectID": "labs/lab-6.html#grading",
    "href": "labs/lab-6.html#grading",
    "title": "Lab 6",
    "section": "Grading",
    "text": "Grading\n\n\nExercise\nPoints\n\n\n\nExercise 1\n6\n\n\nExercise 2\n4\n\n\nExercise 3\n4\n\n\nExercise 4\n6\n\n\nExercise 5\n5\n\n\nExercise 6\n4\n\n\nExercise 7\n5\n\n\nExercise 8\n7\n\n\nExercise 9\n5\n\n\nExercise 10\n4\n\n\nTotal\n50",
    "crumbs": [
      "Labs",
      "Lab 6"
    ]
  },
  {
    "objectID": "labs/lab-5.html",
    "href": "labs/lab-5.html",
    "title": "Lab 5",
    "section": "",
    "text": "By the end of this lab you will construct and interpret confidence intervals via bootstrapping.",
    "crumbs": [
      "Labs",
      "Lab 5"
    ]
  },
  {
    "objectID": "labs/lab-5.html#getting-started",
    "href": "labs/lab-5.html#getting-started",
    "title": "Lab 5",
    "section": "Getting started",
    "text": "Getting started\n\nUse these steps to navigate to the STA101 version of RStudio using the Duke Container Manager;\nUse these steps to download all of the lab 5 files from our Canvas page, upload them to your RStudio files, and move them into an appropriately named folder (lab-5, for instance);\nOnce lab-5.qmd is where it needs to be, open it, and verify that you can click the “Render” button in RStudio and get a PDF file. See this answer on Ed if you want more guidance here;\nNow proceed to complete the exercises in this lab.",
    "crumbs": [
      "Labs",
      "Lab 5"
    ]
  },
  {
    "objectID": "labs/lab-5.html#pointers",
    "href": "labs/lab-5.html#pointers",
    "title": "Lab 5",
    "section": "Pointers",
    "text": "Pointers\n\nFor all visualizations you create, be sure to include informative titles for the plot, axes, and legend;\nRespond in complete sentences as much as possible;\nBe sure to observe good code style:\n\nThere is a line break after each |&gt; in a pipeline or + in a ggplot;\nThere are spaces around = signs;\nThere is a space after each ,;\nCode is properly indented;\nCode doesn’t exceed 80 characters in each line, longer lines of code are spread across multiple lines with appropriately placed line breaks (so in the rendered PDF, your code shouldn’t run off the page);\nCode chunks are labeled, informatively and without spaces.",
    "crumbs": [
      "Labs",
      "Lab 5"
    ]
  },
  {
    "objectID": "labs/lab-5.html#packages",
    "href": "labs/lab-5.html#packages",
    "title": "Lab 5",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse) \nlibrary(tidymodels)",
    "crumbs": [
      "Labs",
      "Lab 5"
    ]
  },
  {
    "objectID": "labs/lab-5.html#part-1-lemurs",
    "href": "labs/lab-5.html#part-1-lemurs",
    "title": "Lab 5",
    "section": "Part 1: Lemurs",
    "text": "Part 1: Lemurs\nThis week we’ll be working with data from the Duke Lemur Center, which houses over 200 lemurs across 14 species – the most diverse population of lemurs on Earth, outside their native Madagascar.\n\nLemurs are the most threatened group of mammals on the planet, and 95% of lemur species are at risk of extinction. Our mission is to learn everything we can about lemurs – because the more we learn, the better we can work to save them from extinction. They are endemic only to Madagascar, so it’s essentially a one-shot deal: once lemurs are gone from Madagascar, they are gone from the wild.\n\n\nBy studying the variables that most affect their health, reproduction, and social dynamics, the Duke Lemur Center learns how to most effectively focus their conservation efforts. And the more we learn about lemurs, the better we can educate the public around the world about just how amazing these animals are, why they need to be protected, and how each and every one of us can make a difference in their survival.\n\n\nSource: TidyTuesday\n\nThe codebook for the deataset can be found here. While the TidyTuesday project used the full dataset, we’ll be working with a subset.\nExercise 1\n\nLoad the lemurs data from your data folder and save it as lemurs. Then, report which “types” of lemurs are represented in the sample and how many of each. Note that this information is in the taxon variable. You should refer back to the linked data dictionary to understand what the different values of taxon mean.\n\nExercise 2\n\nWhat is the mean weight of ring-tailed lemurs? Calculate, visualize, and interpret a 95% bootstrap confidence interval together with your point estimate. Use set.seed(123) and reps = 1000 to create your bootstrap confidence interval.\n\nHints:\nBelow is a step-by-step recipe for constructing and visualizing a confidence interval. The code snippets shown are not “complete”, they’re intended to guide you in the right direction.\n\nStep 1: Create a dataset of ring-tailed lemurs, and call it lemurs_rt. Note, these are taxon == \"LCAT\".\nStep 2: Calculate the mean weight of ring-tailed lemurs.\n\n\nobs_stat_mean_rt &lt;- lemurs_rt |&gt;\n  specify(response = weight_g) |&gt;\n  calculate(stat = \"mean\")\n\n\nStep 3: Construct a bootstrap distribution. Note: There is a generate() step, so you also need to set a seed before running the following.\n\n\nboot_dist_mean_rt &lt;- lemurs_rt |&gt;\n  specify(response = weight_g) |&gt;\n  generate(reps = 1000, type = \"bootstrap\") |&gt;\n  calculate(stat = \"mean\")\n\n\nStep 4: Calculate the bounds of the confidence interval. Don’t forget to interpret it as well!\n\n\nci_95_mean_rt &lt;- boot_dist_mean_rt |&gt;\n  get_confidence_interval(\n    point_estimate = obs_stat_mean_rt, \n    level = 0.95\n  )\n\n\nStep 5: Visualize the confidence interval, overlaying it on the bootstrap distribution.\n\n\nvisualize(boot_dist_mean_rt) +\n  shade_confidence_interval(endpoints = ci_95_mean_rt)\n\nExercise 3\n\nWhat is the median weight of ring-tailed lemurs? What is the median weight of mongoose lemurs? Report a 95% bootstrap confidence interval together with your point estimates. Use set.seed(123) and reps = 10000 to create your bootstrap confidence intervals.\n\nExercise 4\n\nYour friend has never taken a statistics course. Describe to your friend the process of “bootstrap sampling” in the previous exercise to create a bootstrap confidence interval in your own words. Walk your friend through the process of collecting a bootstrap sample, computing a statistic and then repeating). You may find it helpful to describe it as drawing pieces of paper from a bag like we did in class.\n\nHint: What would you write on the slips of paper? How many pieces of paper would be in the bag(s)? How many bag(s) would you need for the above exercise?\nExercise 5\n\nDo female lemurs differ in weight from male lemurs? Report the difference in mean weights between groups. Next, construct a 99% bootstrap distribution for the difference in means between groups and interpret it. If the interval covers 0, you might think there is no difference between groups. Does the interval cover 0? Use set.seed(123) and reps = 10000.\n\nExercise 6\n\nCreate a new column that tells you whether or not a lemur was born in the first or second half of the year (January through June vs July through December). Create a meaningful plot to illustrate the weights of each group of lemurs. Use theme_bw() to replace the default gray plot background. Based on your figure, which group of lemurs weighs more? Is the distribution of one or both groups skewed or symmetric? See here for more information about skew vs symmetric distributions.\n\nExercise 7\n\nIs there more variability in weight for lemurs born in the first half or second half of the year? Report the estimated standard deviation of each group. Report a 90% bootstrap confidence interval of s.d. for each group. Interpret the confidence intervals in context. Use set.seed(123) and remember to set reps = 10000.",
    "crumbs": [
      "Labs",
      "Lab 5"
    ]
  },
  {
    "objectID": "labs/lab-5.html#part-2-ims-exercises",
    "href": "labs/lab-5.html#part-2-ims-exercises",
    "title": "Lab 5",
    "section": "Part 2: IMS Exercises",
    "text": "Part 2: IMS Exercises\nThe exercises in this section do not require code. Make sure to answer the questions in full sentences.\nExercise 8\n\nIMS - Chapter 12 exercises, #2: Chronic illness.\n\nExercise 9\n\nIMS - Chapter 12 exercises, #6: Bootstrap distributions of \\(\\hat{p}\\), III.\n\nExercise 10\n\nIMS - Chapter 12 exercises, #8: Waiting at an ER.",
    "crumbs": [
      "Labs",
      "Lab 5"
    ]
  },
  {
    "objectID": "labs/lab-5.html#lastly",
    "href": "labs/lab-5.html#lastly",
    "title": "Lab 5",
    "section": "Lastly",
    "text": "Lastly\nRecommend some music for us to listen to while we grade this.",
    "crumbs": [
      "Labs",
      "Lab 5"
    ]
  },
  {
    "objectID": "labs/lab-5.html#wrap-up",
    "href": "labs/lab-5.html#wrap-up",
    "title": "Lab 5",
    "section": "Wrap up",
    "text": "Wrap up\nSubmitting\n\n\n\n\n\n\nImportant\n\n\n\nBefore you proceed, first, make sure that you have updated the document YAML with your name! Then, render your document one last time, for good measure.\n\n\nTo submit your assignment to Gradescope:\n\nGo to your Files pane and check the box next to the PDF output of your document (lab-5.pdf).\nThen, in the Files pane, go to More &gt; Export. This will download the PDF file to your computer. Save it somewhere you can easily locate, e.g., your Downloads folder or your Desktop.\nGo to the course Canvas page and click on Gradescope and then click on the assignment. You’ll be prompted to submit it.\nMark the pages associated with each exercise. All of the papers of your lab should be associated with at least one question (i.e., should be “checked”).\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you fail to mark the pages associated with an exercise, that exercise won’t be graded. This means, if you fail to mark the pages for all exercises, you will receive a 0 on the assignment. The TAs can’t mark your pages for you, and for them to be able to grade, you must mark them.\n\n\nGrading\n\n\nExercise\nPoints\n\n\n\nExercise 1\n3\n\n\nExercise 2\n6\n\n\nExercise 3\n7\n\n\nExercise 4\n5\n\n\nExercise 5\n5\n\n\nExercise 6\n7\n\n\nExercise 7\n8\n\n\nExercise 8\n3\n\n\nExercise 9\n3\n\n\nExercise 10\n3\n\n\nTotal\n50",
    "crumbs": [
      "Labs",
      "Lab 5"
    ]
  },
  {
    "objectID": "labs/lab-1.html",
    "href": "labs/lab-1.html",
    "title": "Lab 1: Hello R!",
    "section": "",
    "text": "The goal of this lab is to acquaint you with R and RStudio1.",
    "crumbs": [
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/lab-1.html#quarto",
    "href": "labs/lab-1.html#quarto",
    "title": "Lab 1: Hello R!",
    "section": "Quarto",
    "text": "Quarto\nlab-1.qmd is an example of a Quarto Markdown file. Quarto allows you to seamlessly combine written text and code to produce clean and professional looking reports. You will use this system to create all of your assignment submissions in this course. When you open your .qmd file, you see a block of text at the top in between dashed lines. This is called the YAML2, and it contains the settings for your document, like the title, author name, date, and what sort of document you want to create. In our case, it will always be a PDF.\nChange the author name to your name and update the date with today’s date. Click the Render button to render the document. What do you notice?\n\n\n\n\n\n\nNote\n\n\n\nTo avoid issues that can occur while rendering, it is a good idea to render early and often. At least after every exercise.",
    "crumbs": [
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/lab-1.html#packages",
    "href": "labs/lab-1.html#packages",
    "title": "Lab 1: Hello R!",
    "section": "Packages",
    "text": "Packages\nIn this lab we will work with three packages: the tidyverse package which is a collection of packages for doing data analysis in a “tidy” way, the datasauRus package which contains the data set for the first part of your lab.\n\nlibrary(tidyverse) \nlibrary(datasauRus)",
    "crumbs": [
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/lab-1.html#exercise-1",
    "href": "labs/lab-1.html#exercise-1",
    "title": "Lab 1: Hello R!",
    "section": "Exercise 1",
    "text": "Exercise 1\n\nBased on the help file, how many rows and how many columns does the datasaurus_dozen file have? What are the variables included in the data frame? Add your responses to your lab report under “Exercise 1”.\n\nLet’s take a look at the names of the data sets inside of datasaurus_dozen. To do this, we can make a frequency table of the “data set” variable. Run the code chunk below. Note: when you run the code chunk below, a table “prints” to the screen. In general, we say “print to screen” to mean that the output of your code should show up on your screen (when asked to ‘print to screen’ in an assignment, you should make sure the output displays in your rendered document).\n\ndatasaurus_dozen |&gt;\n  count(dataset)\n\n# A tibble: 13 × 2\n   dataset        n\n   &lt;chr&gt;      &lt;int&gt;\n 1 away         142\n 2 bullseye     142\n 3 circle       142\n 4 dino         142\n 5 dots         142\n 6 h_lines      142\n 7 high_lines   142\n 8 slant_down   142\n 9 slant_up     142\n10 star         142\n11 v_lines      142\n12 wide_lines   142\n13 x_shape      142\n\n\nThe original Datasaurus (dino) data was created by Alberto Cairo. The other Dozen were generated using simulated annealing and the process is described in the paper Same Stats, Different Graphs: Generating data sets with Varied Appearance and Identical Statistics through Simulated Annealing by Justin Matejka and George Fitzmaurice. In the paper, the authors simulate a variety of data sets that have the same summary statistics as the original Datasaurus but have very different data.\n\n\n\n\n\n\nNote\n\n\n\nYou can view the whole data frame by running the code view(datasaurus_dozen) in the console. This will open the data frame in a new tab. Try it out!",
    "crumbs": [
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/lab-1.html#exercise-2",
    "href": "labs/lab-1.html#exercise-2",
    "title": "Lab 1: Hello R!",
    "section": "Exercise 2",
    "text": "Exercise 2\n\nPlot y vs. x for the dino data set. Then, calculate the correlation coefficient between x and y for this data set. Make sure that this value is printed in your document.\n\nBelow is the code you will need to complete this exercise. Basically, the answer is already given, but you need to include relevant bits in your .qmd document and successfully render it and view the results.\nStart with the datasaurus_dozen and pipe it into the filter function to filter for observations where dataset == \"dino\". Store the resulting filtered data frame as a new data frame called dino_data.\n\ndino_data &lt;- datasaurus_dozen |&gt;\n  filter(dataset == \"dino\")\n\nThere is a lot going on here, so let’s slow down and unpack it a bit.\nFirst, the pipe operator: |&gt;, takes what comes before it and sends it as the first argument to what comes after it. So here, we’re saying filter the datasaurus_dozen data frame for observations where dataset == \"dino\".\nSecond, the assignment operator: &lt;-, assigns the name dino_data to the filtered data frame.\n\n\n\n\n\n\nNote\n\n\n\nNote in R you may use either &lt;- or = for an assignment operator. We’ll use &lt;- in this class as it’s the more commonly used assignment operator, but when you look for R help online, you might see = being used as well.\n\n\nNext, we need to visualize these data. We will use the ggplot function for this. Its first argument is the data you’re visualizing. Next we define the aesthetic mappings. In other words, the columns of the data that get mapped to certain aesthetic features of the plot, e.g. the x axis will represent the variable called x and the y axis will represent the variable called y. Then, we add another layer to this plot where we define which geometric shapes we want to use to represent each observation in the data. In this case we want these to be points, hence geom_point.\n\nggplot(dino_data, aes(x = x, y = y)) +\n  geom_point()\n\n\n\n\n\n\n\nFor the second part of this exercise, we need to calculate a summary statistic: the correlation coefficient. The correlation coefficient (r) measures the strength and direction of the linear association between two variables. You will see that some of the pairs of variables we plot do not have a linear relationship between them. This is exactly why we want to visualize first: visualize to assess the form of the relationship, and calculate r only if relevant.\nIn this case, calculating a correlation coefficient really doesn’t make sense since the relationship between x and y is definitely not linear, but is instead more ‘dinosaur-esque’.\nFor illustrative purposes only, let’s calculate the correlation coefficient between x and y.\n\ndino_data |&gt;\n  summarize(r = cor(x, y))\n\n# A tibble: 1 × 1\n        r\n    &lt;dbl&gt;\n1 -0.0645",
    "crumbs": [
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/lab-1.html#exercise-3",
    "href": "labs/lab-1.html#exercise-3",
    "title": "Lab 1: Hello R!",
    "section": "Exercise 3",
    "text": "Exercise 3\n\nPlot y vs. x for the star dataset. You can (and should) reuse code we introduced above, just replace the dataset name with the desired dataset. Then, calculate the correlation coefficient between x and y for this dataset. How does this value compare to the r of dino?\n\nTo begin, edit the name of the code chunks from ex-3-1 and ex-3-2 to something more meaningful, e.g: plot-star and r-star respectively.",
    "crumbs": [
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/lab-1.html#exercise-4",
    "href": "labs/lab-1.html#exercise-4",
    "title": "Lab 1: Hello R!",
    "section": "Exercise 4",
    "text": "Exercise 4\n\nFinally, let’s plot all datasets at once. In order to do this we will make use of faceting, given by the code below:\n\nggplot(datasaurus_dozen, aes(x = x, y = y, color = dataset)) +\n  geom_point() +\n  facet_wrap(~ dataset, ncol = 3)\n\nAnd we can use the group_by function to generate all the summary correlation coefficients. We’ll see these functions again and again.\n\ndatasaurus_dozen |&gt;\n  group_by(dataset) |&gt;\n  summarize(r = cor(x, y))",
    "crumbs": [
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/lab-1.html#exercise-5",
    "href": "labs/lab-1.html#exercise-5",
    "title": "Lab 1: Hello R!",
    "section": "Exercise 5",
    "text": "Exercise 5\n\nDescribe what |&gt; does. Hint: run the following two code chunks. What do you notice?\n\ndino_data |&gt;\n  summarize(\n    mu_x = mean(x),\n    mu_y = mean(y)\n  )\n\n\nsummarize(dino_data, mu_x = mean(x), mu_y = mean(y))",
    "crumbs": [
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/lab-1.html#exercise-6",
    "href": "labs/lab-1.html#exercise-6",
    "title": "Lab 1: Hello R!",
    "section": "Exercise 6",
    "text": "Exercise 6\n\nIn the above code chunk, identify each of the following as an argument or a function:\n\nsummarize\ndino_data\nmean\nx\ny\nmu_x = mean(x)",
    "crumbs": [
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/lab-1.html#exercise-7",
    "href": "labs/lab-1.html#exercise-7",
    "title": "Lab 1: Hello R!",
    "section": "Exercise 7",
    "text": "Exercise 7\n\nCombine the code from exercises 4 and 5 to compute the mean(x) and mean(y) for each data set. Print your result to the screen. What do you notice? What does this say about the importance of visualizing your data as opposed to only looking at summary statistics?",
    "crumbs": [
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/lab-1.html#exercise-8",
    "href": "labs/lab-1.html#exercise-8",
    "title": "Lab 1: Hello R!",
    "section": "Exercise 8",
    "text": "Exercise 8\n\nIMS - Chapter 1 exercises, #4: Cheaters, study components.",
    "crumbs": [
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/lab-1.html#exercise-9",
    "href": "labs/lab-1.html#exercise-9",
    "title": "Lab 1: Hello R!",
    "section": "Exercise 9",
    "text": "Exercise 9\n\nIMS - Chapter 1 exercises, #14: UN Votes.",
    "crumbs": [
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/lab-1.html#exercise-10",
    "href": "labs/lab-1.html#exercise-10",
    "title": "Lab 1: Hello R!",
    "section": "Exercise 10",
    "text": "Exercise 10\n\nIMS - Chapter 1 exercises, #16: Shows on Netflix.",
    "crumbs": [
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/lab-1.html#lastly",
    "href": "labs/lab-1.html#lastly",
    "title": "Lab 1: Hello R!",
    "section": "Lastly…",
    "text": "Lastly…\nRecommend some music for us to listen to while we grade this.",
    "crumbs": [
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/lab-1.html#submitting",
    "href": "labs/lab-1.html#submitting",
    "title": "Lab 1: Hello R!",
    "section": "Submitting",
    "text": "Submitting\n\n\n\n\n\n\nImportant\n\n\n\nBefore you proceed, first, make sure that you have updated the document YAML with your name! Then, render your document one last time, for good measure.\n\n\nTo submit your assignment to Gradescope:\n\nGo to your Files pane and check the box next to the PDF output of your document (lab-1.pdf).\nThen, in the Files pane, go to More &gt; Export. This will download the PDF file to your computer. Save it somewhere you can easily locate, e.g., your Downloads folder or your Desktop.\nGo to the course Canvas page and click on Gradescope and then click on the assignment. You’ll be prompted to submit it.\nMark the pages associated with each exercise. All of the papers of your lab should be associated with at least one question (i.e., should be “checked”).\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you fail to mark the pages associated with an exercise, that exercise won’t be graded. This means, if you fail to mark the pages for all exercises, you will receive a 0 on the assignment. The TAs can’t mark your pages for you, and for them to be able to grade, you must mark them.",
    "crumbs": [
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/lab-1.html#grading",
    "href": "labs/lab-1.html#grading",
    "title": "Lab 1: Hello R!",
    "section": "Grading",
    "text": "Grading\n\n\nExercise\nPoints\n\n\n\nExercise 1\n5\n\n\nExercise 2\n4\n\n\nExercise 3\n6\n\n\nExercise 4\n5\n\n\nExercise 5\n2\n\n\nExercise 6\n6\n\n\nExercise 7\n7\n\n\nExercise 8\n5\n\n\nExercise 9\n5\n\n\nExercise 10\n5\n\n\nTotal\n50",
    "crumbs": [
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/lab-1.html#acknowledgements",
    "href": "labs/lab-1.html#acknowledgements",
    "title": "Lab 1: Hello R!",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis assignment was adapted from a lab in Data Science in a Box.",
    "crumbs": [
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/lab-1.html#footnotes",
    "href": "labs/lab-1.html#footnotes",
    "title": "Lab 1: Hello R!",
    "section": "Footnotes",
    "text": "Footnotes\n\nR is a programming language, just like Python, Java, C++, and other things you may have heard of. RStudio is a platform for writing R code in an easy and organized way. It is an example of an integrated development environment (IDE).↩︎\nThat stands for “Yet Another Markup Language.” Now please promptly forget that.↩︎",
    "crumbs": [
      "Labs",
      "Lab 1"
    ]
  },
  {
    "objectID": "labs/lab-2.html",
    "href": "labs/lab-2.html",
    "title": "Lab 2",
    "section": "",
    "text": "The goal of this lab is to effectively visualize numerical and categorical data.\nFor all visualizations you create, be sure to include informative titles for the plot, axes, and legend!",
    "crumbs": [
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/lab-2.html#exercise-1",
    "href": "labs/lab-2.html#exercise-1",
    "title": "Lab 2",
    "section": "Exercise 1",
    "text": "Exercise 1\n\nCreate a bar plot of the results of games for NC Courage. Additionally, calculate the numbers of wins, losses, and ties. Write a one sentence narrative for your findings.\n\nHint: result is a categorical variable, so use a bar plot for the visualization and the count() function for calculating the frequencies of levels of this variable. This primer may help you get started with the plot.",
    "crumbs": [
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/lab-2.html#exercise-2",
    "href": "labs/lab-2.html#exercise-2",
    "title": "Lab 2",
    "section": "Exercise 2",
    "text": "Exercise 2\n\nCreate a new variable indicating whether the game was played at home or away for NC Courage. This variable should be called home_courage and take the value “home” if NC Courage is the home team and “away” if NC Courage is the away team. (Instructions for how to do this are given below.)\nThen, calculate the number of home and away games, and write a one sentence narrative for your findings.\n\nUse the example code below to get started.\n\ncourage &lt;- courage |&gt;\n  mutate(home_courage = if_else(home_team == \"NC\", \"home\", \"away\"))\n\nThere are two things of note here:\n\nThe use of the assignment operator (&lt;-) to assign the resulting data frame to courage, thus overwriting the courage dataset to contain this new column. We do this because we will use this new variable, home_courage, in a subsequent exercise.\n\nThe use of a new function, if_else() to determine whether the game is played at home or away.\n\n\nhome_team == \"NC\" finds all rows where the home team is NC Courage.\nIf the home team is NC Courage, then we set the value of home_courage to `“home”.\nOtherwise (else) Courage must be the away team and we set the value of home_courage to \"away\".",
    "crumbs": [
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/lab-2.html#exercise-3",
    "href": "labs/lab-2.html#exercise-3",
    "title": "Lab 2",
    "section": "Exercise 3",
    "text": "Exercise 3\n\n\nThis code creates a visualization that displays the relationship between home_courage and result:\n\n\nggplot(courage, aes(x = home_courage, fill = result)) +\n  geom_bar(position = \"fill\")\n\n\n\n\n\n\n\nExplain what each piece of the code is doing. Why does it produce the plot that it produces?\nHint: to understand what the different ingredients do, try removing or altering some of them, and see how it changes the plot.\n\n\n\nCalculate the proportions of home and away games that the Courage won. Based on these, do your findings suggest a home-field advantage? Why or why not?\n\n\nSo far we have focused on whether the game was at home or away and whether the Courage won. Next, we dive deeper and focus on the number of points the Courage wins by, at home and away.",
    "crumbs": [
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/lab-2.html#exercise-4",
    "href": "labs/lab-2.html#exercise-4",
    "title": "Lab 2",
    "section": "Exercise 4",
    "text": "Exercise 4\n\nHow many points do the Courage typically win by (on average)? Use the example code below to get started. You’ll encounter a new function: abs() is the absolute value function. It takes the absolute value of a number. Why do we want to use this absolute value function here?\n\nHint: We are only interested in games the Courage wins, therefore we should filter() for those games first.\n\ncourage |&gt;\n  filter(___) |&gt;\n  mutate(win_pts = abs(home_pts - away_pts)) |&gt;\n  summarize(___)",
    "crumbs": [
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/lab-2.html#exercise-5",
    "href": "labs/lab-2.html#exercise-5",
    "title": "Lab 2",
    "section": "Exercise 5",
    "text": "Exercise 5\n\nHow many points do NC Courage score when they win (on average)? Note this is different than how many points they “win by”. How many points do the Courage score when they lose on average?\n\nTo calculate this we first need to determine how many points NC Courage scored in every game. We can use if_else() logic again to find this value for each game, and store it in a new column, courage_pts.\n\ncourage &lt;- courage |&gt;\n  mutate(courage_pts = if_else(home_team == \"NC\", home_pts, away_pts))\n\ncourage |&gt;\n  group_by(___) |&gt;\n  summarize(___)",
    "crumbs": [
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/lab-2.html#exercise-6",
    "href": "labs/lab-2.html#exercise-6",
    "title": "Lab 2",
    "section": "Exercise 6",
    "text": "Exercise 6\n\nNext we’ll investigate visually whether or not NC Courage has a home-field advantage. Mutate the courage data frame to create two new variables:\n\ntotal_pts: Sum of points scored by both teams, i.e. home_pts + away_pts.\nopponent_pts: Points scored by the opposing team, i.e., total_pts - courage_pts.\n\nSave the resulting data frame as courage again and print the three points columns (total_pts, opponent_pts, courage_pts) to screen.\n\nHint:\n\nUse the mutate() function to create the columns.\n\n\ncourage &lt;- courage |&gt;\n  mutate(\n    total_pts = ___,\n    opponent_pts = ___\n    )\n\n\nUse the select() function to print them to screen:\n\n\ncourage |&gt;\n  select(total_pts, opponent_pts, courage_pts)",
    "crumbs": [
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/lab-2.html#exercise-7",
    "href": "labs/lab-2.html#exercise-7",
    "title": "Lab 2",
    "section": "Exercise 7",
    "text": "Exercise 7\n\nCreate a scatter plot:\n\nopponent_pts (y) vs. courage_pts (x)\nColor the scatter plot by whether NC Courage are home or away.\nRepresent the data with “jittered” points wth geom_jitter().\nOverlay a \\(y = x\\) line with geom_abline().\nFaceted by season.\n\nWhat does the line represent? What does it mean for a point to fall above the line? Below the line?\n\n\nggplot(courage, aes(x = ___, y = ___, color = ___)) + \n  geom_jitter(width = 0.1, height = 0.1) + \n  geom_abline(slope = 1, intercept = 0) +\n  facet_wrap(~ ___) +\n  labs(\n    x = \"___\", \n    y = \"___\", \n    title = \"___\", \n    color = \"___\"\n  )",
    "crumbs": [
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/lab-2.html#exercise-8",
    "href": "labs/lab-2.html#exercise-8",
    "title": "Lab 2",
    "section": "Exercise 8",
    "text": "Exercise 8\n\nIf we want to formally test whether the Courage have a home-field advantage, then we must first define what this means! In your own words, what do you think a home-field advantage means? Then, now that you’ve defined what it means to have a home field advantage, define what it means to not have a home-field advantage.\n\n\n\n\n\n\n\nNote\n\n\n\nWhile there is a right answer, this part is graded for completion, so don’t worry too much about answering this in exactly the right way. Although graded for completion, your response must make sense to receive full points.",
    "crumbs": [
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/lab-2.html#exercise-9",
    "href": "labs/lab-2.html#exercise-9",
    "title": "Lab 2",
    "section": "Exercise 9",
    "text": "Exercise 9\n\nIMS - Chapter 2 exercises, #20: Vitamin supplements.",
    "crumbs": [
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/lab-2.html#exercise-10",
    "href": "labs/lab-2.html#exercise-10",
    "title": "Lab 2",
    "section": "Exercise 10",
    "text": "Exercise 10\n\nIMS - Chapter 2 exercises, #30: Screens, teens, and psychological well-being.",
    "crumbs": [
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/lab-2.html#lastly",
    "href": "labs/lab-2.html#lastly",
    "title": "Lab 2",
    "section": "Lastly…",
    "text": "Lastly…\nRecommend some music for us to listen to while we grade this.",
    "crumbs": [
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/lab-2.html#submitting",
    "href": "labs/lab-2.html#submitting",
    "title": "Lab 2",
    "section": "Submitting",
    "text": "Submitting\n\n\n\n\n\n\nImportant\n\n\n\nBefore you proceed, first, make sure that you have updated the document YAML with your name! Then, render your document one last time, for good measure.\n\n\nTo submit your assignment to Gradescope:\n\nGo to your Files pane and check the box next to the PDF output of your document (lab-2.pdf).\nThen, in the Files pane, go to More &gt; Export. This will download the PDF file to your computer. Save it somewhere you can easily locate, e.g., your Downloads folder or your Desktop.\nGo to the course Canvas page and click on Gradescope and then click on the assignment. You’ll be prompted to submit it.\nMark the pages associated with each exercise. All of the papers of your lab should be associated with at least one question (i.e., should be “checked”).\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you fail to mark the pages associated with an exercise, that exercise won’t be graded. This means, if you fail to mark the pages for all exercises, you will receive a 0 on the assignment. The TAs can’t mark your pages for you, and for them to be able to grade, you must mark them.",
    "crumbs": [
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/lab-2.html#grading",
    "href": "labs/lab-2.html#grading",
    "title": "Lab 2",
    "section": "Grading",
    "text": "Grading\n\n\nExercise\nPoints\n\n\n\nExercise 1\n5\n\n\nExercise 2\n5\n\n\nExercise 3\n6\n\n\nExercise 4\n6\n\n\nExercise 5\n6\n\n\nExercise 6\n4\n\n\nExercise 7\n6\n\n\nExercise 8\n2\n\n\nExercise 9\n5\n\n\nExercise 10\n5\n\n\nTotal\n50",
    "crumbs": [
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/lab-2.html#acknowledgements",
    "href": "labs/lab-2.html#acknowledgements",
    "title": "Lab 2",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nThis assignment was adapted from a similar exercise by Dr. Alex Fisher.",
    "crumbs": [
      "Labs",
      "Lab 2"
    ]
  },
  {
    "objectID": "labs/lab-3.html",
    "href": "labs/lab-3.html",
    "title": "Lab 3",
    "section": "",
    "text": "The goal of this lab is to solidify some of the skills and concepts from the first two weeks of the course.",
    "crumbs": [
      "Labs",
      "Lab 3"
    ]
  },
  {
    "objectID": "labs/lab-3.html#getting-started",
    "href": "labs/lab-3.html#getting-started",
    "title": "Lab 3",
    "section": "Getting started",
    "text": "Getting started\n\nUse these steps to navigate to the STA101 version of RStudio using the Duke Container Manager;\nUse these steps to download all of the lab 3 files from our Canvas page, upload them to your RStudio files, and move them into an appropriately named folder (lab-3, for instance);\nOnce lab-3.qmd is where it needs to be, open it, and verify that you can click the “Render” button in RStudio and get a PDF file. See this answer on Ed if you want more guidance here;\nNow proceed to complete the exercises in this lab.",
    "crumbs": [
      "Labs",
      "Lab 3"
    ]
  },
  {
    "objectID": "labs/lab-3.html#pointers",
    "href": "labs/lab-3.html#pointers",
    "title": "Lab 3",
    "section": "Pointers",
    "text": "Pointers\n\nFor all visualizations you create, be sure to include informative titles for the plot, axes, and legend;\nRespond in complete sentences as much as possible;\nBe sure to observe good code style:\n\nThere is a line break after each |&gt; in a pipeline or + in a ggplot;\nThere are spaces around = signs;\nThere is a space after each ,;\nCode is properly indented;\nCode doesn’t exceed 80 characters in each line, longer lines of code are spread across multiple lines with appropriately placed line breaks (so in the rendered PDF, your code shouldn’t run off the page);\nCode chunks are labeled, informatively and without spaces.",
    "crumbs": [
      "Labs",
      "Lab 3"
    ]
  },
  {
    "objectID": "labs/lab-3.html#packages",
    "href": "labs/lab-3.html#packages",
    "title": "Lab 3",
    "section": "Packages",
    "text": "Packages\nIn this lab we will work with the tidyverse packages, which is a collection of packages for doing data analysis in a “tidy” way.\n\nlibrary(tidyverse)",
    "crumbs": [
      "Labs",
      "Lab 3"
    ]
  },
  {
    "objectID": "labs/lab-3.html#part-1-nobel-laureates",
    "href": "labs/lab-3.html#part-1-nobel-laureates",
    "title": "Lab 3",
    "section": "Part 1: Nobel laureates",
    "text": "Part 1: Nobel laureates\nWe will now consider a data set on the characteristics of winners (laureates) of a Nobel prize: nobel.csv. You can read it in using the following.\n\nnobel &lt;- read_csv(\"data/nobel.csv\")\n\n\n\n\n\n\n\nNote\n\n\n\nRecall that you may have to adjust the file path to match how you have organized your files and folders.\n\n\nThe descriptions of the variables are as follows:\n\n\nid: ID number\n\nfirstname: First name of laureate\n\nsurname: Surname\n\nyear: Year prize won\n\ncategory: Category of prize\n\naffiliation: Affiliation of laureate\n\ncity: City of laureate in prize year\n\ncountry: Country of laureate in prize year\n\nborn_date: Birth date of laureate\n\ndied_date: Death date of laureate\n\ngender: Gender of laureate\n\nborn_city: City where laureate was born\n\nborn_country: Country where laureate was born\n\nborn_country_code: Code of country where laureate was born\n\ndied_city: City where laureate died\n\ndied_country: Country where laureate died\n\ndied_country_code: Code of country where laureate died\n\noverall_motivation: Overall motivation for recognition\n\nshare: Number of other winners award is shared with\n\nmotivation: Motivation for recognition\n\nIn a few cases the name of the city/country changed after laureate was given (e.g. in 1975 Bosnia and Herzegovina was called the Socialist Federative Republic of Yugoslavia). In these cases the variables below reflect a different name than their counterparts without the suffix _original.\n\n\nborn_country_original: Original country where laureate was born\n\nborn_city_original: Original city where laureate was born\n\ndied_country_original: Original country where laureate died\n\ndied_city_original: Original city where laureate died\n\ncity_original: Original city where laureate lived at the time of winning the award\n\ncountry_original: Original country where laureate lived at the time of winning the award\n\nThere are some observations in this dataset that we will exclude from our analysis. This code creates a new data frame called nobel_living_science that filters for\n\nlaureates for whom country is available: !is.na(country);\nlaureates who are people as opposed to organizations, i.e., organizations are denoted with \"org\" as their gender: gender != \"org\";\nlaureates who are still alive, i.e., their died_date is NA: is.na(died_date).\nlaureates in the sciences (so not literature or peace).\n\n\nnobel_living_science &lt;- nobel |&gt;\n  filter(!is.na(country) & gender != \"org\" & is.na(died_date)) |&gt;\n  filter(category %in% c(\"Physics\", \"Medicine\", \"Chemistry\", \"Economics\"))\n\nOnce you have filtered for these characteristics you are left with a data frame with 228 observations (check this!).\n\nMost living Nobel laureates were based in the US when they won their prizes\n\n… says this Buzzfeed article. Let’s see if that’s true.\nFirst, we’ll create a new variable to identify whether the laureate was in the US when they won their prize. We’ll use the mutate() function for this. The following pipeline mutates the nobel_living_science data frame by adding a new variable called country_us. We use an if statement to create this variable. The first argument in the if_else() function we’re using to write this if statement is the condition we’re testing for. If country is equal to \"USA\", we set country_us to \"USA\". If not, we set the country_us to \"Other\".\n\nnobel_living_science &lt;- nobel_living_science |&gt;\n  mutate(\n    country_us = if_else(country == \"USA\", \"USA\", \"Other\")\n  )\n\nFor the following exercises, work with the nobel_living_science data frame you created above.\nExercise 1\n\nCreate a faceted bar plot visualizing the relationship between the category of prize and whether the laureate was in the US when they won the Nobel prize. Interpret your visualization, and say a few words about whether the Buzzfeed headline is supported by the data.\n\nYour visualization should be faceted by category;\nFor each facet you should have two bars, one for winners in the US and one for Other;\nFlip the coordinates so the bars are horizontal, not vertical;\nMake sure everything is clearly labeled!\n\n\nExercise 2\nNext, let’s investigate, of those US-based Nobel laureates, what proportion were born in other countries.\n\nCreate a new variable called born_country_us in nobel_living_science that has the value \"USA\" if the laureate is born in the US, and \"Other\" otherwise. How many of the winners are born in the US?\n\nExercise 3\n\nAdd a second variable to your visualization from Exercise 1 based on whether the laureate was born in the US or not.\nCreate two visualizations with this new variable added:\n\nPlot 1: Segmented frequency bar plot\nPlot 2: Segmented relative frequency bar plot (Hint: Add position = \"fill\" to geom_bar().)\n\nHere are some instructions that apply to both of these visualizations:\n\nYour final visualization should contain a facet for each category.\nWithin each facet, there should be two bars for whether the laureate won the award in the US or not.\nEach bar should have segments for whether the laureate was born in the US or not.\n\nWhich of these visualizations is a better fit for answering the following question: “Do the data appear to support Buzzfeed’s claim that of those US-based Nobel laureates, many were born in other countries?” First, state which plot you’re using to answer the question. Then, answer the question, explaining your reasoning in 1-2 sentences.",
    "crumbs": [
      "Labs",
      "Lab 3"
    ]
  },
  {
    "objectID": "labs/lab-3.html#part-2-practicing-our-coding-basics",
    "href": "labs/lab-3.html#part-2-practicing-our-coding-basics",
    "title": "Lab 3",
    "section": "Part 2: practicing our coding basics",
    "text": "Part 2: practicing our coding basics\nExercise 4\nThe file nsw-crime.csv contains monthly data on all criminal incidents recorded by police in New South Wales, Australia (the state that includes the city of Sydney). So a row in this data set corresponds to a month, and a column corresponds to a crime (murder, dealing cannabis, escaping custody, etc). The data count the number of cases of each crime in each month. In this exercise we will consider the variable offensive_language, an incident of disorderly conduct where the perpetrator…said some things.\n\nLoad the crime data into R and create four histogram plots of the variable offensive_language. In each plot, use a different number of histogram bins: 20, 40, 80, and 160. Decide which picture you think best visualizes the distribution of the crime counts, and explain why you think this.\n\nExercise 5\nLoad the data about the COVID delta variant (adjusting your file path as needed):\n\ndelta &lt;- read_csv(\"delta.csv\")\n\n\nConsider this sequence of commands:\n\n# part 1\n\ndelta |&gt;\n  count(vaccine, outcome)\n\n# part 2\n\ndelta |&gt;\n  count(vaccine, outcome) |&gt;\n  group_by(vaccine)\n\n# part 3\n\ndelta |&gt;\n  count(vaccine, outcome) |&gt;\n  group_by(vaccine) |&gt;\n  mutate(prop = n / sum(n))\n\nPick this code apart like a vulture and explain in complete sentences what is going on in each part. The more detail, the better.",
    "crumbs": [
      "Labs",
      "Lab 3"
    ]
  },
  {
    "objectID": "labs/lab-3.html#part-3-ims-exercises",
    "href": "labs/lab-3.html#part-3-ims-exercises",
    "title": "Lab 3",
    "section": "Part 3: IMS Exercises",
    "text": "Part 3: IMS Exercises\nThe exercises in this section do not require code. Make sure to answer the questions in full sentences.\nExercise 6\n\nIMS - Chapter 4 exercises, #4: Raise taxes.\n\nExercise 7\n\nIMS - Chapter 5 exercises, #4: Office productivity.\n\nExercise 8\n\nIMS - Chapter 5 exercises, #15: Distributions and appropriate statistics.\n\nExercise 9\n\nIMS - Chapter 5 exercises, #26: NYC marathon winners.",
    "crumbs": [
      "Labs",
      "Lab 3"
    ]
  },
  {
    "objectID": "labs/lab-3.html#part-4-waxing-poetic",
    "href": "labs/lab-3.html#part-4-waxing-poetic",
    "title": "Lab 3",
    "section": "Part 4: waxing poetic",
    "text": "Part 4: waxing poetic\nExercise 10\n\nDescribe a situation from your everyday life where you have to make a decision, but it is difficult to make a good decision because you face some uncertainty about the world. Common examples (which you should not now use!) might include deciding when to get food at the dining hall, because you are uncertain about the length of the lines, or deciding when to have a picnic, because you are uncertain about the weather. Next, describe some data (information) that, if you had access to it, would resolve some of the uncertainty you face and help you make a better decision. Describe two versions of the data: an ideal version that would theoretically resolve all of the uncertainty, and an imperfect version that you are more likely to actually encounter in practice. How would you use these data to guide your decision making?\n\n\n\n\n\n\n\nNote\n\n\n\nThis is basically going to be graded for completion, but I hope some of you will get creative, and I look forward to reading these!",
    "crumbs": [
      "Labs",
      "Lab 3"
    ]
  },
  {
    "objectID": "labs/lab-3.html#lastly",
    "href": "labs/lab-3.html#lastly",
    "title": "Lab 3",
    "section": "Lastly",
    "text": "Lastly\nRecommend some music for us to listen to while we grade this.",
    "crumbs": [
      "Labs",
      "Lab 3"
    ]
  },
  {
    "objectID": "labs/lab-3.html#wrap-up",
    "href": "labs/lab-3.html#wrap-up",
    "title": "Lab 3",
    "section": "Wrap up",
    "text": "Wrap up\nSubmitting\n\n\n\n\n\n\nImportant\n\n\n\nBefore you proceed, first, make sure that you have updated the document YAML with your name! Then, render your document one last time, for good measure.\n\n\nTo submit your assignment to Gradescope:\n\nGo to your Files pane and check the box next to the PDF output of your document (lab-3.pdf).\nThen, in the Files pane, go to More &gt; Export. This will download the PDF file to your computer. Save it somewhere you can easily locate, e.g., your Downloads folder or your Desktop.\nGo to the course Canvas page and click on Gradescope and then click on the assignment. You’ll be prompted to submit it.\nMark the pages associated with each exercise. All of the papers of your lab should be associated with at least one question (i.e., should be “checked”).\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you fail to mark the pages associated with an exercise, that exercise won’t be graded. This means, if you fail to mark the pages for all exercises, you will receive a 0 on the assignment. The TAs can’t mark your pages for you, and for them to be able to grade, you must mark them.",
    "crumbs": [
      "Labs",
      "Lab 3"
    ]
  },
  {
    "objectID": "labs/lab-3.html#grading",
    "href": "labs/lab-3.html#grading",
    "title": "Lab 3",
    "section": "Grading",
    "text": "Grading\n\n\nExercise\nPoints\n\n\n\nExercise 1\n8\n\n\nExercise 2\n6\n\n\nExercise 3\n8\n\n\nExercise 4\n6\n\n\nExercise 5\n6\n\n\nExercise 6\n2\n\n\nExercise 7\n2\n\n\nExercise 8\n5\n\n\nExercise 9\n4\n\n\nExercise 10\n3\n\n\nTotal\n50",
    "crumbs": [
      "Labs",
      "Lab 3"
    ]
  },
  {
    "objectID": "labs/lab-4.html",
    "href": "labs/lab-4.html",
    "title": "Lab 4",
    "section": "",
    "text": "Use these steps to navigate to the STA101 version of RStudio using the Duke Container Manager;\nUse these steps to download all of the lab 4 files from our Canvas page, upload them to your RStudio files, and move them into an appropriately named folder (lab-4, for instance);\nOnce lab-4.qmd is where it needs to be, open it, and verify that you can click the “Render” button in RStudio and get a PDF file. See this answer on Ed if you want more guidance here;\nNow proceed to complete the exercises in this lab.",
    "crumbs": [
      "Labs",
      "Lab 4"
    ]
  },
  {
    "objectID": "labs/lab-4.html#getting-started",
    "href": "labs/lab-4.html#getting-started",
    "title": "Lab 4",
    "section": "",
    "text": "Use these steps to navigate to the STA101 version of RStudio using the Duke Container Manager;\nUse these steps to download all of the lab 4 files from our Canvas page, upload them to your RStudio files, and move them into an appropriately named folder (lab-4, for instance);\nOnce lab-4.qmd is where it needs to be, open it, and verify that you can click the “Render” button in RStudio and get a PDF file. See this answer on Ed if you want more guidance here;\nNow proceed to complete the exercises in this lab.",
    "crumbs": [
      "Labs",
      "Lab 4"
    ]
  },
  {
    "objectID": "labs/lab-4.html#pointers",
    "href": "labs/lab-4.html#pointers",
    "title": "Lab 4",
    "section": "Pointers",
    "text": "Pointers\n\nFor all visualizations you create, be sure to include informative titles for the plot, axes, and legend;\nRespond in complete sentences as much as possible;\nBe sure to observe good code style:\n\nThere is a line break after each |&gt; in a pipeline or + in a ggplot;\nThere are spaces around = signs;\nThere is a space after each ,;\nCode is properly indented;\nCode doesn’t exceed 80 characters in each line, longer lines of code are spread across multiple lines with appropriately placed line breaks (so in the rendered PDF, your code shouldn’t run off the page);\nCode chunks are labeled, informatively and without spaces.",
    "crumbs": [
      "Labs",
      "Lab 4"
    ]
  },
  {
    "objectID": "labs/lab-4.html#packages",
    "href": "labs/lab-4.html#packages",
    "title": "Lab 4",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse) \nlibrary(tidymodels)",
    "crumbs": [
      "Labs",
      "Lab 4"
    ]
  },
  {
    "objectID": "labs/lab-4.html#part-1-do-you-even-lift",
    "href": "labs/lab-4.html#part-1-do-you-even-lift",
    "title": "Lab 4",
    "section": "Part 1: Do you even lift?",
    "text": "Part 1: Do you even lift?\nToday, we will be working with data from www.openpowerlifting.org. This data was sourced from Tidy Tuesday and contains international powerlifting records at various meets. At each meet, each lifter gets three attempts at lifting max weight on three lifts: the bench press, squat and deadlift.\n\nipf &lt;- read_csv(\"data/ipf.csv\")\n\n\n\n\n\n\n\nNote\n\n\n\nRecall that you may have to adjust the file path to match how you have organized your files and folders.\n\n\nThe data dictionary for this dataset from TidyTuesday is reproduced below:\n\n\n\n\n\n\nvariable\ndescription\n\n\n\nname\nIndividual lifter name\n\n\nsex\nBinary gender (M/F)\n\n\nevent\n\nThe type of competition that the lifter entered. Values are as follows:\n\nSBD: Squat-Bench-Deadlift, also commonly called “Full Power”\nBD: Bench-Deadlift, also commonly called “Ironman” or “Push-Pull”\nSD: Squat-Deadlift, very uncommon\nSB: Squat-Bench, very uncommon\nS: Squat-only\nB: Bench-only\nD: Deadlift-only\n\n\n\n\nequipment\n\nThe equipment category under which the lifts were performed. Values are as follows:\n\nRaw: Bare knees or knee sleeves\nWraps: Knee wraps were allowed\nSingle-ply: Equipped, single-ply suits\nMulti-ply: Equipped, multi-ply suits (includes Double-ply)\nStraps: Allowed straps on the deadlift (used mostly for exhibitions, not real meets)\n\n\n\n\nage\nThe age of the lifter on the start date of the meet, if known.\n\n\nage_class\nThe age class in which the filter falls, for example 40-45\n\n\n\ndivision\nFree-form UTF-8 text describing the division of competition, like Open or Juniors 20-23 or Professional.\n\n\nbodyweight_kg\nThe recorded bodyweight of the lifter at the time of competition, to two decimal places.\n\n\nweight_class_kg\n\nThe weight class in which the lifter competed, to two decimal places.\nWeight classes can be specified as a maximum or as a minimum. Maximums are specified by just the number, for example 90 means “up to (and including) 90kg.” minimums are specified by a + to the right of the number, for example 90+ means “above (and excluding) 90kg.”\n\n\n\nbest3squat_kg\n\nMaximum of the first three successful attempts for the lift.\nRarely may be negative: that is used by some federations to report the lowest weight the lifter attempted and failed.\n\n\n\nbest3bench_kg\n\nMaximum of the first three successful attempts for the lift.\nRarely may be negative: that is used by some federations to report the lowest weight the lifter attempted and failed.\n\n\n\nbest3deadlift_kg\n\nMaximum of the first three successful attempts for the lift.\nRarely may be negative: that is used by some federations to report the lowest weight the lifter attempted and failed.\n\n\n\nplace\n\nThe recorded place of the lifter in the given division at the end of the meet. Values are as follows:\n\nPositive number: the place the lifter came in.\nG: Guest lifter. The lifter succeeded, but wasn’t eligible for awards.\nDQ: Disqualified. Note that DQ could be for procedural reasons, not just failed attempts.\nDD: Doping Disqualification. The lifter failed a drug test.\nNS: No-Show. The lifter did not show up on the meet day.\n\n\n\n\ndate\nISO 8601 Date of the event\n\n\nfederation\nThe federation that hosted the meet. (limited to IPF for this data subset)\n\n\nmeet_name\nThe name of the meet. The name is defined to never include the year or the federation. For example, the meet officially called 2019 USAPL Raw National Championships would have the MeetName Raw National Championshps.\n\n\n\nFor all of the following exercises, you should include units on axes labels, e.g. “Bench press (lbs)” or “Bench press (kg)”. “Age (years)” etc. This is good practice.\nExercise 1\n\nLet’s begin by taking a look at the squat lifting records.\nTo begin, remove any observations that are negative for squat. Next, create a new column called best3_squat_lbs that converts the record from kg to lbs (you may have to Google the conversion). Save your data frame as ipf_squat. Report the number of rows and columns of this new data frame.\n\n\n\n\n\n\n\nHint\n\n\n\nFirst, you’re taking a dataset and filtering it for certain records, and then you’re mutate-ing that dataset to gain a new column, and you’re assigning the resulting dataset to a new object called ipf_squat.\n\n\nExercise 2\n\nUsing ipf_squat from the previous exercise, create a scatter plot to investigate the relationship between squat (in lbs) and age. Age should be on the x-axis. Adjust the alpha level of your points to get a better sense of the density of the data. Add a linear trend-line. Be sure to label all axes and give the plot a title. Comment on what you observe.\n\nExercise 3\n\nWrite down the linear model to predict lift squat lbs from age in \\(x\\), \\(y\\), \\(\\beta\\) notation. What is \\(x\\)? What is \\(y\\)? Next, fit the linear model, and save it as age_fit. Re-write your previous equation replacing \\(\\beta\\) with the numeric estimates. This is called the “fitted” linear model. Interpret each estimate of \\(\\beta\\). Are the interpretations sensible?\n\nExercise 4\n\nBuilding on your ipf_squat data frame, create a new column called age2 that takes the age of each lifter and squares it. Save it to your data frame ipf_squat. Next, plot squat in lbs vs age2 and add a linear best fit line. Does this model look like it fits the data better?\n\n\n\n\n\n\n\nHint\n\n\n\nTo raise a value to a power, use ^ in R, e.g.: 2 ^ 2 gives you 4, 2 ^ 3 gives you 8, etc.\n\n\nExercise 5\n\nOne metric to assess the fit of a model is \\(R^2\\). Fit the age\\(^2\\) model and save the object as age2_fit. Compare \\(R^2\\) of the age\\(^2\\) model to the \\(R^2\\) of the model from Exercise 3. Which has a higher \\(R^2\\)?\n\nExercise 6\n\nNext, let’s turn our attention to dead lifting records.\nRecreate the plot below. Make sure axes and title labels are exactly matching, including spelling, capitalization, etc. Based on the plot below, which impacts deadlift weight more, age category or sex?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHint 1\n\n\n\nYou will need to create a couple of new columns. One to classify age appropriately and one to convert best3deadlift_kg to the plotted units (lbs). Notice that there are no negative deadlift values on the x-axis.\n\n\n\n\n\n\n\n\nHint 2\n\n\n\nThese plots are called density plots. It’s just a smoothed out version of a histogram, and like the histogram it lets you visualize the distribution (center, spread, skew, etc) of a dataset. The geom_... that creates density plots is geom_density. Try it out!\n\n\nExercise 7\n\nFinally, let’s turn our attention to bench press records.\nTo begin, remove any observations that are negative for bench press, create two new columns: best3bench_lbs and bodyweight_lbs. Save the result in a new data frame called ipf_bench.\nThen, create a scatter plot to investigate the relationship between best bench press (in lbs) and the lifter’s bodyweight (in lbs). Bodyweight should be on the x-axis. Add a linear trend-line. Be sure to label all axes and give the plot a title. Comment on what you observe.\n\nExercise 8\n\nFit the linear model displayed in the previous exercise and write down the fitted model equation only, replacing \\(\\hat{\\beta}\\)s with their fitted estimates. Interpret the \\(\\hat{\\beta}\\)s (intercept and slope). Report \\(R^2\\). Is body weight an important predictor of bench press ability? Why or why not?",
    "crumbs": [
      "Labs",
      "Lab 4"
    ]
  },
  {
    "objectID": "labs/lab-4.html#part-2-ims-exercises",
    "href": "labs/lab-4.html#part-2-ims-exercises",
    "title": "Lab 4",
    "section": "Part 2: IMS Exercises",
    "text": "Part 2: IMS Exercises\nThe exercises in this section do not require code. Make sure to answer the questions in full sentences.\nExercise 9\n\nIMS - Chapter 7 exercises, #18: Over-under, II.\n\nExercise 10\n\nIMS - Chapter 7 exercises, #24: Cats weights.",
    "crumbs": [
      "Labs",
      "Lab 4"
    ]
  },
  {
    "objectID": "labs/lab-4.html#lastly",
    "href": "labs/lab-4.html#lastly",
    "title": "Lab 4",
    "section": "Lastly",
    "text": "Lastly\nRecommend some music for us to listen to while we grade this.",
    "crumbs": [
      "Labs",
      "Lab 4"
    ]
  },
  {
    "objectID": "labs/lab-4.html#wrap-up",
    "href": "labs/lab-4.html#wrap-up",
    "title": "Lab 4",
    "section": "Wrap up",
    "text": "Wrap up\nSubmitting\n\n\n\n\n\n\nImportant\n\n\n\nBefore you proceed, first, make sure that you have updated the document YAML with your name! Then, render your document one last time, for good measure.\n\n\nTo submit your assignment to Gradescope:\n\nGo to your Files pane and check the box next to the PDF output of your document (lab-4.pdf).\nThen, in the Files pane, go to More &gt; Export. This will download the PDF file to your computer. Save it somewhere you can easily locate, e.g., your Downloads folder or your Desktop.\nGo to the course Canvas page and click on Gradescope and then click on the assignment. You’ll be prompted to submit it.\nMark the pages associated with each exercise. All of the papers of your lab should be associated with at least one question (i.e., should be “checked”).\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you fail to mark the pages associated with an exercise, that exercise won’t be graded. This means, if you fail to mark the pages for all exercises, you will receive a 0 on the assignment. The TAs can’t mark your pages for you, and for them to be able to grade, you must mark them.\n\n\nGrading\n\n\nExercise\nPoints\n\n\n\nExercise 1\n3\n\n\nExercise 2\n5\n\n\nExercise 3\n5\n\n\nExercise 4\n5\n\n\nExercise 5\n5\n\n\nExercise 6\n8\n\n\nExercise 7\n6\n\n\nExercise 8\n7\n\n\nExercise 9\n1\n\n\nExercise 10\n5\n\n\nTotal\n50",
    "crumbs": [
      "Labs",
      "Lab 4"
    ]
  },
  {
    "objectID": "labs/lab-7.html",
    "href": "labs/lab-7.html",
    "title": "Lab 7 - Inference for proportions and means",
    "section": "",
    "text": "Use these steps to navigate to the STA101 version of RStudio using the Duke Container Manager;\nUse these steps to download all of the lab 7 files from our Canvas page, upload them to your RStudio files, and move them into an appropriately named folder (lab-7, for instance);\nOnce lab-7.qmd is where it needs to be, open it, and verify that you can click the “Render” button in RStudio and get a PDF file. See this answer on Ed if you want more guidance here;\nNow proceed to complete the exercises in this lab.",
    "crumbs": [
      "Labs",
      "Lab 7"
    ]
  },
  {
    "objectID": "labs/lab-7.html#getting-started",
    "href": "labs/lab-7.html#getting-started",
    "title": "Lab 7 - Inference for proportions and means",
    "section": "",
    "text": "Use these steps to navigate to the STA101 version of RStudio using the Duke Container Manager;\nUse these steps to download all of the lab 7 files from our Canvas page, upload them to your RStudio files, and move them into an appropriately named folder (lab-7, for instance);\nOnce lab-7.qmd is where it needs to be, open it, and verify that you can click the “Render” button in RStudio and get a PDF file. See this answer on Ed if you want more guidance here;\nNow proceed to complete the exercises in this lab.",
    "crumbs": [
      "Labs",
      "Lab 7"
    ]
  },
  {
    "objectID": "labs/lab-7.html#pointers",
    "href": "labs/lab-7.html#pointers",
    "title": "Lab 7 - Inference for proportions and means",
    "section": "Pointers",
    "text": "Pointers\n\nFor all visualizations you create, be sure to include informative titles for the plot, axes, and legend;\nRespond in complete sentences as much as possible;\nBe sure to observe good code style:\n\nThere is a line break after each |&gt; in a pipeline or + in a ggplot;\nThere are spaces around = signs;\nThere is a space after each ,;\nCode is properly indented;\nCode doesn’t exceed 80 characters in each line, longer lines of code are spread across multiple lines with appropriately placed line breaks (so in the rendered PDF, your code shouldn’t run off the page);\nCode chunks are labeled, informatively and without spaces.",
    "crumbs": [
      "Labs",
      "Lab 7"
    ]
  },
  {
    "objectID": "labs/lab-7.html#packages",
    "href": "labs/lab-7.html#packages",
    "title": "Lab 7 - Inference for proportions and means",
    "section": "Packages",
    "text": "Packages\n\nlibrary(tidyverse) \nlibrary(tidymodels)\nlibrary(openintro)\nlibrary(dsbox)",
    "crumbs": [
      "Labs",
      "Lab 7"
    ]
  },
  {
    "objectID": "labs/lab-7.html#part-1-general-social-survey",
    "href": "labs/lab-7.html#part-1-general-social-survey",
    "title": "Lab 7 - Inference for proportions and means",
    "section": "Part 1: General Social Survey",
    "text": "Part 1: General Social Survey\nThe General Social Survey (GSS) is a sociological survey used to collect data on demographic characteristics and attitudes of residents of the United States. The GSS has been conducted each year since 1972 by the National Opinion Research Center at the University of Chicago. The GSS collects data on demographic characteristics and attitudes of residents of the United States. The GSS sample is designed as a multistage stratified sample.\nFor the exercises in this part, we’ll use the gss16 data set from the dsbox package. You can find out more about the dataset by inspecting its documentation, which you can access by running ?gss16 in the Console or using the Help menu in RStudio to search for gss16. You can also find this information here.\nIn 2016, the GSS added a new question on harassment at work. The question is phrased as the following.\n\nOver the past five years, have you been harassed by your superiors or co-workers at your job, for example, have you experienced any bullying, physical or psychological abuse?\n\nAnswers to this question are stored in the harass5 variable in gss16 set.\nExercise 1\n\nCreate a subset of the data that only contains Yes and No answers for the harassment question. How many responses chose each of these answers?\n\nExercise 2\n\nDescribe how bootstrapping can be used to estimate the proportion of all Americans who have been harassed by their superiors or co-workers at their job.\n\nExercise 3\n\n\nCalculate a 95% bootstrap confidence interval for the proportion of Americans who have been harassed by their superiors or co-workers at their job. Use 1,000 iterations when creating your bootstrap distribution. Set the seed to 1234. Interpret this interval in context of the data. Make sure to visualize this interval as well.\nWhere was your 95% confidence interval centered? Why does this make sense?\nNow, calculate 90% bootstrap confidence interval for the proportion of Americans who have been harassed by their superiors or co-workers at their job. Interpret this interval in context of the data. Is it wider or more narrow than the 95% confidence interval? Additionally, visualize the interval by layering it on the visualization (for the 95% confidence interval) from part (a). You can do this simply by adding another shading layer where the endpoints now come from the new 90% confidence interval. Use a different pair of colors for the color and fill arguments, e.g., darkorange4 and darkorange1.\nNow, suppose you created a bootstrap distribution with 50,000 simulations instead of 1,000. What would you expect to change (if anything)? Center of your confidence interval? Width of your confidence interval? Do not run this simulation, just conceptually think about it.\n\n\n\n\n\n\n\n\nPause and render\n\n\n\nNow that you’ve completed the second part of the exercises, pause and render your document. If it renders without any issues, great! Move on to the next exercise. If it does not, debug the issue before moving on. Ask for help from your TA if you need. Do not proceed without rendering your document.",
    "crumbs": [
      "Labs",
      "Lab 7"
    ]
  },
  {
    "objectID": "labs/lab-7.html#part-2-course-scores",
    "href": "labs/lab-7.html#part-2-course-scores",
    "title": "Lab 7 - Inference for proportions and means",
    "section": "Part 2: Course scores",
    "text": "Part 2: Course scores\nStudents in large university course that is offered each semester average 32 (out of 50) on the midterm, with a standard deviation of 4. The scores are distributed normally.\nExercise 4\n\n\nWhat is the probability a randomly selected student scored less than 28?\nWhat is the probability a randomly selected student scored higher than than 35?\nWhat score did only 20 percent of the class exceed?\n\n\nExercise 5\n\nI pick 10 students at random from the class.\n\n[Without looking at the next question] How does the standard error of their average score compare to the standard deviation of the population of all STA 101 students? Explain your reasoning.\nThe standard error of the mean can be calculated as \\(SE = \\frac{\\sigma}{\\sqrt{n}}\\). What is the probability the average midterm score of these 10 students is less than 28?\nWhat is the probability the average midterm score of these 10 students is higher than 35?\nHow do the probabilities you calculated in the previous two questions (less than 28 and higher than 35) compare to the probabilities you calculated in Exercise 4? Why?\n\n\n\n\n\n\n\n\nPause and render\n\n\n\nNow that you’ve completed the first part of the exercises, pause and render your document. If it renders without any issues, great! Move on to the next exercise. If it does not, debug the issue before moving on. Ask for help from your TA if you need. Do not proceed without rendering your document.",
    "crumbs": [
      "Labs",
      "Lab 7"
    ]
  },
  {
    "objectID": "labs/lab-7.html#part-3-ims-exercises",
    "href": "labs/lab-7.html#part-3-ims-exercises",
    "title": "Lab 7 - Inference for proportions and means",
    "section": "Part 3: IMS exercises",
    "text": "Part 3: IMS exercises\nThe exercises in this section do not require code. Make sure to answer the questions in full sentences.\nExercise 6\n\nIMS - Chapter 16 exercises, #2: Married at 25.\n\nExercise 7\n\nIMS - Chapter 16 exercises, #5: If I fits, I sits, bootstrap test.\n\nExercise 8\n\nIMS - Chapter 16 exercises, #6: Legalization of marijuana, bootstrap test.\n\nExercise 9\n\nIMS - Chapter 16 exercises, #8: Legalization of marijuana, standard errors.\n\nExercise 10\n\nIMS - Chapter 16 exercises, #22: Legalization of marijuana, mathematical interval.",
    "crumbs": [
      "Labs",
      "Lab 7"
    ]
  },
  {
    "objectID": "labs/lab-7.html#lastly",
    "href": "labs/lab-7.html#lastly",
    "title": "Lab 7 - Inference for proportions and means",
    "section": "Lastly",
    "text": "Lastly\nRecommend some music for us to listen to while we grade this.",
    "crumbs": [
      "Labs",
      "Lab 7"
    ]
  },
  {
    "objectID": "labs/lab-7.html#wrap-up",
    "href": "labs/lab-7.html#wrap-up",
    "title": "Lab 7 - Inference for proportions and means",
    "section": "Wrap up",
    "text": "Wrap up\nSubmitting\n\n\n\n\n\n\nImportant\n\n\n\nBefore you proceed, first, make sure that you have updated the document YAML with your name! Then, render your document one last time, for good measure.\n\n\nTo submit your assignment to Gradescope:\n\nGo to your Files pane and check the box next to the PDF output of your document (lab-7.pdf).\nThen, in the Files pane, go to More &gt; Export. This will download the PDF file to your computer. Save it somewhere you can easily locate, e.g., your Downloads folder or your Desktop.\nGo to the course Canvas page and click on Gradescope and then click on the assignment. You’ll be prompted to submit it.\nMark the pages associated with each exercise. All of the papers of your lab should be associated with at least one question (i.e., should be “checked”).\n\n\n\n\n\n\n\nWarning\n\n\n\nIf you fail to mark the pages associated with an exercise, that exercise won’t be graded. This means, if you fail to mark the pages for all exercises, you will receive a 0 on the assignment. The TAs can’t mark your pages for you, and for them to be able to grade, you must mark them.\n\n\nGrading\nThe lab will be graded out of 50 total points.",
    "crumbs": [
      "Labs",
      "Lab 7"
    ]
  },
  {
    "objectID": "course-materials.html",
    "href": "course-materials.html",
    "title": "Course materials",
    "section": "",
    "text": "All books are freely available online:\n\n[ims]: Mine Çetinkaya-Rundel and Jo Hardin. Introduction to Modern Statistics. 2nd edition. OpenIntro, 2024.\n[r4ds]: Hadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund. R for Data Science. 2nd edition. O’Reilly, 2022.",
    "crumbs": [
      "Syllabus",
      "Course materials"
    ]
  },
  {
    "objectID": "course-materials.html#textbooks",
    "href": "course-materials.html#textbooks",
    "title": "Course materials",
    "section": "",
    "text": "All books are freely available online:\n\n[ims]: Mine Çetinkaya-Rundel and Jo Hardin. Introduction to Modern Statistics. 2nd edition. OpenIntro, 2024.\n[r4ds]: Hadley Wickham, Mine Çetinkaya-Rundel, and Garrett Grolemund. R for Data Science. 2nd edition. O’Reilly, 2022.",
    "crumbs": [
      "Syllabus",
      "Course materials"
    ]
  },
  {
    "objectID": "course-materials.html#technology",
    "href": "course-materials.html#technology",
    "title": "Course materials",
    "section": "Technology",
    "text": "Technology\nYou will need to bring a laptop to all lectures and labs. Options for obtaining a laptop through the university are described here. Armed with your trusty laptop, you must be able to access the following:\n\nThis course page that you are on right now;\nR/RStudio as provided by the Duke Container Manager;\nCanvas, through which you can access…\n\nGradescope;\nEd Discussion;\n\nZoom (e.g. for remote office hours).\n\nIf access to technology becomes a concern for you during the semester, contact the instructor immediately to discuss options.",
    "crumbs": [
      "Syllabus",
      "Course materials"
    ]
  },
  {
    "objectID": "ae/ae-01-unvotes.html",
    "href": "ae/ae-01-unvotes.html",
    "title": "UN Votes",
    "section": "",
    "text": "How do various countries vote in the United Nations General Assembly, how have their voting patterns evolved throughout time, and how similarly or differently do they view certain issues? Answering these questions (at a high level) is the focus of this analysis.\n\n\nWe will use the tidyverse, lubridate, and scales packages for data wrangling and visualization, and the DT package for interactive display of tabular output, and the unvotes package for the data.\n\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(DT)\nlibrary(unvotes)\nlibrary(ggthemes)\n\n\n\n\nThe data we’re using originally come from the unvotes package. In the chunk below we modify the data by joining the various data frames provided in the package to help you get started with the analysis.\n\nunvotes &lt;- un_votes |&gt;\n  inner_join(un_roll_calls, by = \"rcid\") |&gt;\n  inner_join(un_roll_call_issues, by = \"rcid\", relationship = \"many-to-many\")"
  },
  {
    "objectID": "ae/ae-01-unvotes.html#introduction",
    "href": "ae/ae-01-unvotes.html#introduction",
    "title": "UN Votes",
    "section": "",
    "text": "How do various countries vote in the United Nations General Assembly, how have their voting patterns evolved throughout time, and how similarly or differently do they view certain issues? Answering these questions (at a high level) is the focus of this analysis.\n\n\nWe will use the tidyverse, lubridate, and scales packages for data wrangling and visualization, and the DT package for interactive display of tabular output, and the unvotes package for the data.\n\nlibrary(tidyverse)\nlibrary(scales)\nlibrary(DT)\nlibrary(unvotes)\nlibrary(ggthemes)\n\n\n\n\nThe data we’re using originally come from the unvotes package. In the chunk below we modify the data by joining the various data frames provided in the package to help you get started with the analysis.\n\nunvotes &lt;- un_votes |&gt;\n  inner_join(un_roll_calls, by = \"rcid\") |&gt;\n  inner_join(un_roll_call_issues, by = \"rcid\", relationship = \"many-to-many\")"
  },
  {
    "objectID": "ae/ae-01-unvotes.html#un-voting-patterns",
    "href": "ae/ae-01-unvotes.html#un-voting-patterns",
    "title": "UN Votes",
    "section": "UN voting patterns",
    "text": "UN voting patterns\nLet’s create a data visualization that displays how the voting record of the UK & NI changed over time on a variety of issues, and compares it to two other countries: US and Turkey.\nWe can easily change which countries are being plotted by changing which countries the code above filters for. Note that the country name should be spelled and capitalized exactly the same way as it appears in the data. See the Appendix for a list of the countries in the data.\n\nunvotes |&gt;\n  filter(country %in% c(\"United Kingdom\", \"United States\", \"Turkey\")) |&gt;\n  mutate(year = year(date)) |&gt;\n  group_by(country, year, issue) |&gt;\n  summarize(percent_yes = mean(vote == \"yes\")) |&gt;\n  ggplot(mapping = aes(x = year, y = percent_yes, color = country)) +\n  geom_point(alpha = 0.4) +\n  geom_smooth(method = \"loess\", se = FALSE) +\n  facet_wrap(~issue) +\n  scale_y_continuous(labels = percent) +\n  scale_color_colorblind() +\n  labs(\n    title = \"Percentage of 'Yes' votes in the UN General Assembly\",\n    subtitle = \"1946 to 2019\",\n    y = \"% Yes\",\n    x = \"Year\",\n    color = \"Country\"\n  )"
  },
  {
    "objectID": "ae/ae-01-unvotes.html#references",
    "href": "ae/ae-01-unvotes.html#references",
    "title": "UN Votes",
    "section": "References",
    "text": "References\n\nRobinson D (2021). unvotes: United Nations General Assembly Voting Data. R package version 0.3.0, https://github.com/dgrtwo/unvotes.\nErik Voeten “Data and Analyses of Voting in the UN General Assembly” Routledge Handbook of International Organization, edited by Bob Reinalda (published May 27, 2013).\nMuch of the analysis has been modeled on the examples presented in the unvotes package vignette."
  },
  {
    "objectID": "ae/ae-01-unvotes.html#appendix",
    "href": "ae/ae-01-unvotes.html#appendix",
    "title": "UN Votes",
    "section": "Appendix",
    "text": "Appendix\nBelow is a list of countries in the dataset:"
  },
  {
    "objectID": "ae/ae-12-voters.html",
    "href": "ae/ae-12-voters.html",
    "title": "Worked example: estimating a single proportion",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "ae/ae-12-voters.html#packages",
    "href": "ae/ae-12-voters.html#packages",
    "title": "Worked example: estimating a single proportion",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)"
  },
  {
    "objectID": "ae/ae-12-voters.html#data",
    "href": "ae/ae-12-voters.html#data",
    "title": "Worked example: estimating a single proportion",
    "section": "Data",
    "text": "Data\nIn a survey conducted by Survey USA between September 30, 2023 and October 3, 2023, 2759 registered voters from all 50 US states were asked\n\nAmerica will hold an election for President of the United States next November. Not everyone makes the time to vote in every election. Which best describes you? Are you certain to vote? Will you probably vote? Are the chances you will vote about 50/50? Or will you probably not vote?\n\nThe data from this survey can be found in voting-survey.csv."
  },
  {
    "objectID": "ae/ae-12-voters.html#task-1",
    "href": "ae/ae-12-voters.html#task-1",
    "title": "Worked example: estimating a single proportion",
    "section": "Task 1",
    "text": "Task 1\nDo three things:\n\nload the data;\nvisualize the distribution of survey responses;\ncalculate the proportion of respondents who are certain to vote in the next presidential election.\n\n\n# Part 1\n\nvoting_survey &lt;- read_csv(\"data/voting-survey.csv\")\n\nRows: 2759 Columns: 2\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): vote\ndbl (1): id\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# Part 2\n\nggplot(voting_survey, aes(y = vote)) +\n  geom_bar()\n\n\n\n\n\n\n\n# Part 3\n\nvoting_survey |&gt;\n  count(vote) |&gt;\n  mutate(props = n / sum(n))\n\n# A tibble: 5 × 3\n  vote                       n  props\n  &lt;chr&gt;                  &lt;int&gt;  &lt;dbl&gt;\n1 About 50/50 chance       293 0.106 \n2 Certain to vote         1921 0.696 \n3 Not sure                  44 0.0159\n4 Probably will not vote    92 0.0333\n5 Will probably vote       409 0.148"
  },
  {
    "objectID": "ae/ae-12-voters.html#task-2",
    "href": "ae/ae-12-voters.html#task-2",
    "title": "Worked example: estimating a single proportion",
    "section": "Task 2",
    "text": "Task 2\nBased on these data, we want to estimate the true proportion of registered US voters who are certain to vote in the next presidential election. Use the bootstrap to approximate a 95% confidence interval for this quantity.\n\n# Prepare the data for analysis (pre-processing)\n\nvoting_survey &lt;- voting_survey |&gt;\n  mutate(vote = if_else(\n                        vote == \"Certain to vote\",\n                        \"Certain to vote\",\n                        \"Not certain to vote\")\n         )\n\nset.seed(20)\n\nboot_dist &lt;- voting_survey |&gt;\n  specify(response = vote, success = \"Certain to vote\") |&gt;\n  generate(reps = 1000, type = \"bootstrap\") |&gt;\n  calculate(stat = \"prop\")\n\nci &lt;- boot_dist |&gt;\n  get_ci()\nci\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1    0.679    0.714\n\nvisualize(boot_dist) + \n  shade_ci(ci)"
  },
  {
    "objectID": "ae/ae-12-voters.html#task-3",
    "href": "ae/ae-12-voters.html#task-3",
    "title": "Worked example: estimating a single proportion",
    "section": "Task 3",
    "text": "Task 3\nBAD: “There is a 95% probability that the true proportion is between 67.9% and 71.4%.”\nNOT SO BAD: “95% of the time, the true proportion is between 67.9% and 71.4%.”\nCORRECT, BUT SHADY: “We are 95% confident that the true proportion is between 67.9% and 71.4%.”\nCorrect: “95% of the time, an interval constructed like this one will contain the true proportion.”"
  },
  {
    "objectID": "ae/ae-12-voters.html#task-4",
    "href": "ae/ae-12-voters.html#task-4",
    "title": "Worked example: estimating a single proportion",
    "section": "Task 4",
    "text": "Task 4\nPew says that 66% of eligible US voters turned out for the 2020 presidential election. A newspaper claims that even more people will turnout in 2024, and cites this survey as evidence. Do these data provide convincing evidence for this claim?\nDo two things:\n- state the hypotheses\np = proportion of voters that turn out in 2024\nH_0: p = 0.66\nH_1: p &gt; 0.66\nOne-sided test\n- conduct a randomization test, at 5% discernability level. What is the conclusion of the test?\n\nobs_stat &lt;- voting_survey |&gt;\n    specify(response = vote, success = \"Certain to vote\") |&gt;\n    calculate(stat = \"prop\")\n\nset.seed(525600)\n\nnull_dist &lt;- voting_survey |&gt;\n  specify(response = vote, success = \"Certain to vote\") |&gt;\n  hypothesize(null = \"point\", p = 0.66) |&gt;\n  generate(reps = 1000, type = \"draw\") |&gt;\n  calculate(stat = \"prop\")\n\nnull_dist |&gt;\n  get_p_value(obs_stat, direction = \"greater\")\n\nWarning: Please be cautious in reporting a p-value of 0. This result is an approximation\nbased on the number of `reps` chosen in the `generate()` step.\nℹ See `get_p_value()` (`?infer::get_p_value()`) for more information.\n\n\n# A tibble: 1 × 1\n  p_value\n    &lt;dbl&gt;\n1       0"
  },
  {
    "objectID": "ae/ae-12-voters.html#task-5",
    "href": "ae/ae-12-voters.html#task-5",
    "title": "Worked example: estimating a single proportion",
    "section": "Task 5",
    "text": "Task 5\n“There is a 0% chance that the null is true.”\n“If the null were true, there is a 0% chance of seeing data like what we saw.”"
  },
  {
    "objectID": "ae/ae-04-durham.html",
    "href": "ae/ae-04-durham.html",
    "title": "2020 Durham City and County Resident Survey",
    "section": "",
    "text": "The main question we’ll explore today is “What are the demographics and priorities of City of Durham residents?”"
  },
  {
    "objectID": "ae/ae-04-durham.html#exercise-1",
    "href": "ae/ae-04-durham.html#exercise-1",
    "title": "2020 Durham City and County Resident Survey",
    "section": "Exercise 1",
    "text": "Exercise 1\nHow many rows and columns are in this dataset? What does each row represent and what does each column represent?\nAdd your answer here."
  },
  {
    "objectID": "ae/ae-04-durham.html#exercise-2",
    "href": "ae/ae-04-durham.html#exercise-2",
    "title": "2020 Durham City and County Resident Survey",
    "section": "Exercise 2",
    "text": "Exercise 2\nThe variables we’ll use in this analysis are as follows. Rename the variables to the updated names shown below.\n\n\n\nOriginal name\nUpdated name\n\n\n\n\nprimary_language\nprimary_language\n\n\ndo_you_own_or_rent_your_current_resi_31\nown_rent\n\n\nwould_you_say_your_total_annual_hous_35\nincome\n\n\n\n\n# add code here"
  },
  {
    "objectID": "ae/ae-04-durham.html#exercise-3",
    "href": "ae/ae-04-durham.html#exercise-3",
    "title": "2020 Durham City and County Resident Survey",
    "section": "Exercise 3",
    "text": "Exercise 3\nWhat language do Durham residents speak: primary_language?\n\nWhat is the primary language used in your household?\n\nAdd your answer here.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-04-durham.html#exercise-4",
    "href": "ae/ae-04-durham.html#exercise-4",
    "title": "2020 Durham City and County Resident Survey",
    "section": "Exercise 4",
    "text": "Exercise 4\nMake similar bar plots of own_rent and income. What distinct values do these variables take?\n\n# add code here"
  },
  {
    "objectID": "ae/ae-04-durham.html#exercise-5",
    "href": "ae/ae-04-durham.html#exercise-5",
    "title": "2020 Durham City and County Resident Survey",
    "section": "Exercise 5",
    "text": "Exercise 5\nThe variables own_rent and income are both categorical, but they’re stored as numbers. In R, categorical data are called factors. Recode these variables as factors with the as_factor() function.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-04-durham.html#exercise-6",
    "href": "ae/ae-04-durham.html#exercise-6",
    "title": "2020 Durham City and County Resident Survey",
    "section": "Exercise 6",
    "text": "Exercise 6\nRecreate the visualization from the previous exerciseincome` barplot, improving it for both visual appeal and better communication of findings.\nAdd your answer here.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-04-durham.html#exercise-7",
    "href": "ae/ae-04-durham.html#exercise-7",
    "title": "2020 Durham City and County Resident Survey",
    "section": "Exercise 7",
    "text": "Exercise 7\nRecreate the visualization from the previous exercise, but first calculate relative frequencies (proportions) of income (the marginal distribution) and plot the proportions instead of counts.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-04-durham.html#exercise-8",
    "href": "ae/ae-04-durham.html#exercise-8",
    "title": "2020 Durham City and County Resident Survey",
    "section": "Exercise 8",
    "text": "Exercise 8\nVisualize and describe the relationship between income and home ownership of Durham residents.\nStretch goal: Customize the colors using named colors from http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf.\nAdd your answer here.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-04-durham.html#exercise-9",
    "href": "ae/ae-04-durham.html#exercise-9",
    "title": "2020 Durham City and County Resident Survey",
    "section": "Exercise 9",
    "text": "Exercise 9\nCalculate the proportions of home owners for each category of Durham residents. Describe the relationship between these two variables, this time with the actual values from the conditional distribution of home ownership based on income level.\nAdd your answer here.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-04-durham.html#exercise-10",
    "href": "ae/ae-04-durham.html#exercise-10",
    "title": "2020 Durham City and County Resident Survey",
    "section": "Exercise 10",
    "text": "Exercise 10\nStretch goal: Recode the levels of these two variables to be more informatively labeled and calculate the proportions from the previous exercise again.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-04-durham.html#conceptual",
    "href": "ae/ae-04-durham.html#conceptual",
    "title": "2020 Durham City and County Resident Survey",
    "section": "Conceptual",
    "text": "Conceptual\nSome of the terms we introduced are:\n\nMarginal distribution: Distribution of a single variable.\nConditional distribution: Distribution of a variable conditioned on the values (or levels, in the context of categorical data) of another."
  },
  {
    "objectID": "ae/ae-04-durham.html#r",
    "href": "ae/ae-04-durham.html#r",
    "title": "2020 Durham City and County Resident Survey",
    "section": "R",
    "text": "R\nIn this application exercise we:\n\nDefined factors – the data type that R uses for categorical variables, i.e., variables that can take on values from a finite set of levels.\n\n\n\nReviewed data imports, visualization, and wrangling functions encountered before:\n\nImport: read_csv(): Read data from a CSV (comma separated values) file\nVisualization:\n\nggplot(): Create a plot using the ggplot2 package\naes(): Map variables from the data to aesthetic elements of the plot, generally passed as an argument to ggplot() or to geom_*() functions (define only x or y aesthetic)\ngeom_bar(): Represent data with bars, after calculating heights of bars under the hood\nlabs(): Label x axis, y axis, legend for color of plot, title` of plot, etc.\n\nWrangling:\n\nmutate(): Mutate the data frame by creating a new column or overwriting one of the existing columns\ncount(): Count the number of observations for each level of a categorical variable (factor) or each distinct value of any other type of variable\ngroup_by(): Perform each subsequent action once per each group of the variable, where groups can be defined based on the levels of one or more variables\n\n\nIntroduced new data wrangling functions:\n\nrename(): Rename columns in a data frame\nas_factor(): Convert a variable to a factor\ndrop_na(): Drop rows that have NA in one ore more specified variables\nif_else(): Write logic for what happens if a condition is true and what happens if it’s not\ncase_when(): Write a generalized if_else() logic for more than one codition\n\nIntroduced new data visualization functions:\n\ngeom_col(): Represent data with bars (columns), for heights that have already been calculated (must define x and y aesthetics)\nscale_fill_viridis_d(): Customize the discrete fill scale, using a color-blind friendly, ordinal discrete color scale\nscale_y_discrete(): Customize the discrete y scale\nscale_fill_manual(): Customize the fill scale by manually adjusting values for colors"
  },
  {
    "objectID": "ae/ae-04-durham.html#quarto",
    "href": "ae/ae-04-durham.html#quarto",
    "title": "2020 Durham City and County Resident Survey",
    "section": "Quarto",
    "text": "Quarto\nWe also introduced chunk options for managing figure sizes:\n\nfig-width: Width of figure\nfig-asp: Aspect ratio of figure (height / width)\nfig-height: Height of figure – but I recommend using fig-width and fig-asp, instead of fig-width and fig-height"
  },
  {
    "objectID": "ae/ae-04-durham-sa.html",
    "href": "ae/ae-04-durham-sa.html",
    "title": "2020 Durham City and County Resident Survey",
    "section": "",
    "text": "The main question we’ll explore today is “What are the demographics and priorities of City of Durham residents?”"
  },
  {
    "objectID": "ae/ae-04-durham-sa.html#exercise-1",
    "href": "ae/ae-04-durham-sa.html#exercise-1",
    "title": "2020 Durham City and County Resident Survey",
    "section": "Exercise 1",
    "text": "Exercise 1\nHow many rows and columns are in this dataset? What does each row represent and what does each column represent?"
  },
  {
    "objectID": "ae/ae-04-durham-sa.html#exercise-2",
    "href": "ae/ae-04-durham-sa.html#exercise-2",
    "title": "2020 Durham City and County Resident Survey",
    "section": "Exercise 2",
    "text": "Exercise 2\nThe variables we’ll use in this analysis are as follows. Rename the variables to the updated names shown below.\n\n\n\nOriginal name\nUpdated name\n\n\n\n\nprimary_language\nprimary_language\n\n\ndo_you_own_or_rent_your_current_resi_31\nown_rent\n\n\nwould_you_say_your_total_annual_hous_35\nincome\n\n\n\n\ndurham &lt;- durham |&gt;\n  rename(\n    own_rent = do_you_own_or_rent_your_current_resi_31,\n    income = would_you_say_your_total_annual_hous_35\n  )"
  },
  {
    "objectID": "ae/ae-04-durham-sa.html#exercise-3",
    "href": "ae/ae-04-durham-sa.html#exercise-3",
    "title": "2020 Durham City and County Resident Survey",
    "section": "Exercise 3",
    "text": "Exercise 3\nWhat language do Durham residents speak: primary_language?\n\nWhat is the primary language used in your household?\n\n\nggplot(durham, aes(x = primary_language)) +\n  geom_bar()"
  },
  {
    "objectID": "ae/ae-04-durham-sa.html#exercise-4",
    "href": "ae/ae-04-durham-sa.html#exercise-4",
    "title": "2020 Durham City and County Resident Survey",
    "section": "Exercise 4",
    "text": "Exercise 4\nMake similar bar plots of own_rent and income. What distinct values do these variables take?\n\nggplot(durham, aes(x = own_rent)) +\n  geom_bar()\n\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_count()`).\n\n\n\n\n\n\n\n\nggplot(durham, aes(x = income)) +\n  geom_bar()\n\nWarning: Removed 110 rows containing non-finite outside the scale range\n(`stat_count()`)."
  },
  {
    "objectID": "ae/ae-04-durham-sa.html#exercise-5",
    "href": "ae/ae-04-durham-sa.html#exercise-5",
    "title": "2020 Durham City and County Resident Survey",
    "section": "Exercise 5",
    "text": "Exercise 5\nThe variables own_rent and income are both categorical, but they’re stored as numbers. In R, categorical data are called factors. Recode these variables as factors with the as_factor() function.\n\ndurham &lt;- durham |&gt;\n  mutate(\n    income = as_factor(income),\n    own_rent = as_factor(own_rent)\n    )"
  },
  {
    "objectID": "ae/ae-04-durham-sa.html#exercise-6",
    "href": "ae/ae-04-durham-sa.html#exercise-6",
    "title": "2020 Durham City and County Resident Survey",
    "section": "Exercise 6",
    "text": "Exercise 6\nRecreate the visualization from the previous exercise, improving it for both visual appeal and better communication of findings.\n\ndurham |&gt;\n  ggplot(aes(y = income, fill = income)) +\n  geom_bar(show.legend = FALSE) +\n  labs(\n    x = \"Count\",\n    y = NULL,\n    title = \"Would you say your total annual household income is...\"\n  ) + \n  scale_y_discrete(\n    labels = c(\n      \"1\" = \"Under $30,000\",\n      \"2\" = \"$30,000-$59,999\",\n      \"3\" = \"$60,000-$99,999\",\n      \"4\" = \"$100,000 or more\"\n    )\n  )"
  },
  {
    "objectID": "ae/ae-04-durham-sa.html#exercise-7",
    "href": "ae/ae-04-durham-sa.html#exercise-7",
    "title": "2020 Durham City and County Resident Survey",
    "section": "Exercise 7",
    "text": "Exercise 7\nRecreate the visualization from the previous exercise, but first calculate relative frequencies (proportions) of income (the marginal distribution) and plot the proportions instead of counts.\n\nmy_proportions &lt;- durham |&gt;\n  count(income) |&gt;\n  mutate(prop = n / sum(n))\n\n\nggplot(my_proportions, aes(y = income, x = prop, fill = income)) +\n  geom_col(show.legend = FALSE) +\n  scale_fill_viridis_d(na.value = \"gray\") +\n  labs(\n    x = \"Proportion\",\n    y = NULL,\n    title = \"Would you say your total annual household income is...\"\n  ) +   \n  scale_y_discrete(\n    labels = c(\n      \"1\" = \"Under $30,000\",\n      \"2\" = \"$30,000-$59,999\",\n      \"3\" = \"$60,000-$99,999\",\n      \"4\" = \"$100,000 or more\"\n    )\n  )"
  },
  {
    "objectID": "ae/ae-04-durham-sa.html#exercise-8",
    "href": "ae/ae-04-durham-sa.html#exercise-8",
    "title": "2020 Durham City and County Resident Survey",
    "section": "Exercise 8",
    "text": "Exercise 8\nVisualize and describe the relationship between income and home ownership of Durham residents.\n\ndurham |&gt;\n  select(income, own_rent) |&gt;\n  drop_na() |&gt;\n  ggplot(aes(y = income, fill = own_rent)) +\n  geom_bar(position = \"fill\") +\n  scale_y_discrete(\n    labels = c(\n      \"1\" = \"Under $30,000\",\n      \"2\" = \"$30,000-$59,999\",\n      \"3\" = \"$60,000-$99,999\",\n      \"4\" = \"$100,000 or more\"\n    )\n  ) +\n  scale_fill_manual(\n    values = c(\"1\" = \"cadetblue\", \"2\" = \"coral\"),\n    labels = c(\"1\" = \"Own\", \"2\" = \"Rent\")\n  ) +\n  labs(\n    x = \"Proportion\",\n    y = \"Would you say your total\\nannual household income is...\",\n    fill = \"Do you own\\nor rent\\nyour current\\nresidence?\",\n    title = \"Income vs. home ownership of Durham residents\"\n  )"
  },
  {
    "objectID": "ae/ae-04-durham-sa.html#exercise-9",
    "href": "ae/ae-04-durham-sa.html#exercise-9",
    "title": "2020 Durham City and County Resident Survey",
    "section": "Exercise 9",
    "text": "Exercise 9\nCalculate the proportions of home owners for each category of Durham residents. Describe the relationship between these two variables, this time with the actual values from the conditional distribution of home ownership based on income level.\n\ndurham |&gt;\n  select(income, own_rent) |&gt;\n  drop_na() |&gt;\n  count(income, own_rent) |&gt;\n  group_by(income) |&gt;\n  mutate(prop = n / sum(n))\n\n# A tibble: 8 × 4\n# Groups:   income [4]\n  income own_rent     n   prop\n  &lt;fct&gt;  &lt;fct&gt;    &lt;int&gt;  &lt;dbl&gt;\n1 1      1           51 0.362 \n2 1      2           90 0.638 \n3 2      1          105 0.565 \n4 2      2           81 0.435 \n5 3      1          107 0.552 \n6 3      2           87 0.448 \n7 4      1          160 0.930 \n8 4      2           12 0.0698"
  },
  {
    "objectID": "ae/ae-04-durham-sa.html#exercise-10",
    "href": "ae/ae-04-durham-sa.html#exercise-10",
    "title": "2020 Durham City and County Resident Survey",
    "section": "Exercise 10",
    "text": "Exercise 10\nStretch goal: Recode the levels of these two variables to be more informatively labeled.\n\ndurham &lt;- durham |&gt;\n  mutate(\n    income = case_when(\n      income == \"1\" ~ \"Under $30,000\",\n      income == \"2\" ~ \"$30,000-$59,999\",\n      income == \"3\" ~ \"$60,000-$99,999\",\n      income == \"4\" ~ \"$100,000 or more\"      \n    ),\n    own_rent = if_else(own_rent == 1, \"Own\", \"Rent\")\n  )\n\ndurham |&gt;\n  select(income, own_rent) |&gt;\n  drop_na() |&gt;\n  count(income, own_rent) |&gt;\n  group_by(income) |&gt;\n  mutate(prop = n / sum(n))\n\n# A tibble: 8 × 4\n# Groups:   income [4]\n  income           own_rent     n   prop\n  &lt;chr&gt;            &lt;chr&gt;    &lt;int&gt;  &lt;dbl&gt;\n1 $100,000 or more Own        160 0.930 \n2 $100,000 or more Rent        12 0.0698\n3 $30,000-$59,999  Own        105 0.565 \n4 $30,000-$59,999  Rent        81 0.435 \n5 $60,000-$99,999  Own        107 0.552 \n6 $60,000-$99,999  Rent        87 0.448 \n7 Under $30,000    Own         51 0.362 \n8 Under $30,000    Rent        90 0.638"
  },
  {
    "objectID": "ae/ae-04-durham-sa.html#conceptual",
    "href": "ae/ae-04-durham-sa.html#conceptual",
    "title": "2020 Durham City and County Resident Survey",
    "section": "Conceptual",
    "text": "Conceptual\nSome of the terms we introduced are:\n\nMarginal distribution: Distribution of a single variable.\nConditional distribution: Distribution of a variable conditioned on the values (or levels, in the context of categorical data) of another."
  },
  {
    "objectID": "ae/ae-04-durham-sa.html#r",
    "href": "ae/ae-04-durham-sa.html#r",
    "title": "2020 Durham City and County Resident Survey",
    "section": "R",
    "text": "R\nIn this application exercise we:\n\nDefined factors – the data type that R uses for categorical variables, i.e., variables that can take on values from a finite set of levels.\n\n\n\nReviewed data imports, visualization, and wrangling functions encountered before:\n\nImport: read_csv(): Read data from a CSV (comma separated values) file\nVisualization:\n\nggplot(): Create a plot using the ggplot2 package\naes(): Map variables from the data to aesthetic elements of the plot, generally passed as an argument to ggplot() or to geom_*() functions (define only x or y aesthetic)\ngeom_bar(): Represent data with bars, after calculating heights of bars under the hood\nlabs(): Label x axis, y axis, legend for color of plot, title` of plot, etc.\n\nWrangling:\n\nmutate(): Mutate the data frame by creating a new column or overwriting one of the existing columns\ncount(): Count the number of observations for each level of a categorical variable (factor) or each distinct value of any other type of variable\ngroup_by(): Perform each subsequent action once per each group of the variable, where groups can be defined based on the levels of one or more variables\n\n\nIntroduced new data wrangling functions:\n\nrename(): Rename columns in a data frame\nas_factor(): Convert a variable to a factor\ndrop_na(): Drop rows that have NA in one ore more specified variables\nif_else(): Write logic for what happens if a condition is true and what happens if it’s not\ncase_when(): Write a generalized if_else() logic for more than one codition\n\nIntroduced new data visualization functions:\n\ngeom_col(): Represent data with bars (columns), for heights that have already been calculated (must define x and y aesthetics)\nscale_fill_viridis_d(): Customize the discrete fill scale, using a color-blind friendly, ordinal discrete color scale\nscale_y_discrete(): Customize the discrete y scale\nscale_fill_manual(): Customize the fill scale by manually adjusting values for colors"
  },
  {
    "objectID": "ae/ae-04-durham-sa.html#quarto",
    "href": "ae/ae-04-durham-sa.html#quarto",
    "title": "2020 Durham City and County Resident Survey",
    "section": "Quarto",
    "text": "Quarto\nWe also introduced chunk options for managing figure sizes:\n\nfig-width: Width of figure\nfig-asp: Aspect ratio of figure (height / width)\nfig-height: Height of figure – but I recommend using fig-width and fig-asp, instead of fig-width and fig-height"
  },
  {
    "objectID": "ae/ae-03-delta.html",
    "href": "ae/ae-03-delta.html",
    "title": "COVID vaccine and deaths from Delta variant",
    "section": "",
    "text": "The main question we’ll explore today is “How do deaths from COVID cases compare between vaccinated and unvaccinated?”\nWhat do you think?"
  },
  {
    "objectID": "ae/ae-03-delta.html#exercise-1",
    "href": "ae/ae-03-delta.html#exercise-1",
    "title": "COVID vaccine and deaths from Delta variant",
    "section": "Exercise 1",
    "text": "Exercise 1\nHow many rows and columns are in this dataset? Answer in a full sentence using inline code. What does each row represent and what does each column represent? For each variable, identify its type.\nAdd your answer here."
  },
  {
    "objectID": "ae/ae-03-delta.html#exercise-2",
    "href": "ae/ae-03-delta.html#exercise-2",
    "title": "COVID vaccine and deaths from Delta variant",
    "section": "Exercise 2",
    "text": "Exercise 2\nDo these data come from an observational study or experiment? Why?\nAdd your answer here."
  },
  {
    "objectID": "ae/ae-03-delta.html#exercise-3",
    "href": "ae/ae-03-delta.html#exercise-3",
    "title": "COVID vaccine and deaths from Delta variant",
    "section": "Exercise 3",
    "text": "Exercise 3\nCreate a visualization of health outcome by vaccine status that allows you to compare the proportion of deaths across those who are and are not vaccinated. What can you say about death rates in these two groups based on this visualization?\nAdd your answer here.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-03-delta.html#exercise-4",
    "href": "ae/ae-03-delta.html#exercise-4",
    "title": "COVID vaccine and deaths from Delta variant",
    "section": "Exercise 4",
    "text": "Exercise 4\nCalculate the proportion of deaths in among those who are vaccinated. Then, calculate the proportion among those who are not vaccinated.\nAdd your answer here.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-03-delta.html#exercise-5",
    "href": "ae/ae-03-delta.html#exercise-5",
    "title": "COVID vaccine and deaths from Delta variant",
    "section": "Exercise 5",
    "text": "Exercise 5\nCreate the visualization and calculate proportions from the two previous exercises, this time controlling for age. How do the proportions compare?\nAdd your answer here.\n\n# add code here"
  },
  {
    "objectID": "ae/ae-03-delta.html#exercise-6",
    "href": "ae/ae-03-delta.html#exercise-6",
    "title": "COVID vaccine and deaths from Delta variant",
    "section": "Exercise 6",
    "text": "Exercise 6\nBased on your findings so far, fill in the blanks with more, less, or equally: Is there anything surprising about these statements? Speculate on what, if anything, the discrepancy might be due to.\n\nIn 2021, among those in the UK who were COVID Delta cases, the vaccinated were ___ likely to die than the unvaccinated.\nFor those under 50, those who were unvaccinated were ___ likely to die than those who were vaccinated.\nFor those 50 and up, those who were unvaccinated were ___ likely to die than those who were vaccinated.\n\nAdd your answer here."
  },
  {
    "objectID": "ae/ae-03-delta.html#exercise-7",
    "href": "ae/ae-03-delta.html#exercise-7",
    "title": "COVID vaccine and deaths from Delta variant",
    "section": "Exercise 7",
    "text": "Exercise 7\nLet’s rephrase the previous question which asked you to speculate on why deaths among vaccinated cases overall is higher while deaths among unvaccinated cases are higher when we split the data into two groups (below 50 and 50 and up). What might be the confounding variable in the relationship between vaccination and deaths?\nAdd your answer here."
  },
  {
    "objectID": "ae/ae-03-delta.html#exercise-8",
    "href": "ae/ae-03-delta.html#exercise-8",
    "title": "COVID vaccine and deaths from Delta variant",
    "section": "Exercise 8",
    "text": "Exercise 8\nVisualize and describe the distribution of seniors (50 and up) based on (a.k.a. conditional on) vaccination status. Hint: Your description will benefit from calculating proportions of seniors in each of the vaccination groups and working those values into your narrative.\nAdd your answer here.\n\n# add your code here"
  },
  {
    "objectID": "course-overview.html",
    "href": "course-overview.html",
    "title": "Course overview",
    "section": "",
    "text": "Introduction to statistics as a science of understanding and analyzing data. Themes include data collection, exploratory analysis, inference, and modeling. Focus on principles underlying quantitative research in social sciences, humanities, and public policy. Research projects teach the process of scientific discovery and synthesis and critical evaluation of research and statistical arguments. Readings give perspective on why in 1950, S. Wilks said, ‘Statistical thinking will one day be as necessary a qualification for efficient citizenship as the ability to read and write.’\nPrerequisites: none.",
    "crumbs": [
      "Syllabus",
      "Overview"
    ]
  },
  {
    "objectID": "course-overview.html#course-description",
    "href": "course-overview.html#course-description",
    "title": "Course overview",
    "section": "",
    "text": "Introduction to statistics as a science of understanding and analyzing data. Themes include data collection, exploratory analysis, inference, and modeling. Focus on principles underlying quantitative research in social sciences, humanities, and public policy. Research projects teach the process of scientific discovery and synthesis and critical evaluation of research and statistical arguments. Readings give perspective on why in 1950, S. Wilks said, ‘Statistical thinking will one day be as necessary a qualification for efficient citizenship as the ability to read and write.’\nPrerequisites: none.",
    "crumbs": [
      "Syllabus",
      "Overview"
    ]
  },
  {
    "objectID": "course-overview.html#class-meetings",
    "href": "course-overview.html#class-meetings",
    "title": "Course overview",
    "section": "Class meetings",
    "text": "Class meetings\n\n\n\n\n\n\n\n\n\nMeeting\nLocation\nTime\nStaff\n\n\n\n\nLectures\nSocial Sciences 139\nTue, Thu 03:05 PM - 04:20 PM\nJohn Z\n\n\nLab 1\nPerkins LINK 087 (Classroom 3)\nFri 10:05 AM - 11:20 AM\nJohn G, Camilla\n\n\nLab 2\nPerkins LINK 087 (Classroom 3)\nFri 11:45 AM - 01:00 PM\nKatie, Meghna\n\n\nLab 3\nPerkins LINK 087 (Classroom 3)\nFri 01:25 PM - 02:40 PM\nLeah, Nichole\n\n\nLab 4\nPerkins LINK 087 (Classroom 3)\nFri 08:30 AM - 09:45 AM\nBenjamin, Sayali\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nFor reasons unknown to rational people, the labs are not exactly numbered in chronological order. “Lab 4” is in fact the first one of the day.",
    "crumbs": [
      "Syllabus",
      "Overview"
    ]
  },
  {
    "objectID": "course-team.html",
    "href": "course-team.html",
    "title": "Teaching team",
    "section": "",
    "text": "See the course schedule here.\n\n\n\n\n\n\n\n\n\nName\nRole\nLab section\nOffice hours\n\n\n\n\nJohn Zito\nInstructor\n\nThu 07:00 PM - 09:00 PM\nOld Chem 207\n\n\nLeah Johnson\nHead TA + Lab Leader\n3 (01:25 PM)\nWed 02:00 PM - 04:00 PM\nOld Chem 203B\n\n\nBenjamin Dahl\nLab Leader\n4 (08:30 AM)\nThu 08:00 AM - 10:00 AM\nOld Chem 203B\n\n\nJohn Gillen\nLab Leader\n1 (10:05 AM)\nSun 12:00 PM - 01:00 PM\nTue 05:00 PM - 06:00 PM\nZoom (Canvas)\n\n\nKatie Solarz\nLab Leader\n2 (11:45 AM)\nMon 4:30 PM - 6:30 PM\nOld Chem 203B\n\n\nCamilla Hanson\nLab Helper\n1 (10:05 AM)\nTue 12:00 PM - 01:00 PM\nWed 11:00 AM - 12:00 PM\nOld Chem 203B\n\n\nMeghna Katyal\nLab Helper\n2 (11:45 AM)\nMon 6:30 PM - 8:30 PM\nZoom (Canvas)\n\n\nSayali Pingle\nLab Helper\n4 (08:30 AM)\nMon 02:30 PM - 04:30 PM\nZoom (Canvas)\n\n\nNichole Zhang\nLab Helper\n3 (01:25 PM)\nWed 7:00 PM - 9:00 PM\nOld Chem 025\n\n\n\nIn addition to the teaching assistants, there is a peer tutor named Emilie Dorrestein that you can visit on Sundays at 05:00 PM - 06:00 PM (Perkins 413) and at 08:00 PM - 09:00 PM (Zoom – see Canvas for details).",
    "crumbs": [
      "Syllabus",
      "Teaching team"
    ]
  },
  {
    "objectID": "course-support.html",
    "href": "course-support.html",
    "title": "Course support",
    "section": "",
    "text": "Most of you will need help at some point and we want to make sure you can identify when that is without getting too frustrated and feel comfortable seeking help."
  },
  {
    "objectID": "course-support.html#lectures-and-labs",
    "href": "course-support.html#lectures-and-labs",
    "title": "Course support",
    "section": "Lectures and labs",
    "text": "Lectures and labs\nIf you have a question during lecture or lab, feel free to ask it! There are likely other students with the same question, so by asking you will create a learning opportunity for everyone."
  },
  {
    "objectID": "course-support.html#office-hours",
    "href": "course-support.html#office-hours",
    "title": "Course support",
    "section": "Office hours",
    "text": "Office hours\nThe teaching team is here to help you be successful in the course. You are encouraged to attend office hours during the times posted on the home page to ask questions about the course content and assignments. A lot of questions are most effectively answered in-person, so office hours are a valuable resource. I encourage each and every one of you to take advantage of this resource! Make a pledge to stop by office hours at least once during the first three weeks of class. If you truly have no questions to ask, just stop by and say hi and introduce yourself. You can find a list of everyone’s office hours here."
  },
  {
    "objectID": "course-support.html#slack",
    "href": "course-support.html#slack",
    "title": "Course support",
    "section": "Slack",
    "text": "Slack\nHave a question that can’t wait for office hours? Prefer to write out your question in detail rather than asking in person? The course Slack is the best venue for these! There is a chance another student has already asked a similar question, so please check the other posts on Slack before asking a new question. If you know the answer to a question that is posted, I encourage you to respond!"
  },
  {
    "objectID": "course-support.html#email",
    "href": "course-support.html#email",
    "title": "Course support",
    "section": "Email",
    "text": "Email\nPlease refrain from emailing any course content questions (those should go to Slack), and only use email for questions about personal matters that may not be appropriate for the public course forum (e.g., illness, accommodations, etc.). For such matters, you may email Dr. Mine Çetinkaya-Rundel at mc301@duke.edu.\nIf there is a question that’s not appropriate for the public forum, you are welcome to email me directly. If you email me, please include “STA 101” in the subject line. Barring extenuating circumstances, I will respond to STA 101 emails within 48 hours Monday - Friday. Response time may be slower for emails sent Friday evening - Sunday."
  },
  {
    "objectID": "course-support.html#academic-support",
    "href": "course-support.html#academic-support",
    "title": "Course support",
    "section": "Academic support",
    "text": "Academic support\nThere are times may need help with the class that is beyond what can be provided by the teaching team. In those instances, I encourage you to visit the Academic Resource Center. The Academic Resource Center (ARC) offers free services to all students during their undergraduate careers at Duke. Services include Learning Consultations, Peer Tutoring and Study Groups, ADHD/LD Coaching, Outreach Workshops, and more. Because learning is a process unique to every individual, they work with each student to discover and develop their own academic strategy for success at Duke. Contact the ARC to schedule an appointment. Undergraduates in any year, studying any discipline can benefit! Contact ARC@duke.edu, 919-684-5917."
  },
  {
    "objectID": "course-support.html#mental-health-and-wellness",
    "href": "course-support.html#mental-health-and-wellness",
    "title": "Course support",
    "section": "Mental health and wellness",
    "text": "Mental health and wellness\nStudent mental health and wellness are of primary importance at Duke, and the university offers resources to support students in managing daily stress and self-care. Duke offers several resources for students to seek assistance on coursework and to nurture daily habits that support overall well-being, some of which are listed below.\n\nThe Academic Resource Center: (919) 684-5917, theARC@duke.edu, or arc.duke.edu.\nDuWell: (919) 681-8421, provides Moments of Mindfulness (stress management and resilience building) and meditation programming to assist students in developing a daily emotional well-being practice. To see schedules for programs please see https://studentaffairs.duke.edu/duwell. All are welcome and no experience necessary.\n\nIf your mental health concerns and/or stressful events negatively affect your daily emotional state, academic performance, or ability to participate in your daily activities, many resources are available to help you through difficult times. Duke encourages all students to access these resources.\n\nDukeReach: Provides comprehensive outreach services to identify and support students in managing all aspects of well-being. If you have concerns about a student’s behavior or health visit the website for resources and assistance: http://studentaffairs.duke.edu/dukereach.\nCounseling and Psychological Services (CAPS): CAPS services include individual and group counseling services, psychiatric services, and workshops. To initiate services, walk-in/call-in 9-4 M,W,Th,F and 9-6 Tuesdays. CAPS also provides referral to off-campus resources for specialized care. (919) 660-1000 or https://students.duke.edu/wellness/caps.\nTimelyCare: (formerly known as Blue Devils Care) An online platform that is a convenient, confidential, and free way for Duke students to receive 24/7 mental health support through TalkNow and scheduled counseling: https://bluedevilscare.duke.edu."
  },
  {
    "objectID": "course-support.html#course-costs",
    "href": "course-support.html#course-costs",
    "title": "Course support",
    "section": "Course costs",
    "text": "Course costs\n\nTextbooks: The textbooks for this course are freely available on the web.\nLaptops: Each student is expected to have a laptop they can bring to each lecture and lab.\n\nIf you are having difficulty with costs associated with this course, here are some resources:\n\nContact the financial aid office (whether or not you are on aid). They have loans and resources for connecting students with programs on campus that might be able to help alleviate these costs.\nDukeLIFE offers course materials assistance for eligible students. Please note that students who are eligible for DukeLIFE benefits are notified prior to the start of the semester; program resources are limited.\nDuke Libraries offers textbook rentals through the Top Textbook Program, where you can rent out a textbook for 3 hours at a time.\nFor course-specific technology needs such as Digital Voice Recorder, HD Video Camera, TI-84 Plus CE, DSLR camera kit, Tripod, Shotgun Mic, iPad Mini 4, a Handheld Projector, or a GoPro, you can reserve rental equipment from the Link."
  },
  {
    "objectID": "course-support.html#assistance-with-canvas-and-zoom",
    "href": "course-support.html#assistance-with-canvas-and-zoom",
    "title": "Course support",
    "section": "Assistance with Canvas and Zoom",
    "text": "Assistance with Canvas and Zoom\nFor technical help with Canvas or Zoom, contact the Duke OIT Service Desk at oit.duke.edu/help. You can also access the self-service help documentation for Zoom here and for Canvas here.\nNote that we will be making minimal use of Canvas in this course (primarily for announcements and grade book). All assignment submission will take place on Gradescope and conversation on Slack.\nZoom will be used for online office hours as well as as a backup option should we need to hold the course online instead of in person."
  },
  {
    "objectID": "computing/computing-regression.html",
    "href": "computing/computing-regression.html",
    "title": "Simple linear regression",
    "section": "",
    "text": "The simple linear regression model relates a predictor \\(x\\) to a response \\(y\\) via a linear function with error:\n\\[\ny=\\beta_0+\\beta_1x+\\varepsilon.\n\\]\nThis primer leads you down the path of least resistance to fitting this model, creating a scatterplot with the best fit line added to it, and producing a table with the coefficient estimates (\\(\\hat{\\beta}_0,\\,\\hat{\\beta}_1\\)).",
    "crumbs": [
      "Computing primers",
      "Simple linear regression"
    ]
  },
  {
    "objectID": "computing/computing-regression.html#setup",
    "href": "computing/computing-regression.html#setup",
    "title": "Simple linear regression",
    "section": "Setup",
    "text": "Setup\nThe commands for working with linear regressions are in the tidymodels package, so load that:\n\nlibrary(tidyverse) \nlibrary(tidymodels) \n\nNext, we need something to model, so let us load in a data set. We will consider this data set on the stock price of Microsoft and Apple (introduced during Lecture 6 on 9/12/2024):\n\nstocks &lt;- read_csv(\"stocks.csv\")\n\n\n\n\n\n\n\nNote\n\n\n\nRecalling the information here, be prepared to adjust the file path to match how you have organized your files and folders.\n\n\nTo keep things simple, we’ll work with a subset of the data, stock prices in January 2020.\n\nstocks_jan2020 &lt;- stocks |&gt;\n  filter(month(date) == 1 & year(date) == 2020)",
    "crumbs": [
      "Computing primers",
      "Simple linear regression"
    ]
  },
  {
    "objectID": "computing/computing-regression.html#plotting-the-least-squares-regression-line-line-of-best-fit",
    "href": "computing/computing-regression.html#plotting-the-least-squares-regression-line-line-of-best-fit",
    "title": "Simple linear regression",
    "section": "Plotting the least squares regression line (“line of best fit”)",
    "text": "Plotting the least squares regression line (“line of best fit”)\nThis chunk of code creates a scatter plot of the Microsoft and Apple opening stock prices on the various trading days of January 2020, and it adds the line of best fit:\n\nggplot(stocks_jan2020, aes(x = MSFT.Open, y = AAPL.Open)) +\n  geom_point() + \n  geom_smooth(method = \"lm\", se = FALSE, color = \"red\") +\n  labs(\n    title = \"Daily stock price data\",\n    subtitle = \"January 2020\",\n    x = \"Microsoft opening price\",\n    y = \"Apple opening price\"\n  )\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nSo, all we have to do is add a new geom_WHAT layer to a scatterplot to add the line. This is what geom_smooth is doing. If you set se = TRUE, you get the band indicating the margin of error. Try it out!",
    "crumbs": [
      "Computing primers",
      "Simple linear regression"
    ]
  },
  {
    "objectID": "computing/computing-regression.html#what-are-the-coefficient-estimates",
    "href": "computing/computing-regression.html#what-are-the-coefficient-estimates",
    "title": "Simple linear regression",
    "section": "What are the coefficient estimates?",
    "text": "What are the coefficient estimates?\nThis code will give you a table with the estimates:\n\nstock_fit &lt;- linear_reg() |&gt;\n  fit(AAPL.Open ~ MSFT.Open, data = stocks_jan2020)\n\ntidy(stock_fit)\n\n# A tibble: 2 × 5\n  term        estimate std.error statistic      p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1 (Intercept)    3.31     8.87       0.373 0.713       \n2 MSFT.Open      0.454    0.0541     8.40  0.0000000808\n\n\nThere is a lot going on in that table, and we will explore some of it later, but focus on the first column for now. This column gives the estimates \\(\\hat{\\beta}_0,\\,\\hat{\\beta}_1\\). The first row has the estimate \\(\\hat{\\beta}_0\\) of the intercept, and the second row has the estimate \\(\\hat{\\beta}_1\\) of the slope. So the fitted model here is: \\[\n\\begin{align*}\n\\widehat{\\text{AAPL}}&=\\hat{\\beta}_0+\\hat{\\beta}_1{\\text{MSFT}}\\\\\n&\\approx3.31+0.45\\cdot{\\text{MSFT}}.\n\\end{align*}\n\\] So\n\n3.31 is the price you would predict for Apple stock if you knew Microsoft stock was opening at $0;\n0.45 is the price increase in Apple stock that you would predict if Microsoft stock became more expensive by $1 (remember, slope = rise/run, so \\(\\Delta\\text{AAPL}/\\Delta\\text{MSFT}\\) in this case).\n\nThe syntax in the fit commands is like fit(y ~ x). So the variable to the left of the ~ will be treated as the response variable (\\(y\\)), and the variable to the right will be treated as the predictor (\\(x\\)).",
    "crumbs": [
      "Computing primers",
      "Simple linear regression"
    ]
  },
  {
    "objectID": "computing/computing-file-paths.html",
    "href": "computing/computing-file-paths.html",
    "title": "File paths and reading in data",
    "section": "",
    "text": "Hopefully you have followed the advice here and organized your RStudio files into folders like this:\n\nAnd so, for example, when you go inside your Lecture 2 folder, it looks like this:\n\nThis is great for organization, but it causes a small problem. A command like this may not work:\n\nlibrary(tidyverse)\n\nflint &lt;- read_csv(\"flint.csv\")\n\nWhy not? Because the file flint.csv is inside one of your handy dandy folders, but you have not told read_csv which folder, and so it cannot find it. To fix this, change the code to this:\n\nflint &lt;- read_csv(\"~/lecture-2/flint.csv\")\n\nThe string \"~/lecture-2/flint.csv\" is called a file path and it tells the function read_csv the path of folders it has to search down in order to find the file flint.csv. If you have chosen to give your folders slightly different names like Lecture 2 or Lecture-2 or lecture 2 or john-sucks, that’s fine. Just make sure you make the appropriate change when you run the command. So flint &lt;- read_csv(\"~/Lecture 2/flint.csv\") or flint &lt;- read_csv(\"~/john-sucks/flint.csv\") , etc.",
    "crumbs": [
      "Computing primers",
      "Adjusting file paths"
    ]
  },
  {
    "objectID": "computing/computing-seeds.html",
    "href": "computing/computing-seeds.html",
    "title": "Random number seeds",
    "section": "",
    "text": "Statistical computing software like R can generate (pseudo)random numbers, like this:\n\nrnorm(5)\n\n[1] -0.3932406  1.2852251 -0.5689218 -0.4123566  1.2358167\n\n\nOr this:\n\nrnorm(5)\n\n[1] -1.9392724 -0.9563330 -1.3858488 -0.4422137 -0.4598596\n\n\nOr this:\n\nrnorm(5)\n\n[1] -0.3931979 -0.8684721 -1.0429014 -1.6626016  1.3405364\n\n\nLook pretty random to me. This allows us to perform simulations (eg. the bootstrap), which is an enormously important part of the modern statistician’s toolkit. Having said that, when you’re working with computer-generated random numbers, you want your work to be reproducible so that other people can check it. This means that you want to set a random number seed before you do a simulation. This ensures that the stream of random numbers in your simulation is the same every time, and someone else could run your code and get the exact same results that you did.\nSetting a seed looks like this:\n\nset.seed(8675309)\n\nrnorm(5)\n\n[1] -0.9965824  0.7218241 -0.6172088  2.0293916  1.0654161\n\n\nEvery time you run that code, you will get the same numbers:\n\nset.seed(8675309)\n\nrnorm(5)\n\n[1] -0.9965824  0.7218241 -0.6172088  2.0293916  1.0654161\n\n\nSo, if you ever write a code chunk that generates random numbers (eg. using the generate function), you should begin the code chunk by setting a random number seed so that you get the same results every time you run your stuff. The syntax as you saw above is set.seed(INTEGER). Sometimes we will tell you what number to put. Other times (and once you exit the course), you can put whatever you want. It doesn’t really matter. If you require inspiration, try these:\n\n1\n20\n988\n24601\n362436\n525600\n8675309\n8005882300",
    "crumbs": [
      "Computing primers",
      "Random number seed"
    ]
  },
  {
    "objectID": "computing/computing-access.html",
    "href": "computing/computing-access.html",
    "title": "Access RStudio",
    "section": "",
    "text": "In this class, you will always and everywhere access RStudio through the Duke Container Manager. This ensures that all of us are using the same version of all the software. If this were not the case, unexpected and difficult to diagnose/resolve coding incompatibilities could arise when you seek help from the teaching team or collaborate with your project partners. Nobody needs that, so please stick to the containers. Here is how you get in:\n\nGo here: https://cmgr.oit.duke.edu/containers. You may have to log in with your NetID at some point;\n(The first time you do this, you look for STA101 under “Reservations available” on the righthand side, and click “reserve STA101”. After you do that once, STA101 will appear under “My reservations” on the lefthand side forever more)\nClick STA101 under “My reservations”;\nLogin;\nStart. It may take a while, but then RStudio should launch in your browser.",
    "crumbs": [
      "Computing primers",
      "Accessing RStudio"
    ]
  },
  {
    "objectID": "computing/computing-mlr.html",
    "href": "computing/computing-mlr.html",
    "title": "Multiple linear regression (with two predictors)",
    "section": "",
    "text": "The multiple linear regression model relates several predictors to a single response via a linear function with error. Here is what it looks like with two predictors:\n\\[\ny=\\beta_0+\\beta_1x_1+\\beta_2x_2+\\varepsilon.\n\\]\nThis primer leads you down the path of least resistance to fitting this model when one of the predictors (\\(x_1\\)) is a numerical variable and the other (\\(x_2\\)) is a categorical variable with two levels. You will learn how to…",
    "crumbs": [
      "Computing primers",
      "Multiple linear regression"
    ]
  },
  {
    "objectID": "computing/computing-mlr.html#setup",
    "href": "computing/computing-mlr.html#setup",
    "title": "Multiple linear regression (with two predictors)",
    "section": "Setup",
    "text": "Setup\nThe commands for plotting are in the tidyverse package, and the commands for working with linear regressions are in the tidymodels package, so we crank that up:\n\nlibrary(tidyverse) \nlibrary(tidymodels) \n\nNext consider the dataset allbacks from the DAAG package (you may need to install this):\n\nlibrary(DAAG) \n\nallbacks\n\n   volume area weight cover\n1     885  382    800    hb\n2    1016  468    950    hb\n3    1125  387   1050    hb\n4     239  371    350    hb\n5     701  371    750    hb\n6     641  367    600    hb\n7    1228  396   1075    hb\n8     412    0    250    pb\n9     953    0    700    pb\n10    929    0    650    pb\n11   1492    0    975    pb\n12    419    0    350    pb\n13   1010    0    950    pb\n14    595    0    425    pb\n15   1034    0    725    pb\n\n\nThe dataset has 15 observations and 4 columns. Each observation represents a book. Note that volume is measured in cubic centimeters and weight is measured in grams. More information on the dataset can be found in the documentation for allbacks, with ?allbacks.\nWe are interested in the relationship between book volume (\\(x_1\\)) and book weight (\\(y\\)), but as you can see if you view the spreadsheet, some of the books are hardcover and some are paperback. We might expect a hardcover book to weigh more than a paperback book with the same volume because of the different materials used, and so we should account for that.",
    "crumbs": [
      "Computing primers",
      "Multiple linear regression"
    ]
  },
  {
    "objectID": "computing/computing-mlr.html#creating-a-grouped-scatterplot",
    "href": "computing/computing-mlr.html#creating-a-grouped-scatterplot",
    "title": "Multiple linear regression (with two predictors)",
    "section": "Creating a grouped scatterplot",
    "text": "Creating a grouped scatterplot\nThis code creates a scatterplot of volume vs. weight where the points have different color and shape depending on the value of cover:\n\nggplot(allbacks, aes(x = volume, y = weight)) +\n  geom_point(aes(color = cover, shape = cover)) +\n  labs(\n    x = \"Volume (cubic centimeters)\",\n    y = \"Weight (grams)\"\n  )\n\n\n\n\n\n\n\n\nThe only difference between this code and a simple scatter plot is that we included an aesthetic mapping aes(color = cover, shape = cover) inside the call to geom_point. You’re asking it to alter the color and shape of the points depending on the value of the variable cover. The computer then made some default choices for you about what the different colors and shapes will be.",
    "crumbs": [
      "Computing primers",
      "Multiple linear regression"
    ]
  },
  {
    "objectID": "computing/computing-mlr.html#adding-lines-of-best-fit",
    "href": "computing/computing-mlr.html#adding-lines-of-best-fit",
    "title": "Multiple linear regression (with two predictors)",
    "section": "Adding lines of best fit",
    "text": "Adding lines of best fit\nIf we want to add lines of best fit for each group, we just have to add one new layer to our code above:\n\nggplot(allbacks, aes(x = volume, y = weight)) +\n  geom_point(aes(color = cover, shape = cover)) + \n  geom_smooth(aes(color = cover), method = \"lm\", se = F) +\n  labs(\n    x = \"Volume (cubic centimeters)\",\n    y = \"Weight (grams)\"\n  )\n\n\n\n\n\n\n\n\nAgain, the only difference between this code and the code that would generate one line of best fit is that we added an aesthetic mapping aes(color = cover) to the arguments of geom_smooth, and the computer knows to plot several lines for each level of the variable cover, which only has two levels in this case.\nAs we expected, the line of best fit for the paperback books is below the line for the hardcover books, capturing our intuition that, for the same volume, a hardcover book will weigh a bit more.",
    "crumbs": [
      "Computing primers",
      "Multiple linear regression"
    ]
  },
  {
    "objectID": "computing/computing-mlr.html#getting-the-actual-coefficient-estimates",
    "href": "computing/computing-mlr.html#getting-the-actual-coefficient-estimates",
    "title": "Multiple linear regression (with two predictors)",
    "section": "Getting the actual coefficient estimates",
    "text": "Getting the actual coefficient estimates\nThis code will give you a table with the estimates:\n\nweight_cover_fit &lt;- linear_reg() |&gt;\n  fit(weight ~ volume + cover, data = allbacks)\n\ntidy(weight_cover_fit)\n\n# A tibble: 3 × 5\n  term        estimate std.error statistic      p.value\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;\n1 (Intercept)  198.      59.2         3.34 0.00584     \n2 volume         0.718    0.0615     11.7  0.0000000660\n3 coverpb     -184.      40.5        -4.55 0.000672    \n\n\nSo the fitted model is\n\\[\n\\begin{align*}\n\\widehat{\\text{weight}}&=\\hat{\\beta}_0+\\hat{\\beta}_1{\\text{volume}}+\\hat{\\beta}_2{\\text{cover}}\\\\\n&\\approx197.96+0.71\\cdot{\\text{volume}}-184.05\\cdot{\\text{cover}}.\n\\end{align*}\n\\] The interpretation of these estimates is:\n\n197.96 is the weight you would predict for a hardback book with zero volume (it’s a little silly that this is not zero, which highlights one limitation of a linear model here);\n0.71 is the amount (in grams) you would predict the weight to increase by if the volume of the book increased by 1 cubic centimeter (remember, slope = rise/run, so \\(\\Delta\\text{weight}/\\Delta\\text{volume}\\) in this case);\n-184.05 is the amount by which the line shifts downward (because it’s negative) to account for the fact that paperback books will typically be lighter than hardcover, even keeping the volume constant.\n\nThe only difference from the code for simple linear regression is that we added variables to the formula weight ~ volume + cover in the fit command. So in general, if you have a data frame df with columns y, x1, x2, and so on, you can fit a multiple linear regression with code that will look something like this:\n\nmy_regression_fit &lt;- linear_reg() |&gt;\n  fit(y ~ x1 + x2 + x3, data = df)\n\nTo add more predictors, you just add them to the formula y ~ x1 + x2 + x3 + ....",
    "crumbs": [
      "Computing primers",
      "Multiple linear regression"
    ]
  },
  {
    "objectID": "computing/computing-mlr.html#compute-the-r2-values",
    "href": "computing/computing-mlr.html#compute-the-r2-values",
    "title": "Multiple linear regression (with two predictors)",
    "section": "Compute the \\(R^2\\) values",
    "text": "Compute the \\(R^2\\) values\nOnce you have created a regression object with the code like linear_reg() |&gt; fit(), you can view the \\(R^2\\) values with the glance function to assess the goodness-of-fit of the model. Here they are for the simple linear regression that just has volume as a predictor:\n\nweight_fit &lt;- linear_reg() |&gt;\n  fit(weight ~ volume, data = allbacks)\n\nglance(weight_fit)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic    p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.803         0.787  124.      52.9 0.00000626     1  -92.5  191.  193.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nHere they are for the multiple linear regression that has volume and cover as predictors:\n\nweight_fit &lt;- linear_reg() |&gt;\n  fit(weight ~ volume + cover, data = allbacks)\n\nglance(weight_cover_fit)\n\n# A tibble: 1 × 12\n  r.squared adj.r.squared sigma statistic     p.value    df logLik   AIC   BIC\n      &lt;dbl&gt;         &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     0.927         0.915  78.2      76.7 0.000000145     2  -85.0  178.  181.\n# ℹ 3 more variables: deviance &lt;dbl&gt;, df.residual &lt;int&gt;, nobs &lt;int&gt;\n\n\nWe see that adjusted \\(R^2\\) increased substantially when we added cover as a predictor, supporting our intuition that including this covariate to the model ought to improve fit.",
    "crumbs": [
      "Computing primers",
      "Multiple linear regression"
    ]
  },
  {
    "objectID": "computing/computing-confidence.html",
    "href": "computing/computing-confidence.html",
    "title": "Confidence intervals with the bootstrap",
    "section": "",
    "text": "A \\(100\\times(1-\\alpha)\\%\\) confidence interval is an interval \\((L_n,\\,U_n)\\) that contains the true value \\(\\theta\\) of some quantity of interest with high probability across repeated sampling. In order to approximate such an interval in general, we use the bootstrap to approximate the sampling distribution of an estimator \\(\\hat{\\theta}\\), and then we use the quantiles of that boostrap distribution to determine the bounds of our interval.\nThis primer leads you down the path of least resistance to…doing all that.",
    "crumbs": [
      "Computing primers",
      "Confidence intervals"
    ]
  },
  {
    "objectID": "computing/computing-confidence.html#setup",
    "href": "computing/computing-confidence.html#setup",
    "title": "Confidence intervals with the bootstrap",
    "section": "Setup",
    "text": "Setup\nLoad our packages as usual:\n\nlibrary(tidyverse) \nlibrary(tidymodels) \n\nNext, consider this lil’ dataset:\n\nabb &lt;- read_csv(\"asheville.csv\")\n\nabb\n\n# A tibble: 50 × 1\n     ppg\n   &lt;dbl&gt;\n 1  48  \n 2  40  \n 3  99  \n 4  13  \n 5  55  \n 6  75  \n 7  74  \n 8 169  \n 9  56.2\n10  74.5\n# ℹ 40 more rows\n\n\nThese are data on the price per guest in USD (ppg) for a random sample of fifty Airbnb listings in 2020 for Asheville, NC. We are going to use these data to investigate what we would be expected to pay for an Airbnb in in Asheville, NC in June 2020. Here is what the data look like:\n\nggplot(abb, aes(x = ppg)) + \n  geom_histogram() +\n  labs(x = \"Price per guest ($)\")\n\n\n\n\n\n\n\n\nTo answer the question “what was the typical or expected price,” we can calculate a point estimate like the sample mean or sample median:\n\nabb |&gt; \n  summarize(mean = mean(ppg),\n            median = median(ppg))\n\n# A tibble: 1 × 2\n   mean median\n  &lt;dbl&gt;  &lt;dbl&gt;\n1  76.6   62.5\n\n\nIn this case they disagree quite a bit, which in general is something you should think about, but for the purposes of this primer, we will set the issue aside and focus just on the sample mean as our estimate.",
    "crumbs": [
      "Computing primers",
      "Confidence intervals"
    ]
  },
  {
    "objectID": "computing/computing-confidence.html#approximating-the-sampling-distribution-of-the-mean-with-the-bootstrap",
    "href": "computing/computing-confidence.html#approximating-the-sampling-distribution-of-the-mean-with-the-bootstrap",
    "title": "Confidence intervals with the bootstrap",
    "section": "Approximating the sampling distribution of the mean with the bootstrap",
    "text": "Approximating the sampling distribution of the mean with the bootstrap\nThe sampling distribution of the mean refers to the random variation we would observe in our mean estimate if we recomputed it based on many different hypothetical samples from the population (June 2020 Airbnb listings in Asheville, in this case). Unfortunately, we do not have many different samples from this population. We only have one sample. But if that sample is a decent approximation to the population (debateable, in this case), we can randomly generate different hypothetical samples from our sample (resamples), calculate a different mean for each, and look at the variation in those estimates.\nThis is dirty work, but thankfully the computer handles it with aplomb:\n\nset.seed(8675309) \n\nboot_dist_abb &lt;- abb |&gt;\n  specify(response = ppg) |&gt;\n  generate(reps = 5000, type = \"bootstrap\") |&gt;\n  calculate(stat = \"mean\")\n\nboot_dist_abb\n\nResponse: ppg (numeric)\n# A tibble: 5,000 × 2\n   replicate  stat\n       &lt;int&gt; &lt;dbl&gt;\n 1         1  70.8\n 2         2  66.2\n 3         3  72.8\n 4         4  73.1\n 5         5  76.4\n 6         6  80.1\n 7         7  80.4\n 8         8  69.1\n 9         9  91.7\n10        10  74.1\n# ℹ 4,990 more rows\n\n\nThe code works like this:\n\nspecify(response = ppg): this is a bookkeeping step where we tell the computer which column in our data frame we are studying. It’s simple in this case because there is only one;\ngenerate(reps = 5000, type = \"bootstrap\"): this generates 5000 bootstrap datasets. In general, we want the numbers of replicates/repetitions to be large enough for the approximation to be good, but not so large that it takes a while to run or hogs all the memory on our computer. For the small applications in our course, 1000 - 5000 replicates should suffice;\ncalculate(stat = \"mean\"): once we have the bootstrap datasets, we need to calculate the value of our desired statistic for each one. The variation in these estimates captures what we mean by “sampling uncertainty.” For these Airbnb data, the statistic we care about is the sample average or mean, so that’s what we ask for, but there are several options: “mean”, “median”, “sd”, “prop”, “diff in means”, “diff in props”, and so on.\n\n\nggplot(boot_dist_abb, aes(x = stat)) + \n  geom_histogram()",
    "crumbs": [
      "Computing primers",
      "Confidence intervals"
    ]
  },
  {
    "objectID": "computing/computing-confidence.html#calculating-an-approximate-confidence-interval",
    "href": "computing/computing-confidence.html#calculating-an-approximate-confidence-interval",
    "title": "Confidence intervals with the bootstrap",
    "section": "Calculating an approximate confidence interval",
    "text": "Calculating an approximate confidence interval\nArmed with our bootstrap sample of different means, we can calculate a 95% confidence interval using the quantile function:\n\nboot_dist_abb |&gt;\n  summarize(\n    lower = quantile(stat, 0.025),\n    upper = quantile(stat, 0.975)\n  )\n\n# A tibble: 1 × 2\n  lower upper\n  &lt;dbl&gt; &lt;dbl&gt;\n1  63.8  90.9\n\n\nAlternatively, we can use this nifty command which gives the same numbers:\n\nci_95_abb &lt;- boot_dist_abb |&gt;\n  get_confidence_interval(\n    point_estimate = mean(abb$ppg), \n    level = 0.95\n  )\nci_95_abb\n\n# A tibble: 1 × 2\n  lower_ci upper_ci\n     &lt;dbl&gt;    &lt;dbl&gt;\n1     63.8     90.9\n\n\nLastly, we can summarize it all with a picture:\n\nvisualize(boot_dist_abb) +\n  shade_confidence_interval(ci_95_abb)\n\n\n\n\n\n\n\n\nThe correct language for interpreting our interval goes something like this:\n\nWe are 95% confident that the true mean price lies in between $65.55 and $88.45.\n\nBut what does “95% confident” actually mean? It refers to the reliability in repeated use of the method that was used to generate the interval. It does not say anything about the reliability of the specific interval (65.55, 88.45), which is a tad disappointing.\nThink of it this way: if each day for 1000 days, we collected a new random sample of prices, and calculated a 95% confidence interval for each of those samples, approximately 950 of those 1000 intervals would contain the truth. That’s the sense in which the interval estimation method is 95% reliable. In practice however, we have one dataset and one interval. So how do we know if our interval is one of those lucky 950, or if we’re one of the 50 that went astray? Welcome to statistics, folks.",
    "crumbs": [
      "Computing primers",
      "Confidence intervals"
    ]
  },
  {
    "objectID": "computing/computing-barplot.html",
    "href": "computing/computing-barplot.html",
    "title": "Barplot basics",
    "section": "",
    "text": "A barplot is similar to a histogram, but instead of visualizing numerical data, we are visualizing categorical data. Each bar in the chart is counting the number of observations (rows) that correspond to each level of the categorical variable.",
    "crumbs": [
      "Computing primers",
      "Barplot basics"
    ]
  },
  {
    "objectID": "computing/computing-barplot.html#load-a-data-set",
    "href": "computing/computing-barplot.html#load-a-data-set",
    "title": "Barplot basics",
    "section": "Load a data set",
    "text": "Load a data set\n\nlibrary(tidyverse)\n\nLet’s load the data from the 2020 Durham City and County Resident Survey (originally in the Canvas lecture 4 folder), where each row is a Durham resident:\n\ndurham &lt;- read_csv(\"durham-2020.csv\")\n\n\n\n\n\n\n\nNote\n\n\n\nRecalling the information here, be prepared to adjust the file path to match how you have organized your files and folders.\n\n\nOne of the columns in this data set is a categorical variable called would_you_say_your_total_annual_hous_35, with four levels corresponding to the resident’s income:\n\nUnder $30,000;\n$30,000-$59,999\n$60,000-$99,999\n$100,000 or more\n\nFurthermore, some folks left this part of the survey blank, and so their income information is missing, which effectively creates a fifth category. Before plotting, let us quickly modify the data to give this income variable a more concise name, as well as making it a proper factor type in R:\n\ndurham &lt;- durham |&gt;\n  rename(income = would_you_say_your_total_annual_hous_35) |&gt;\n  mutate(income = as_factor(income))",
    "crumbs": [
      "Computing primers",
      "Barplot basics"
    ]
  },
  {
    "objectID": "computing/computing-barplot.html#the-basic-plot",
    "href": "computing/computing-barplot.html#the-basic-plot",
    "title": "Barplot basics",
    "section": "The basic plot",
    "text": "The basic plot\n\nggplot(durham, aes(x = income)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nThe basic template here is very similar to what we saw for the histogram. The first layer uses ggplot to indicate what data you wish to plot. Then the next layer uses geom_bar to specifically make it a bar plot.\nIn the first layer we used x = inside aes to make the bars vertical, but we could have made them horizontal as well:\n\nggplot(durham, aes(y = income)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nDo you see what changed?",
    "crumbs": [
      "Computing primers",
      "Barplot basics"
    ]
  },
  {
    "objectID": "computing/computing-barplot.html#adding-layers-to-prettify-it",
    "href": "computing/computing-barplot.html#adding-layers-to-prettify-it",
    "title": "Barplot basics",
    "section": "Adding layers to prettify it",
    "text": "Adding layers to prettify it\nWe could call it a day at this point, but the basic plot is rather drab. Here is some code that would zhuzh it up in a big way:\n\ndurham |&gt;\n  ggplot(aes(y = income, fill = income)) +\n  geom_bar(show.legend = FALSE) +\n  labs(\n    x = \"Count\",\n    y = NULL,\n    title = \"Would you say your total annual household income is...\"\n  ) + \n  scale_y_discrete(\n    labels = c(\n      \"1\" = \"Under $30,000\",\n      \"2\" = \"$30,000-$59,999\",\n      \"3\" = \"$60,000-$99,999\",\n      \"4\" = \"$100,000 or more\"\n    )\n  )\n\n\n\n\n\n\n\n\n\nThe first ggplot layer tells it what data to use. We also added the fill = argument inside aes to apply a default color scheme;\nthe labs layer allows us to add a title and label the axes;\nthe scale_y_discrete layer allows us to replace the bar labels 1, 2, 3, 4 with more informative text that explains what the levels of the variable actually mean;\nin between every layer, we have a +. Gotta have that icing!",
    "crumbs": [
      "Computing primers",
      "Barplot basics"
    ]
  },
  {
    "objectID": "computing/computing-students-t.html",
    "href": "computing/computing-students-t.html",
    "title": "Using the t-distribution",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)"
  },
  {
    "objectID": "computing/computing-students-t.html#packages",
    "href": "computing/computing-students-t.html#packages",
    "title": "Using the t-distribution",
    "section": "",
    "text": "library(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)"
  },
  {
    "objectID": "computing/computing-students-t.html#data",
    "href": "computing/computing-students-t.html#data",
    "title": "Using the t-distribution",
    "section": "Data",
    "text": "Data\nEvery year, the United States Department of Health and Human Services releases to the public a large dataset containing information on births recorded in the country. This dataset has been of interest to medical researchers who are studying the relation between habits and practices of expectant mothers and the birth of their children. In this case study we work with a random sample of 1,000 cases from the dataset released in 2014. The length of pregnancy, measured in weeks, is commonly referred to as gestation.\n\nglimpse(births14)\n\nRows: 1,000\nColumns: 13\n$ fage           &lt;int&gt; 34, 36, 37, NA, 32, 32, 37, 29, 30, 29, 30, 34, 28, 28,…\n$ mage           &lt;dbl&gt; 34, 31, 36, 16, 31, 26, 36, 24, 32, 26, 34, 27, 22, 31,…\n$ mature         &lt;chr&gt; \"younger mom\", \"younger mom\", \"mature mom\", \"younger mo…\n$ weeks          &lt;dbl&gt; 37, 41, 37, 38, 36, 39, 36, 40, 39, 39, 42, 40, 40, 39,…\n$ premie         &lt;chr&gt; \"full term\", \"full term\", \"full term\", \"full term\", \"pr…\n$ visits         &lt;dbl&gt; 14, 12, 10, NA, 12, 14, 10, 13, 15, 11, 14, 16, 20, 15,…\n$ gained         &lt;dbl&gt; 28, 41, 28, 29, 48, 45, 20, 65, 25, 22, 40, 30, 31, NA,…\n$ weight         &lt;dbl&gt; 6.96, 8.86, 7.51, 6.19, 6.75, 6.69, 6.13, 6.74, 8.94, 9…\n$ lowbirthweight &lt;chr&gt; \"not low\", \"not low\", \"not low\", \"not low\", \"not low\", …\n$ sex            &lt;chr&gt; \"male\", \"female\", \"female\", \"male\", \"female\", \"female\",…\n$ habit          &lt;chr&gt; \"nonsmoker\", \"nonsmoker\", \"nonsmoker\", \"nonsmoker\", \"no…\n$ marital        &lt;chr&gt; \"married\", \"married\", \"married\", \"not married\", \"marrie…\n$ whitemom       &lt;chr&gt; \"white\", \"white\", \"not white\", \"white\", \"white\", \"white…"
  },
  {
    "objectID": "computing/computing-students-t.html#t_test",
    "href": "computing/computing-students-t.html#t_test",
    "title": "Using the t-distribution",
    "section": "t_test",
    "text": "t_test\n\nt_test(births14, \n       response = weeks, \n       alternative = \"two-sided\",\n       mu = 40,\n       conf_int = TRUE, \n       conf_level = 0.95)\n\n# A tibble: 1 × 7\n  statistic  t_df  p_value alternative estimate lower_ci upper_ci\n      &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;          &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;\n1     -16.4   999 5.71e-54 two.sided       38.7     38.5     38.8\n\nbirths14 |&gt;\n  summarize(\n    n = n(),\n    xbar = mean(weeks),\n    s = sd(weeks),\n    se = s / sqrt(n),\n    alpha = 0.05,\n    df = n - 1,\n    t_star = qt(1 - (alpha / 2), df = df),\n    L = xbar - t_star * se,\n    U = xbar + t_star * se\n  )\n\n# A tibble: 1 × 9\n      n  xbar     s     se alpha    df t_star     L     U\n  &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1  1000  38.7  2.56 0.0811  0.05   999   1.96  38.5  38.8"
  },
  {
    "objectID": "computing/computing-histograms.html",
    "href": "computing/computing-histograms.html",
    "title": "Histogram basics",
    "section": "",
    "text": "A histogram displays the distribution or variability of numerical data. It shows you where the data values are typically concentrated, how spread out they are, and whether or not there are any asymmetries.\nThis primer leads you down the path of least resistance to producing a histogram plot using R. Trust me; you would not want to do this by hand.",
    "crumbs": [
      "Computing primers",
      "Histogram basics"
    ]
  },
  {
    "objectID": "computing/computing-histograms.html#loading-a-package",
    "href": "computing/computing-histograms.html#loading-a-package",
    "title": "Histogram basics",
    "section": "Loading a package",
    "text": "Loading a package\nThe command for plotting a histogram is contained inside the tidyverse package, so you need to load that first:\n\nlibrary(tidyverse)",
    "crumbs": [
      "Computing primers",
      "Histogram basics"
    ]
  },
  {
    "objectID": "computing/computing-histograms.html#loading-a-dataset",
    "href": "computing/computing-histograms.html#loading-a-dataset",
    "title": "Histogram basics",
    "section": "Loading a dataset",
    "text": "Loading a dataset\nNext, we need something to plot, so let us load in a data set. We will consider the flint water data that was mentioned during the second lecture on 8/29/2024:\n\nflint &lt;- read_csv(\"flint.csv\")\n\nLet us unpack what this is doing:\n\nread_csv is a function, just like \\(f(x)=x^2\\) or \\(f(x)=ax+b\\) from your math class. It receives an input, and returns an output. The input is the name of a file: \"flint.csv\". The output is a data frame (a spreadsheet) in R;\nWhen you run read_csv, it returns a data frame. But if you don’t give that data frame a name and store it someplace, then you can’t actually use it. So that’s what the flint &lt;- part is doing. You are assigning the output of read_csv to a variable named “flint.” If this went as planned, you should see flint listed in the upper right Environment tab of RStudio:\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nRecalling the information here, you may have to modify the file path to something like \"~/lecture-2/flint.csv\" to match the particular way you have chosen to name and organize your files and folders.",
    "crumbs": [
      "Computing primers",
      "Histogram basics"
    ]
  },
  {
    "objectID": "computing/computing-histograms.html#plotting-a-histogram",
    "href": "computing/computing-histograms.html#plotting-a-histogram",
    "title": "Histogram basics",
    "section": "Plotting a histogram",
    "text": "Plotting a histogram\nHere is the basic command in all its glory:\n\nggplot(flint, aes(x = lead)) +\n  geom_histogram(bins = 250) + \n  coord_cartesian(xlim = c(0, 100))\n\n\n\n\n\n\n\n\nNow let’s talk about it:\n\nggplot(flint, aes(x = lead)) is where we tell R what data frame we are using (flint) and which variable (column) in that data set we want to plot (lead);\nNext, we use geom_histogram(bins = 250) to tell it that we specifically want a histogram with 250 bins;\nLast, we use coord_cartesian(xlim = c(0, 100)) to adjust the bounds of the horizontal axis: start at x = 0 and end at x = 100;\nWe stitch all of these requests together using the + sign, and to make things more readable (not run off the page), we put a new line after each +.",
    "crumbs": [
      "Computing primers",
      "Histogram basics"
    ]
  },
  {
    "objectID": "computing/computing-histograms.html#adding-a-vertical-line-to-the-histogram",
    "href": "computing/computing-histograms.html#adding-a-vertical-line-to-the-histogram",
    "title": "Histogram basics",
    "section": "Adding a vertical line to the histogram",
    "text": "Adding a vertical line to the histogram\nWhen plotting a histogram, it can also be useful to indicate on the plot the location of some particularly meaningful summary statistics, such as the mean or median. In order to do this, we can use the same basic command from the previous section, but then add some extra layers that will place vertical lines on the plot at those values:\n\nmean_value &lt;- mean(flint$lead)\nmedian_value &lt;- median(flint$lead)\n\nggplot(flint, aes(x = lead)) +\n  geom_histogram(bins = 250) + \n  coord_cartesian(xlim = c(0, 100)) + \n  geom_vline(xintercept = mean_value, color = \"red\") + \n  geom_vline(xintercept = median_value, color = \"blue\")\n\n\n\n\n\n\n\n\nNotice that the distribution of lead levels is positively skewed, which results in the mean being larger than the median.",
    "crumbs": [
      "Computing primers",
      "Histogram basics"
    ]
  },
  {
    "objectID": "computing/computing-histograms.html#now-you-try",
    "href": "computing/computing-histograms.html#now-you-try",
    "title": "Histogram basics",
    "section": "Now you try",
    "text": "Now you try\nTo double check that you can follow these instructions, redo everything above using a different data set. This may require you to modify the code in various ways:\n\nthe file path you provide to read_csv;\nthe name you assign to the resulting data frame;\nthe variable you plot in aes(x = lead);\nthe number of histogram bins;\nthe range of the horizontal axis.\n\nCheck that you are comfortable making these changes.",
    "crumbs": [
      "Computing primers",
      "Histogram basics"
    ]
  }
]